{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1964b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is intented to test, some of the\n",
    "# results validation of T5 model \n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/sidhu/Projects/tf-transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "101b8775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_transformers.models import T5Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeeed337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Successful: Converted model using TF HF\n",
      "INFO:absl:Successful: Saved model at /tmp/tf_transformers_cache/t5-base/ckpt-1\n",
      "INFO:absl:Successful: Asserted and Converted `t5-base` from HF and saved it in cache folder /tmp/tf_transformers_cache/t5-base\n"
     ]
    }
   ],
   "source": [
    "# Check TF conversion\n",
    "\n",
    "!rm -rf /tmp/tf_transformers_cache/t5-base\n",
    "\n",
    "model_name = 't5-base'\n",
    "model, config = T5Model.get_model(model_name=model_name, convert_fn_type='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381fe525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235d9a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Successful: Converted model using PT HF\n",
      "INFO:absl:Successful: Saved model at /tmp/tf_transformers_cache/t5-base/ckpt-1\n",
      "INFO:absl:Successful: Asserted and Converted `t5-base` from HF and saved it in cache folder /tmp/tf_transformers_cache/t5-base\n"
     ]
    }
   ],
   "source": [
    "# Chec\n",
    "!rm -rf /tmp/tf_transformers_cache/t5-base\n",
    "\n",
    "model_name = 't5-base'\n",
    "model, config = T5Model.get_model(model_name=model_name, convert_fn_type='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f769719b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b4d15a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d96db216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.encoder_decoder.EncoderDecoder object at 0x7f1840cc0c10> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f1840e7e340>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.encoder_decoder.EncoderDecoder object at 0x7f1840cc0c10> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f1840e7e340>).\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/t5-base\n"
     ]
    }
   ],
   "source": [
    "# T5 text generation without caching\n",
    "text = \"summarize: studies have shown that owning a dog is good for you\"\n",
    "\n",
    "inputs_hf = tokenizer(text, return_tensors='tf')\n",
    "inputs = {}\n",
    "inputs['encoder_input_ids'] = inputs_hf['input_ids']\n",
    "inputs['encoder_input_mask'] = inputs_hf['attention_mask']\n",
    "inputs['decoder_input_ids']  = tf.constant([[0]])\n",
    "\n",
    "predictions_non_auto_regressive = []\n",
    "predictions_prob_non_auto_regressive = []\n",
    "\n",
    "for i in range(10):\n",
    "    outputs = model(inputs)\n",
    "    predicted_ids = tf.cast(tf.expand_dims(tf.argmax(outputs[\"last_token_logits\"], axis=1), 1), tf.int32)\n",
    "    inputs[\"decoder_input_ids\"] = tf.concat([inputs[\"decoder_input_ids\"], predicted_ids], axis=1)\n",
    "    predictions_non_auto_regressive.append(predicted_ids)\n",
    "    predictions_prob_non_auto_regressive.append(\n",
    "        tf.expand_dims(tf.reduce_max(outputs[\"last_token_logits\"], axis=1), 1)\n",
    "    )\n",
    "predictions_non_auto_regressive = tf.concat(predictions_non_auto_regressive, axis=1)\n",
    "predictions_prob_non_auto_regressive = tf.concat(predictions_prob_non_auto_regressive, axis=1)\n",
    "\n",
    "# Text generation with cache\n",
    "model, config = T5Model.get_model(model_name=model_name, convert_fn_type='pt', use_auto_regressive=True)\n",
    "\n",
    "encoder_input_ids = inputs_hf['input_ids']\n",
    "encoder_input_mask = inputs_hf['attention_mask']\n",
    "\n",
    "batch_size = tf.shape(encoder_input_ids)[0]\n",
    "seq_length = tf.shape(encoder_input_ids)[1]\n",
    "\n",
    "decoder_input_ids  = tf.reshape([0] * batch_size, (batch_size,1))\n",
    "\n",
    "\n",
    "encoder_hidden_dim = config['embedding_size']\n",
    "num_hidden_layers  = config['num_hidden_layers']\n",
    "num_attention_heads = config['num_attention_heads']\n",
    "attention_head_size = config['attention_head_size']\n",
    "\n",
    "encoder_hidden_states = tf.zeros((batch_size, seq_length, encoder_hidden_dim))\n",
    "\n",
    "decoder_all_cache_key = tf.zeros((num_hidden_layers, \n",
    "                                  batch_size, \n",
    "                                  num_attention_heads, \n",
    "                                  seq_length, \n",
    "                                  attention_head_size))\n",
    "decoder_all_cahce_value = tf.zeros((num_hidden_layers, \n",
    "                                  batch_size, \n",
    "                                  num_attention_heads, \n",
    "                                  seq_length, \n",
    "                                  attention_head_size))\n",
    "\n",
    "\n",
    "inputs = {}\n",
    "inputs['encoder_input_ids'] = encoder_input_ids\n",
    "inputs['encoder_input_mask']= encoder_input_mask\n",
    "inputs['decoder_input_ids'] = decoder_input_ids\n",
    "inputs['encoder_hidden_states'] = encoder_hidden_states\n",
    "inputs['decoder_all_cache_key'] = decoder_all_cache_key\n",
    "inputs['decoder_all_cache_value'] = decoder_all_cahce_value\n",
    "\n",
    "predictions_auto_regressive = []\n",
    "predictions_prob_auto_regressive = []\n",
    "\n",
    "for i in range(10):\n",
    "    outputs = model(inputs)\n",
    "    predicted_ids = tf.cast(tf.expand_dims(tf.argmax(outputs[\"last_token_logits\"], axis=1), 1), tf.int32)\n",
    "    inputs[\"decoder_input_ids\"] = predicted_ids\n",
    "    inputs[\"decoder_all_cache_key\"] = outputs[\"decoder_all_cache_key\"]\n",
    "    inputs[\"decoder_all_cache_value\"] = outputs[\"decoder_all_cache_value\"]\n",
    "    inputs[\"encoder_hidden_states\"] = outputs[\"encoder_hidden_states\"]\n",
    "    predictions_auto_regressive.append(predicted_ids)\n",
    "    predictions_prob_auto_regressive.append(\n",
    "        tf.expand_dims(tf.reduce_max(outputs[\"last_token_logits\"], axis=1), 1)\n",
    "    )\n",
    "predictions_auto_regressive = tf.concat(predictions_auto_regressive, axis=1)\n",
    "predictions_prob_auto_regressive = tf.concat(predictions_prob_auto_regressive, axis=1)\n",
    "\n",
    "#----------------------------------------------------------------------------------------#\n",
    "tf.assert_equal(predictions_non_auto_regressive, predictions_auto_regressive)\n",
    "assert(np.allclose(predictions_prob_non_auto_regressive.numpy(), \n",
    "            predictions_prob_auto_regressive.numpy()) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "672313b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5ec5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as tf_transformers/t5_encoder_layer_call_fn, tf_transformers/t5_encoder_layer_call_and_return_conditional_losses, tf_transformers/t5_decoder_layer_call_fn, tf_transformers/t5_decoder_layer_call_and_return_conditional_losses, word_embeddings_layer_call_fn while saving (showing 5 of 2220). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as tf_transformers/t5_encoder_layer_call_fn, tf_transformers/t5_encoder_layer_call_and_return_conditional_losses, tf_transformers/t5_decoder_layer_call_fn, tf_transformers/t5_decoder_layer_call_and_return_conditional_losses, word_embeddings_layer_call_fn while saving (showing 5 of 2220). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpj4_wsrz6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpj4_wsrz6/assets\n"
     ]
    }
   ],
   "source": [
    "# Text generation using saved_model with TextDecoder\n",
    "\n",
    "import tempfile\n",
    "import shutil\n",
    "from tf_transformers.text import TextDecoderSeq2Seq\n",
    "\n",
    "text = \"summarize: studies have shown that owning a dog is good for you\"\n",
    "\n",
    "saved_model_dir = tempfile.mkdtemp()\n",
    "model.save_as_serialize_module(saved_model_dir, overwrite=True)\n",
    "\n",
    "loaded   = tf.saved_model.load(saved_model_dir)\n",
    "decoder  = TextDecoderSeq2Seq(\n",
    "    model = loaded, \n",
    "    decoder_start_token_id = 0 # for t5\n",
    ")\n",
    "\n",
    "inputs_hf = tokenizer(text, return_tensors='tf')\n",
    "inputs = {}\n",
    "inputs['encoder_input_ids'] = inputs_hf['input_ids']\n",
    "inputs['encoder_input_mask'] = inputs_hf['attention_mask']\n",
    "\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='greedy', \n",
    "               max_iterations=10, \n",
    "               eos_id=-100)\n",
    "\n",
    "expected_ids = [[[ 293,   53,    3,    9, 1782,   19,  207,   21,   25,    6]]]\n",
    "assert(decoder_results['predicted_ids'].numpy().tolist() == expected_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a2954ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19916179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as tf_transformers/t5_encoder_layer_call_fn, tf_transformers/t5_encoder_layer_call_and_return_conditional_losses, tf_transformers/t5_decoder_layer_call_fn, tf_transformers/t5_decoder_layer_call_and_return_conditional_losses, word_embeddings_layer_call_fn while saving (showing 5 of 2220). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as tf_transformers/t5_encoder_layer_call_fn, tf_transformers/t5_encoder_layer_call_and_return_conditional_losses, tf_transformers/t5_decoder_layer_call_fn, tf_transformers/t5_decoder_layer_call_and_return_conditional_losses, word_embeddings_layer_call_fn while saving (showing 5 of 2220). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpj4_wsrz6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpj4_wsrz6/assets\n"
     ]
    }
   ],
   "source": [
    "# Text generation using saved_model with TextDecoderSerializable\n",
    "\n",
    "import tempfile\n",
    "import shutil\n",
    "#from tf_transformers.text import TextDecoderSerializableSeq2Seq\n",
    "\n",
    "\n",
    "# loaded   = tf.saved_model.load(saved_model_dir)\n",
    "decoder  = TextDecoderSerializableSeq2Seq(\n",
    "    model = model,\n",
    "    decoder_start_token_id = 0,\n",
    "    max_iterations=10,\n",
    "    mode=\"greedy\",\n",
    "    do_sample=False,\n",
    "    eos_id=-100\n",
    ")\n",
    "\n",
    "# Save\n",
    "decoder_model = decoder.get_model()\n",
    "decoder_model.save_serialized(saved_model_dir, overwrite=True)\n",
    "\n",
    "# Load\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "text = \"summarize: studies have shown that owning a dog is good for you\"\n",
    "inputs_hf = tokenizer(text, return_tensors='tf')\n",
    "inputs = {}\n",
    "inputs['encoder_input_ids'] = inputs_hf['input_ids']\n",
    "inputs['encoder_input_mask'] = inputs_hf['attention_mask']\n",
    "\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs)\n",
    "\n",
    "np.allclose(decoder_results_serialized['predicted_ids'].numpy(), expected_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35813332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b17ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
