{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7700d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is intented to test, some of the\n",
    "# results validation of GPT2 model \n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/sidhu/Projects/tf-transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d49e67ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_transformers.models.gpt2.gpt2_model import GPT2Model\n",
    "from transformers import GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a6186d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1118baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dfe69f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Successful: Converted model using PT HF\n",
      "INFO:absl:Successful: Saved model at /tmp/tf_transformers_cache/gpt2/ckpt-1\n",
      "INFO:absl:Successful: Asserted and Converted `gpt2` from HF and saved it in cache folder /tmp/tf_transformers_cache/gpt2\n"
     ]
    }
   ],
   "source": [
    "# Check PT HF conversion\n",
    "!rm -rf /tmp/tf_transformers_cache/gpt2/\n",
    "model_name = 'gpt2'\n",
    "model, conifg = GPT2Model.get_model(model_name=model_name, convert_fn_type='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2554fc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30cd0fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Successful: Converted model using TF HF\n",
      "INFO:absl:Successful: Saved model at /tmp/tf_transformers_cache/gpt2/ckpt-1\n",
      "INFO:absl:Successful: Asserted and Converted `gpt2` from HF and saved it in cache folder /tmp/tf_transformers_cache/gpt2\n"
     ]
    }
   ],
   "source": [
    "# Check TF HF conversion\n",
    "!rm -rf /tmp/tf_transformers_cache/gpt2/\n",
    "model_name = 'gpt2'\n",
    "model, conifg = GPT2Model.get_model(model_name=model_name, convert_fn_type='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd71d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c676bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b334a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation without caching\n",
    "\n",
    "text = 'Sachin Tendulkar is one of the finest'\n",
    "inputs_tf = tokenizer(text, return_tensors='tf')\n",
    "inputs = {}\n",
    "inputs['input_ids'] = inputs_tf['input_ids']\n",
    "\n",
    "predictions_non_auto_regressive = []\n",
    "predictions_prob_non_auto_regressive = []\n",
    "\n",
    "for i in range(10):\n",
    "    outputs = model(inputs)\n",
    "    predicted_ids = tf.cast(tf.expand_dims(tf.argmax(outputs[\"last_token_logits\"], axis=1), 1), tf.int32)\n",
    "    inputs[\"input_ids\"] = tf.concat([inputs[\"input_ids\"], predicted_ids], axis=1)\n",
    "    predictions_non_auto_regressive.append(predicted_ids)\n",
    "    predictions_prob_non_auto_regressive.append(\n",
    "        tf.expand_dims(tf.reduce_max(outputs[\"last_token_logits\"], axis=1), 1)\n",
    "    )\n",
    "predictions_non_auto_regressive = tf.concat(predictions_non_auto_regressive, axis=1)\n",
    "predictions_prob_non_auto_regressive = tf.concat(predictions_prob_non_auto_regressive, axis=1)\n",
    "\n",
    "\n",
    "# Text generation with caching\n",
    "\n",
    "model, conifg = GPT2Model.get_model(model_name=model_name, use_auto_regressive=True)\n",
    "inputs_tf = tokenizer(text, return_tensors='tf')\n",
    "inputs = {}\n",
    "inputs['input_ids'] = inputs_tf['input_ids']\n",
    "\n",
    "seq_length = tf.shape(input_ids)[1]\n",
    "batch_size = tf.shape(input_ids)[0]\n",
    "\n",
    "inputs[\"all_cache_key\"] = tf.zeros((12, batch_size, 12, seq_length, 64))\n",
    "inputs[\"all_cache_value\"] = tf.zeros((12, batch_size, 12, seq_length, 64))\n",
    "inputs[\"past_length\"] = tf.zeros(shape=(1, batch_size), dtype=tf.int32)\n",
    "predictions_auto_regressive = []\n",
    "predictions_prob_auto_regressive = []\n",
    "\n",
    "past_lengths = []\n",
    "for i in range(10):\n",
    "    outputs = model(inputs)\n",
    "    predicted_ids = tf.cast(tf.expand_dims(tf.argmax(outputs[\"last_token_logits\"], axis=1), 1), tf.int32)\n",
    "    inputs[\"input_ids\"] = predicted_ids\n",
    "    inputs[\"all_cache_key\"] = outputs[\"all_cache_key\"]\n",
    "    inputs[\"all_cache_value\"] = outputs[\"all_cache_value\"]\n",
    "    inputs[\"past_length\"] = outputs[\"past_length\"]\n",
    "    past_lengths.append(inputs[\"past_length\"])\n",
    "    predictions_auto_regressive.append(predicted_ids)\n",
    "    predictions_prob_auto_regressive.append(\n",
    "        tf.expand_dims(tf.reduce_max(outputs[\"last_token_logits\"], axis=1), 1)\n",
    "    )\n",
    "predictions_auto_regressive = tf.concat(predictions_auto_regressive, axis=1)\n",
    "predictions_prob_auto_regressive = tf.concat(predictions_prob_auto_regressive, axis=1)\n",
    "\n",
    "tf.assert_equal(predictions_non_auto_regressive, predictions_auto_regressive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dcabff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72832b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch predictions (var len)\n",
    "\n",
    "text = ['Sachin Tendulkar is one of the finest', \n",
    "        'I love stars because']\n",
    "# Batching using -1 is must\n",
    "input_ids = tf.ragged.constant(tokenizer(text)['input_ids']).to_tensor(-1)\n",
    "inputs = {}\n",
    "inputs['input_ids'] = input_ids\n",
    "\n",
    "seq_length = tf.shape(input_ids)[1]\n",
    "batch_size = tf.shape(input_ids)[0]\n",
    "\n",
    "inputs[\"all_cache_key\"] = tf.zeros((12, batch_size, 12, seq_length, 64))\n",
    "inputs[\"all_cache_value\"] = tf.zeros((12, batch_size, 12, seq_length, 64))\n",
    "inputs[\"past_length\"] = tf.zeros(shape=(1, batch_size), dtype=tf.int32)\n",
    "predictions_auto_regressive = []\n",
    "predictions_prob_auto_regressive = []\n",
    "\n",
    "past_lengths = []\n",
    "for i in range(10):\n",
    "    outputs = model(inputs)\n",
    "    predicted_ids = tf.cast(tf.expand_dims(tf.argmax(outputs[\"last_token_logits\"], axis=1), 1), tf.int32)\n",
    "    \n",
    "    if i == 0:\n",
    "        masks = tf.cast(tf.not_equal(input_ids, -1), tf.float32)\n",
    "        masks = tf.reshape(\n",
    "            masks,\n",
    "            (1, batch_size, 1, seq_length, 1),\n",
    "        )\n",
    "        outputs[\"all_cache_key\"]   = outputs[\"all_cache_key\"]   * masks\n",
    "        outputs[\"all_cache_value\"] = outputs[\"all_cache_value\"] * masks\n",
    "        \n",
    "    inputs[\"input_ids\"] = predicted_ids\n",
    "    inputs[\"all_cache_key\"] = outputs[\"all_cache_key\"]\n",
    "    inputs[\"all_cache_value\"] = outputs[\"all_cache_value\"]\n",
    "    inputs[\"past_length\"] = outputs[\"past_length\"]\n",
    "    past_lengths.append(inputs[\"past_length\"])\n",
    "    predictions_auto_regressive.append(predicted_ids)\n",
    "    predictions_prob_auto_regressive.append(\n",
    "        tf.expand_dims(tf.reduce_max(outputs[\"last_token_logits\"], axis=1), 1)\n",
    "    )\n",
    "    \n",
    "    \n",
    "predictions_auto_regressive = tf.concat(predictions_auto_regressive, axis=1)\n",
    "predictions_prob_auto_regressive = tf.concat(predictions_prob_auto_regressive, axis=1)\n",
    "expected_ids = [[1938, 287, 262, 995, 13, 679, 318, 257, 845, 922],\n",
    "                 [484, 821, 523, 881, 517, 621, 655, 257, 3491, 13]]\n",
    "np.allclose(predictions_auto_regressive.numpy().tolist(), expected_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501282ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1762e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation using saved_model with TextDecoder\n",
    "\n",
    "import tempfile\n",
    "import shutil\n",
    "from tf_transformers.text import TextDecoder\n",
    "\n",
    "text = ['Sachin Tendulkar is one of the finest', \n",
    "        'I love stars because']\n",
    "\n",
    "saved_model_dir = tempfile.mkdtemp()\n",
    "model.save_as_serialize_module(saved_model_dir, overwrite=True)\n",
    "\n",
    "loaded   = tf.saved_model.load(saved_model_dir)\n",
    "decoder  = TextDecoder(\n",
    "    model = loaded\n",
    ")\n",
    "\n",
    "# -1 batch\n",
    "input_ids = tf.ragged.constant(tokenizer(text)['input_ids']).to_tensor(-1)\n",
    "inputs = {}\n",
    "inputs['input_ids'] = input_ids\n",
    "\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='greedy', \n",
    "               max_iterations=10, \n",
    "               eos_id=-100)\n",
    "expected_ids = [[1938, 287, 262, 995, 13, 679, 318, 257, 845, 922],\n",
    "                 [484, 821, 523, 881, 517, 621, 655, 257, 3491, 13]]\n",
    "np.allclose(decoder_results['predicted_ids'].numpy().tolist(), \n",
    "           expected_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291850bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5ad79e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation using saved_model with TextDecoderSerializable\n",
    "\n",
    "import tempfile\n",
    "import shutil\n",
    "from tf_transformers.text import TextDecoderSerializable\n",
    "\n",
    "text = ['Sachin Tendulkar is one of the finest', \n",
    "        'I love stars because']\n",
    "\n",
    "saved_model_dir = tempfile.mkdtemp()\n",
    "model.save_as_serialize_module(saved_model_dir, overwrite=True)\n",
    "\n",
    "loaded   = tf.saved_model.load(saved_model_dir)\n",
    "decoder  = TextDecoderSerializable(\n",
    "    model = model,\n",
    "    max_iterations=10,\n",
    "    mode=\"greedy\",\n",
    "    do_sample=False,\n",
    "    eos_id=-100\n",
    ")\n",
    "\n",
    "# Save\n",
    "decoder_model = decoder.get_model()\n",
    "decoder_model.save_serialized(saved_model_dir, overwrite=True)\n",
    "\n",
    "# Load\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "# -1 batch\n",
    "input_ids = tf.ragged.constant(tokenizer(text)['input_ids']).to_tensor(-1)\n",
    "inputs = {}\n",
    "inputs['input_ids'] = input_ids\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs)\n",
    "expected_ids = [[[1938, 287, 262, 995, 13, 679, 318, 257, 845, 922]],\n",
    "               [[484, 821, 523, 881, 517, 621, 655, 257, 3491, 13]]]\n",
    "np.allclose(decoder_results_serialized['predicted_ids'].numpy().tolist(), \n",
    "           expected_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e205d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "803847a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.models import EncoderDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18396c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer, encoder_config = GPT2Model.get_model(model_name=model_name,\n",
    "                                                    mask_mode='user_defined',\n",
    "                                                    return_layer=True\n",
    "                                                    )\n",
    "decoder_layer, decoder_config = GPT2Model.get_model(model_name=model_name, \n",
    "                                                    return_layer=True,  \n",
    "                                                    use_decoder=True)\n",
    "\n",
    "# Decoder layer wont load from checkpoint\n",
    "# As the graph is different\n",
    "\n",
    "# Get decoder variables index and name as dict\n",
    "# Assign encoder weights to decoder wherever it matches variable name\n",
    "num_assigned = 0\n",
    "decoder_var = {var.name: index for index, var in enumerate(decoder_layer.variables)}\n",
    "for encoder_var in encoder_layer.variables:\n",
    "    if encoder_var.name in decoder_var:\n",
    "        index = decoder_var[encoder_var.name]\n",
    "        decoder_layer.variables[index].assign(encoder_var)\n",
    "        num_assigned += 1\n",
    "        \n",
    "model = EncoderDecoder(encoder_layer, decoder_layer, share_embeddings=True, share_encoder=True)\n",
    "\n",
    "# Check encoder decoder generation without caching\n",
    "\n",
    "text = 'Sachin Tendulkar is one of the finest'\n",
    "encoder_input_ids = tf.expand_dims(tf.ragged.constant(tokenizer(text)['input_ids']), 0)\n",
    "encoder_input_mask = tf.ones_like(encoder_input_ids)\n",
    "decoder_input_ids  = tf.constant([[1]])\n",
    "\n",
    "inputs = {}\n",
    "inputs['encoder_input_ids'] = encoder_input_ids\n",
    "inputs['encoder_input_mask']= encoder_input_mask\n",
    "inputs['decoder_input_ids'] = decoder_input_ids\n",
    "\n",
    "predictions_non_auto_regressive = []\n",
    "predictions_prob_non_auto_regressive = []\n",
    "\n",
    "for i in range(10):\n",
    "    outputs = model(inputs)\n",
    "    predicted_ids = tf.cast(tf.expand_dims(tf.argmax(outputs[\"last_token_logits\"], axis=1), 1), tf.int32)\n",
    "    inputs[\"encoder_input_ids\"] = tf.concat([inputs[\"encoder_input_ids\"], predicted_ids], axis=1)\n",
    "    inputs[\"encoder_input_mask\"] = tf.ones_like(inputs[\"encoder_input_ids\"])\n",
    "    predictions_non_auto_regressive.append(predicted_ids)\n",
    "    predictions_prob_non_auto_regressive.append(\n",
    "        tf.expand_dims(tf.reduce_max(outputs[\"last_token_logits\"], axis=1), 1)\n",
    "    )\n",
    "predictions_non_auto_regressive = tf.concat(predictions_non_auto_regressive, axis=1)\n",
    "predictions_prob_non_auto_regressive = tf.concat(predictions_prob_non_auto_regressive, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93bb7138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74a00f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/gpt2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.gpt2.gpt2.GPT2Encoder object at 0x7fe9c6d584f0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7fe9af567d60>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.gpt2.gpt2.GPT2Encoder object at 0x7fe9c6d584f0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7fe9af567d60>).\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/gpt2\n"
     ]
    }
   ],
   "source": [
    "encoder_layer, encoder_config = GPT2Model.get_model(model_name=model_name,\n",
    "                                                    mask_mode='user_defined',\n",
    "                                                    return_layer=True\n",
    "                                                    )\n",
    "decoder_layer, decoder_config = GPT2Model.get_model(model_name=model_name, \n",
    "                                                    return_layer=True,  \n",
    "                                                    use_decoder=True, \n",
    "                                                    use_auto_regressive=True)\n",
    "\n",
    "# Decoder layer wont load from checkpoint\n",
    "# As the graph is different\n",
    "\n",
    "# Get decoder variables index and name as dict\n",
    "# Assign encoder weights to decoder wherever it matches variable name\n",
    "num_assigned = 0\n",
    "decoder_var = {var.name: index for index, var in enumerate(decoder_layer.variables)}\n",
    "for encoder_var in encoder_layer.variables:\n",
    "    if encoder_var.name in decoder_var:\n",
    "        index = decoder_var[encoder_var.name]\n",
    "        decoder_layer.variables[index].assign(encoder_var)\n",
    "        num_assigned += 1\n",
    "        \n",
    "model = EncoderDecoder(encoder_layer, decoder_layer, share_embeddings=True, share_encoder=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check encoder decoder generation  caching\n",
    "\n",
    "encoder_hidden_dim = encoder_config['embedding_size']\n",
    "num_hidden_layers  = decoder_config['num_hidden_layers']\n",
    "num_attention_heads = decoder_config['num_attention_heads']\n",
    "attention_head_size = decoder_config['attention_head_size']\n",
    "\n",
    "\n",
    "\n",
    "text = 'Sachin Tendulkar is one of the finest'\n",
    "encoder_input_ids = tf.expand_dims(tf.ragged.constant(tokenizer(text)['input_ids']), 0)\n",
    "encoder_input_mask = tf.ones_like(encoder_input_ids)\n",
    "decoder_input_ids  = tf.constant([[1]])\n",
    "\n",
    "batch_size = tf.shape(encoder_input_ids)[0]\n",
    "seq_length = tf.shape(encoder_input_ids)[1]\n",
    "\n",
    "encoder_hidden_states = tf.zeros((batch_size, seq_length, 768))\n",
    "decoder_all_cache_key = tf.zeros((num_hidden_layers, \n",
    "                                  batch_size, \n",
    "                                  num_attention_heads, \n",
    "                                  seq_length, \n",
    "                                  attention_head_size))\n",
    "decoder_all_cahce_value = tf.zeros((num_hidden_layers, \n",
    "                                  batch_size, \n",
    "                                  num_attention_heads, \n",
    "                                  seq_length, \n",
    "                                  attention_head_size))\n",
    "\n",
    "\n",
    "inputs = {}\n",
    "inputs['encoder_input_ids'] = encoder_input_ids\n",
    "inputs['encoder_input_mask']= encoder_input_mask\n",
    "inputs['decoder_input_ids'] = decoder_input_ids\n",
    "inputs['encoder_hidden_states'] = encoder_hidden_states\n",
    "inputs['decoder_all_cache_key'] = decoder_all_cache_key\n",
    "inputs['decoder_all_cache_value'] = decoder_all_cahce_value\n",
    "\n",
    "\n",
    "predictions_auto_regressive = []\n",
    "predictions_prob_auto_regressive = []\n",
    "\n",
    "for i in range(10):\n",
    "    outputs = model(inputs)\n",
    "    predicted_ids = tf.cast(tf.expand_dims(tf.argmax(outputs[\"last_token_logits\"], axis=1), 1), tf.int32)\n",
    "    inputs[\"input_ids\"] = predicted_ids\n",
    "    inputs[\"decoder_all_cache_key\"] = outputs[\"decoder_all_cache_key\"]\n",
    "    inputs[\"decoder_all_cache_value\"] = outputs[\"decoder_all_cache_value\"]\n",
    "    inputs[\"encoder_hidden_states\"] = outputs[\"encoder_hidden_states\"]\n",
    "    predictions_auto_regressive.append(predicted_ids)\n",
    "    predictions_prob_auto_regressive.append(\n",
    "        tf.expand_dims(tf.reduce_max(outputs[\"last_token_logits\"], axis=1), 1)\n",
    "    )\n",
    "predictions_auto_regressive = tf.concat(predictions_auto_regressive, axis=1)\n",
    "predictions_prob_auto_regressive = tf.concat(predictions_prob_auto_regressive, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a849f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48047e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba95c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
