{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/PRVATE/Documents/tf_transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from transformers import TFGPT2Model\n",
    "from tf_transformers.models import GPT2Encoder\n",
    "\n",
    "from tf_transformers.core import LegacyModule\n",
    "\n",
    "from tf_transformers.utils import convert_gpt2_hf_to_tf_transformers\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load HuggingFace Model\n",
    "\n",
    "We will load huggingface Model and use it to assign variables to the tf_transformers model.\n",
    "\n",
    "\n",
    "We will be using `convert_gpt2_hf_to_tf_transformers` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2Model.\n",
      "\n",
      "All the layers of TFGPT2Model were initialized from the model checkpoint at /Users/PRVATE/HUggingFace_Models/gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load HF model\n",
    "# Always do this\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "local_dir = \"/Users/PRVATE/HUggingFace_Models/\"\n",
    "hf_model_name = \"gpt2\"\n",
    "if local_dir:\n",
    "    hf_model_location = local_dir + hf_model_name\n",
    "\n",
    "model_hf = TFGPT2Model.from_pretrained(hf_model_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tf_transformers Model\n",
    "\n",
    "Configs are in the `model_configs` folder in the root of the repo.\n",
    "\n",
    "\n",
    "We will be using `convert_albert_hf_to_tf_transformers` function.\n",
    "\n",
    "Always use `is_training=False` to load the model and pass this model for conversion.\n",
    "\n",
    "Do not enablle `pipeline_mode='auto-regressive` while converting. Because, variable name\n",
    "\n",
    "differs due to `tf.cond` usage. \n",
    "\n",
    "# Steps:\n",
    "\n",
    "1. Load a model using **`is_training=False`**\n",
    "\n",
    "2. Convert it using conversion functions from `tf_transformers.utils`\n",
    "\n",
    "3. Save the `checkpoint` .\n",
    "\n",
    "4. For auto-regressive tasks (text generation) use **`pipeline_mode='auto-regressive`**\n",
    "\n",
    "   along with **`is_training=False`** and load from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:We are overwriding `is_training` is False to `is_training`                     to True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids_1:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Deleteing huggingface model for saving memory\n",
      "INFO:absl:Done assigning variables weights . Total 100\n"
     ]
    }
   ],
   "source": [
    "# Load tf_transformers model\n",
    "# Most config we will be providing\n",
    "\n",
    "# Default configs for the model\n",
    "\n",
    "model_config_dir = '/Users/PRVATE/Documents/tf_transformers/model_configs/'\n",
    "model_name = 'gpt2_base'\n",
    "config_location = os.path.join(model_config_dir, model_name, 'config.json')\n",
    "config = json.load(open(config_location))\n",
    "\n",
    "# Always do this\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# tf_transformers Layer (an extension of Keras Layer)\n",
    "# This is not Keras model, but extension of keras Layer\n",
    "\n",
    "# Save as saved_model\n",
    "# If you want to use the model for Auto Regressive tasks ( text-generation ),\n",
    "# you have to enable pipeline_mode='auto-regressive'.\n",
    "# Because TF needs extra cache inputs in the saved_model format for doing efficient caching\n",
    "\n",
    "model_layer = GPT2Encoder(\n",
    "    config=config,\n",
    "    name=\"gpt2\",\n",
    "    mask_mode=config[\"mask_mode\"],\n",
    "    is_training=False,\n",
    ")\n",
    "\n",
    "# Convert to tf.keras.Model\n",
    "model_tf_transformers = model_layer.get_and_load_model(model_dir=None)\n",
    "convert_gpt2_hf_to_tf_transformers(model_hf, model_tf_transformers, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved at model_ckpt/ckpt-1\n"
     ]
    }
   ],
   "source": [
    "# If you want to save the model as checkpoints\n",
    "\n",
    "checkpoint_dir = 'model_ckpt'\n",
    "checkpoint = tf.train.Checkpoint(model=model_tf_transformers)\n",
    "manager = tf.train.CheckpointManager(\n",
    "    checkpoint, directory=checkpoint_dir, max_to_keep=1\n",
    ")\n",
    "manager.save()\n",
    "print(\"Saved at {}\".format(manager.latest_checkpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for reference\n",
    "\n",
    "Have a look at `tf_transformers/extra/*.py` for reference values, to make sure model\n",
    "\n",
    "has loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embeddings --> tf.Tensor(2371.2754, shape=(), dtype=float32) --> (2, 5, 768)\n",
      "token_logits --> tf.Tensor(-34781264.0, shape=(), dtype=float32) --> (2, 5, 50257)\n",
      "last_token_logits --> tf.Tensor(-8346981.0, shape=(), dtype=float32) --> (2, 50257)\n"
     ]
    }
   ],
   "source": [
    "# Please have a look at tf_transformers/extra/*.py for reference values\n",
    "\n",
    "input_ids = tf.constant([[1, 9, 10, 11, 23], [1, 22, 234, 432, 2349]])\n",
    "input_mask = tf.ones_like(input_ids)\n",
    "input_type_ids = tf.ones_like(input_ids)\n",
    "\n",
    "inputs = {\n",
    "    \"input_ids\": input_ids,\n",
    "}\n",
    "\n",
    "results_tf_transformers = model_tf_transformers(inputs)\n",
    "for k, r in results_tf_transformers.items():\n",
    "    if isinstance(r, list):\n",
    "        continue\n",
    "    print(k, \"-->\", tf.reduce_sum(r), \"-->\", r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model for Auto-Regressive tasks\n",
    "\n",
    "**Text generation / Auto regressive decoding*** requires caching of `K` and `V` values.\n",
    "\n",
    "This, means for the model to make use of serialization, thsese values has to be a part of the model.\n",
    "\n",
    "So, K and V are extra inputs required for inference **(only in the case of text generation)**.\n",
    "\n",
    "As a result, training and testing needs different pipleines (for auto regressive tasks)\n",
    "\n",
    "**`Note: We have necessary wrappers to do all these, user doesnt has to worry about any of these`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids_2:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:past_length ---> Tensor(\"past_length:0\", shape=(1, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids_3:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:all_cache_key ---> Tensor(\"all_cache_key_1:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:all_cache_value ---> Tensor(\"all_cache_value_1:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:past_length ---> Tensor(\"past_length_1:0\", shape=(1, None), dtype=int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.gpt2.GPT2Encoder object at 0x13edbdee0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x13ed0bb80>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.gpt2.GPT2Encoder object at 0x13edbdee0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x13ed0bb80>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "model_layer = GPT2Encoder(\n",
    "    config=config,\n",
    "    name=\"gpt2\",\n",
    "    mask_mode=config[\"mask_mode\"],\n",
    "    is_training=False,\n",
    "    pipeline_mode=\"auto-regressive\",\n",
    ")\n",
    "\n",
    "# Convert to tf.keras.Model\n",
    "model_tf_transformers = model_layer.get_and_load_model(model_dir=None)\n",
    "\n",
    "# And now load the checkpints from previously saved model\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model_tf_transformers)\n",
    "manager = tf.train.CheckpointManager(\n",
    "    checkpoint, directory=checkpoint_dir, max_to_keep=1\n",
    ")\n",
    "status = checkpoint.restore(manager.latest_checkpoint)\n",
    "\n",
    "# Important\n",
    "if status.assert_existing_objects_matched():\n",
    "    print(\"Model checkpoints matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model .pb (saved_model)\n",
    "\n",
    "To make use of the benefits of serialization, we have to save the model.\n",
    "\n",
    "Now, why don't `model_tf_transformers.save(\"model_pb\", save_format='tf')` . \n",
    "\n",
    "Reason is when we save the model using above, TF will somehow ignore the proper output node names.\n",
    "\n",
    "It will assign some random names like `['gpt_output1, ect...]`. \n",
    "\n",
    "To preserve the names in the `saved_model` , we have small wrapper function called `LegacyModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model to .pb for make use of proper serialization\n",
    "saved_model_dir = \"model_pb\"\n",
    "tf_transformers_module = LegacyModule(model_tf_transformers)\n",
    "tf_transformers_module.save(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark models\n",
    "\n",
    "We will benchmark on gpt2 model with following approaches.\n",
    "\n",
    "1. Use the model in `saved_model_dir` with **`TextDecoder`** API. This API will consume the model,\n",
    "\n",
    "   **(suppprts `saved_model`, `tf.keras.Model`, `hub.KerasLayer`)** . Recommended is `saved_model` \n",
    "   \n",
    "   or `hub.KerasLayer` . **`TextDecoder`** API is **pure python function, which has for loops for decoding\n",
    "   \n",
    "   \n",
    "2. We will use **`TextDecoderSerializable`** . This `tf.while_loop` implementation . The advantage is\n",
    "\n",
    "   we can serialize the entire operation as it is, so you decoding method will be a part of `saved_model`\n",
    "   \n",
    "   graph.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.text import TextDecoder\n",
    "from tf_transformers.text import TextDecoderSerializable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Greedy Decoding (serialized model)\n",
    "\n",
    "1. We can do, `model=beam` and `model=top_k_top_p` for Beam search and top K nucleus sampling\n",
    "\n",
    "   respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:540: UserWarning: Input dict contained keys ['iterations'] which did not match any model input. They will be ignored by the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_greedy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_greedy/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Save the model for greedy decoding\n",
    "saved_model_dir_strategy = 'model_pb_greedy'\n",
    "\n",
    "decoder_layer = TextDecoderSerializable(\n",
    "    model_tf_transformers,\n",
    "    max_iterations=None,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    hidden_dimension=768,\n",
    "    mode=\"greedy\",\n",
    "    do_sample=False,\n",
    "    eos_id=-100,\n",
    ")\n",
    "# Convert whole operation to a model\n",
    "decoder_model  = decoder_layer.get_model()\n",
    "\n",
    "\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(saved_model_dir_strategy)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "# reser variables\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1398"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Garbage collection\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and benchmark \n",
    "\n",
    "\n",
    "from functools import wraps\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "average_runs = 4\n",
    "\n",
    "def timeit(my_func):\n",
    "    @wraps(my_func)\n",
    "    def timed(*args, **kw):\n",
    "        time_list = []\n",
    "        for i in range(3):\n",
    "          tstart = time.time()\n",
    "          output = my_func(*args, **kw)\n",
    "          tend = time.time()\n",
    "          time_taken = tend-tstart\n",
    "          time_list.append((tend - tstart))\n",
    "          \n",
    "        return np.mean(time_list[1:]) # return last 3\n",
    "    return timed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_transformers.text import TextDecoder\n",
    "\n",
    "saved_model_dir = \"model_pb\"\n",
    "loaded = tf.saved_model.load(saved_model_dir)\n",
    "model = loaded.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_results = {}\n",
    "batch_size_list = [1, 3, 5, 10]\n",
    "max_seq_length_list = [15, 25, 50, 100, 150]\n",
    "\n",
    "decoder_layer = TextDecoder(\n",
    "    model=model,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    hidden_dimension=768,\n",
    "    input_mask_ids=None,\n",
    "    input_type_ids=None\n",
    ")\n",
    "\n",
    "@timeit\n",
    "def call_greedy(input_dict, length):\n",
    "    result_greedy = decoder_layer.decode(input_dict, \n",
    "                                       mode='greedy', \n",
    "                                       eos_id=-100, \n",
    "                                       max_iterations=length, \n",
    "                                       do_sample=False)\n",
    "    return result_greedy\n",
    "\n",
    "minval = 1\n",
    "maxval = 50256\n",
    "input_tensor_length = 100\n",
    "\n",
    "timing_holder = {}\n",
    "for batch in batch_size_list:\n",
    "  for sequence_length in max_seq_length_list:\n",
    "    input_tensor = tf.random.uniform(minval=minval, maxval=maxval, shape=(batch,input_tensor_length), dtype=tf.int32)\n",
    "    # because we need to genrate that much tokens from input_tensor_length\n",
    "    input_dict = {'input_ids': input_tensor}\n",
    "    average_time_taken = call_greedy(input_dict, sequence_length)\n",
    "    timing_holder[(batch, sequence_length)] = average_time_taken\n",
    "    print(\"Done batch_size {} sequence length {}\".format(batch, sequence_length))\n",
    "\n",
    "    \n",
    "timing_results['greedy'] = timing_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_list = [1, 3, 5, 10]\n",
    "max_seq_length_list = [15, 25, 50, 100, 150]\n",
    "beam_list = [1, 2, 5, 7, 10]\n",
    "\n",
    "decoder_layer = TextDecoder(\n",
    "    model=model,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    hidden_dimension=768,\n",
    "    input_mask_ids=None,\n",
    "    input_type_ids=None\n",
    ")\n",
    "\n",
    "@timeit\n",
    "def call_beam(input_dict, length, beam):\n",
    "    result = decoder_layer.decode(input_dict, \n",
    "                                       mode='beam', \n",
    "                                       eos_id=-100, \n",
    "                                       max_iterations=length,\n",
    "                                       beam_size = beam,\n",
    "                                       do_sample=False)\n",
    "    print(result['predicted_ids'].shape)\n",
    "    return result\n",
    "\n",
    "minval = 1\n",
    "maxval = 50256\n",
    "input_tensor_length = 100\n",
    "\n",
    "timing_holder = {}\n",
    "for batch in batch_size_list:\n",
    "  for sequence_length in max_seq_length_list:\n",
    "    for beam in beam_list:\n",
    "      input_tensor = tf.random.uniform(minval=minval, maxval=maxval, shape=(batch,input_tensor_length), dtype=tf.int32)\n",
    "      # because we need to genrate that much tokens from input_tensor_length\n",
    "      input_dict = {'input_ids': input_tensor}\n",
    "      average_time_taken = call_beam(input_dict, sequence_length, beam)\n",
    "      timing_holder['{}_{}'.format(batch, sequence_length)] = average_time_taken\n",
    "      timing_holder[(batch, sequence_length, beam)] = average_time_taken\n",
    "      print(\"Done batch_size {} sequence length {} beam size {}\".format(batch, sequence_length, beam))\n",
    "\n",
    "    \n",
    "#timing_results['beam'] = timing_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top K top P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_list = [1, 3, 5, 10]\n",
    "max_seq_length_list = [15, 25, 50, 100, 150]\n",
    "num_return_sequence_list = [1, 2, 5, 7, 10]\n",
    "\n",
    "decoder_layer = TextDecoder(\n",
    "    model=model,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    hidden_dimension=768,\n",
    "    input_mask_ids=None,\n",
    "    input_type_ids=None\n",
    ")\n",
    "\n",
    "@timeit\n",
    "def call_top_k_top_p(input_dict, length, beam):\n",
    "    result = decoder_layer.decode(input_dict, \n",
    "                                       mode='top_k_top_p', \n",
    "                                       eos_id=-100, \n",
    "                                       max_iterations=length,\n",
    "                                       num_return_sequences = beam,\n",
    "                                       top_k = 25,\n",
    "                                       top_p = 0.75,\n",
    "                                       do_sample=True)\n",
    "    return result\n",
    "\n",
    "minval = 1\n",
    "maxval = 50256\n",
    "input_tensor_length = 100\n",
    "\n",
    "timing_holder = {}\n",
    "for batch in batch_size_list:\n",
    "  for sequence_length in max_seq_length_list:\n",
    "    for num_return_sequence in num_return_sequence_list:\n",
    "      input_tensor = tf.random.uniform(minval=minval, maxval=maxval, shape=(batch,input_tensor_length), dtype=tf.int32)\n",
    "      # because we need to genrate that much tokens from input_tensor_length\n",
    "      input_dict = {'input_ids': input_tensor}\n",
    "      average_time_taken = call_top_k_top_p(input_dict, sequence_length, num_return_sequence)\n",
    "      #timing_holder['{}_{}'.format(batch, sequence_length)] = average_time_taken\n",
    "      timing_holder[(batch, sequence_length, num_return_sequence)] = average_time_taken\n",
    "      print(\"Done batch_size {} sequence length {}\".format(batch, sequence_length))\n",
    "\n",
    "    \n",
    "timing_results['top_k_top_p'] = timing_holder\n",
    "\n",
    "import pickle\n",
    "with open(\"tf_transformers_gpt2_benchmark.pkl\", \"wb\") as f:\n",
    "    pickle.dump(timing_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used completely serialized saved_model for Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = \"model_pb_greedy/\"\n",
    "loaded = tf.saved_model.load(saved_model_dir)\n",
    "model = loaded.signatures['serving_default']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "timing_results = {}\n",
    "batch_size_list = [1, 3, 5, 10]\n",
    "max_seq_length_list = [15, 25, 50, 100, 150]\n",
    "\n",
    "\n",
    "@timeit\n",
    "def call_greedy(input_dict):\n",
    "    result_greedy = model(**input_dict)\n",
    "    return result_greedy\n",
    "\n",
    "minval = 1\n",
    "maxval = 50256\n",
    "input_tensor_length = 100\n",
    "\n",
    "timing_holder = {}\n",
    "for batch in batch_size_list:\n",
    "  for sequence_length in max_seq_length_list:\n",
    "    input_tensor = tf.random.uniform(minval=minval, maxval=maxval, shape=(batch,input_tensor_length), dtype=tf.int32)\n",
    "    # because we need to genrate that much tokens from input_tensor_length\n",
    "    input_dict = {'input_ids': input_tensor}\n",
    "    input_dict['iterations'] = tf.constant([[sequence_length]])\n",
    "    average_time_taken = call_greedy(input_dict)\n",
    "    timing_holder[(batch, sequence_length)] = average_time_taken\n",
    "    print(\"Done batch_size {} sequence length {}\".format(batch, sequence_length))\n",
    "\n",
    "    \n",
    "timing_results['greedy'] = timing_holder\n",
    "\n",
    "import pickle\n",
    "with open(\"tf_transformers_gpt2_benchmark_serializable.pkl\", \"wb\") as f:\n",
    "    pickle.dump(timing_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
