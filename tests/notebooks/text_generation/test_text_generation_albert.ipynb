{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/PRVATE/Documents/tf_transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from transformers import TFAlbertModel\n",
    "from tf_transformers.models import AlbertEncoder\n",
    "\n",
    "from tf_transformers.core import LegacyModule\n",
    "from transformers import AlbertTokenizer\n",
    "\n",
    "from tf_transformers.utils import convert_albert_hf_to_tf_transformers\n",
    "\n",
    "from tf_transformers.data import pad_dataset\n",
    "from tf_transformers.text import TextDecoder\n",
    "from tf_transformers.text import TextDecoderSerializable\n",
    "\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/PRVATE/Documents/tf_transformers/src/tf_transformers/models/albert.py'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getfile(AlbertEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load HuggingFace Model\n",
    "\n",
    "We will load huggingface Model and use it to assign variables to the tf_transformers model.\n",
    "\n",
    "\n",
    "We will be using `convert_albert_hf_to_tf_transformers` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFAlbertModel.\n",
      "\n",
      "All the layers of TFAlbertModel were initialized from the model checkpoint at /Users/PRVATE/HUggingFace_Models/albert-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load HF model\n",
    "# Always do this\n",
    "\n",
    "\n",
    "local_dir = \"/Users/PRVATE/HUggingFace_Models/\"\n",
    "hf_model_name = \"albert-base-v2\"\n",
    "if local_dir:\n",
    "    hf_model_location = local_dir + hf_model_name\n",
    "\n",
    "model_hf = TFAlbertModel.from_pretrained(hf_model_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tf_transformers Model\n",
    "\n",
    "Configs are in the `model_configs` folder in the root of the repo.\n",
    "\n",
    "\n",
    "We will be using `convert_albert_hf_to_tf_transformers` function.\n",
    "\n",
    "Always use `is_training=False` to load the model and pass this model for conversion.\n",
    "\n",
    "Do not enablle `pipeline_mode='auto-regressive` while converting. Because, variable name\n",
    "\n",
    "differs due to `tf.cond` usage. \n",
    "\n",
    "# Steps:\n",
    "\n",
    "1. Load a model using **`is_training=False`**\n",
    "\n",
    "2. Convert it using conversion functions from `tf_transformers.utils`\n",
    "\n",
    "3. Save the `checkpoint` .\n",
    "\n",
    "4. For auto-regressive tasks (text generation) use **`pipeline_mode='auto-regressive`**\n",
    "\n",
    "   along with **`is_training=False`** and load from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:We are overwriding `is_training` is False to `is_training` to                     True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_type_ids ---> Tensor(\"input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids_1:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_mask ---> Tensor(\"input_mask_1:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_type_ids ---> Tensor(\"input_type_ids_1:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Deleteing huggingface model for saving memory\n",
      "INFO:absl:Done assigning variables weights. Total 25\n"
     ]
    }
   ],
   "source": [
    "# Load tf_transformers model\n",
    "# Most config we will be providing\n",
    "\n",
    "# Default configs for the model\n",
    "\n",
    "model_config_dir = '/Users/PRVATE/Documents/tf_transformers/model_configs/'\n",
    "model_name = 'albert_base_v2'\n",
    "config_location = os.path.join(model_config_dir, model_name, 'config.json')\n",
    "config = json.load(open(config_location))\n",
    "\n",
    "# Always do this\n",
    "\n",
    "\n",
    "# tf_transformers Layer (an extension of Keras Layer)\n",
    "# This is not Keras model, but extension of keras Layer\n",
    "\n",
    "# Save as saved_model\n",
    "# If you want to use the model for Auto Regressive tasks ( text-generation ),\n",
    "# you have to enable pipeline_mode='auto-regressive'.\n",
    "# Because TF needs extra cache inputs in the saved_model format for doing efficient caching\n",
    "\n",
    "model_layer = AlbertEncoder(\n",
    "    config=config,\n",
    "    name=\"albert\",\n",
    "    mask_mode=config[\"mask_mode\"],\n",
    "    is_training=False,\n",
    ")\n",
    "\n",
    "# Convert to tf.keras.Model\n",
    "model_tf_transformers = model_layer.get_and_load_model(model_dir=None)\n",
    "convert_albert_hf_to_tf_transformers(model_hf, model_tf_transformers, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved at model_ckpt/ckpt-1\n"
     ]
    }
   ],
   "source": [
    "# If you want to save the model as checkpoints\n",
    "\n",
    "checkpoint_dir = 'model_ckpt'\n",
    "checkpoint = tf.train.Checkpoint(model=model_tf_transformers)\n",
    "manager = tf.train.CheckpointManager(\n",
    "    checkpoint, directory=checkpoint_dir, max_to_keep=1\n",
    ")\n",
    "manager.save()\n",
    "print(\"Saved at {}\".format(manager.latest_checkpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for reference\n",
    "\n",
    "Have a look at `tf_transformers/extra/*.py` for reference values, to make sure model\n",
    "\n",
    "has loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_output --> tf.Tensor(12.337963, shape=(), dtype=float32) --> (2, 768)\n",
      "token_embeddings --> tf.Tensor(-193.53201, shape=(), dtype=float32) --> (2, 5, 768)\n",
      "token_logits --> tf.Tensor(-23480.06, shape=(), dtype=float32) --> (2, 5, 30000)\n",
      "last_token_logits --> tf.Tensor(-4466.8125, shape=(), dtype=float32) --> (2, 30000)\n"
     ]
    }
   ],
   "source": [
    "# Please have a look at tf_transformers/extra/*.py for reference values\n",
    "\n",
    "input_ids = tf.constant([[1, 9, 10, 11, 23], [1, 22, 234, 432, 2349]])\n",
    "input_mask = tf.ones_like(input_ids)\n",
    "input_type_ids = tf.ones_like(input_ids)\n",
    "\n",
    "inputs = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"input_mask\": input_mask,\n",
    "    \"input_type_ids\": input_type_ids,\n",
    "}\n",
    "\n",
    "results_tf_transformers = model_tf_transformers(inputs)\n",
    "for k, r in results_tf_transformers.items():\n",
    "    if isinstance(r, list):\n",
    "        continue\n",
    "    print(k, \"-->\", tf.reduce_sum(r), \"-->\", r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Albert for Auto-Regressive tasks\n",
    "\n",
    "**Text generation / Auto regressive decoding*** requires caching of `K` and `V` values.\n",
    "\n",
    "This, means for the model to make use of serialization, thsese values has to be a part of the model.\n",
    "\n",
    "So, K and V are extra inputs required for inference **(only in the case of text generation)**.\n",
    "\n",
    "As a result, training and testing needs different pipleines (for auto regressive tasks)\n",
    "\n",
    "**`Note: We have necessary wrappers to do all these, user doesnt has to worry about any of these`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids_2:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_mask ---> Tensor(\"input_mask_2:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_type_ids ---> Tensor(\"input_type_ids_2:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:past_length ---> Tensor(\"past_length:0\", shape=(1, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids_3:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_mask ---> Tensor(\"input_mask_3:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_type_ids ---> Tensor(\"input_type_ids_3:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:all_cache_key ---> Tensor(\"all_cache_key_1:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:all_cache_value ---> Tensor(\"all_cache_value_1:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:past_length ---> Tensor(\"past_length_1:0\", shape=(1, None), dtype=int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.albert.AlbertEncoder object at 0x145cc75b0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x145cdac70>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.albert.AlbertEncoder object at 0x145cc75b0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x145cdac70>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "model_layer = AlbertEncoder(\n",
    "    config=config,\n",
    "    name=\"albert\",\n",
    "    mask_mode=config[\"mask_mode\"],\n",
    "    is_training=False,\n",
    "    pipeline_mode=\"auto-regressive\",\n",
    ")\n",
    "\n",
    "# Convert to tf.keras.Model\n",
    "model_tf_transformers = model_layer.get_and_load_model(model_dir=None)\n",
    "\n",
    "# And now load the checkpints from previously saved model\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model_tf_transformers)\n",
    "manager = tf.train.CheckpointManager(\n",
    "    checkpoint, directory=checkpoint_dir, max_to_keep=1\n",
    ")\n",
    "status = checkpoint.restore(manager.latest_checkpoint)\n",
    "\n",
    "# Important\n",
    "if status.assert_existing_objects_matched():\n",
    "    print(\"Model checkpoints matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model .pb (saved_model)\n",
    "\n",
    "To make use of the benefits of serialization, we have to save the model.\n",
    "\n",
    "Now, why don't `model_tf_transformers.save(\"model_pb\", save_format='tf')` . \n",
    "\n",
    "Reason is when we save the model using above, TF will somehow ignore the proper output node names.\n",
    "\n",
    "It will assign some random names like `['gpt_output1, ect...]`. \n",
    "\n",
    "To preserve the names in the `saved_model` , we have small wrapper function called `LegacyModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model to .pb for make use of proper serialization\n",
    "saved_model_dir = \"model_pb\"\n",
    "tf_transformers_module = LegacyModule(model_tf_transformers)\n",
    "tf_transformers_module.save(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pad_dataset\n",
    "def tokenizer_fn(tokenizer, text_list):\n",
    "    \"\"\"Tokenizer fn should return a dict (no padding is required).\n",
    "    Make sure, you pass all primary keys required to the model\n",
    "\n",
    "    text_list: a list of text\n",
    "\n",
    "    {'input_ids': tf.constant([[1, 2]]),\n",
    "     'input_mask': tf.constant([[1, 1]]),\n",
    "     'input_type_ids': tf.constant([[1, 0]])}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    for text in text_list:\n",
    "        input_ids.append(tokenizer.encode(text))\n",
    "    input_ids = tf.ragged.constant(input_ids)\n",
    "    inputs = {\"input_ids\": input_ids}\n",
    "    inputs[\"input_mask\"] = tf.ones_like(input_ids).numpy().tolist()\n",
    "    inputs[\"input_type_ids\"] = tf.zeros_like(input_ids).numpy().tolist()\n",
    "    inputs[\"input_ids\"] = input_ids.numpy().tolist()\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer \n",
    "LegcayAI has in-built tokenizer. You can either use it. (Not recommended).\n",
    "\n",
    "The main difference is how we handle `SPECIAL TOKENS`. Apart from that its the same.\n",
    "\n",
    "Recommended use **HuggingFace tokenizer**\n",
    "\n",
    "For tf_transformers tokenizer usage check **`tf_transformers/tests/notebooks/tokenizers`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "\n",
    "# Convert text to tokens (for Albert Model)\n",
    "@pad_dataset\n",
    "def tokenizer_fn(tokenizer, text_list):\n",
    "    \"\"\"Tokenizer fn should return a dict (no padding is required).\n",
    "    Make sure, you pass all primary keys required to the model\n",
    "\n",
    "    text_list: a list of text\n",
    "\n",
    "    {'input_ids': tf.constant([[1, 2]]),\n",
    "     'input_mask': tf.constant([[1, 1]]),\n",
    "     'input_type_ids': tf.constant([[1, 0]])}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    input_mask = []\n",
    "    input_type_ids = []\n",
    "    for text in text_list:\n",
    "        input_ids.append(tokenizer.encode(text))\n",
    "        input_mask.append(tf.ones_like(input_ids[-1]).numpy().tolist())\n",
    "        input_type_ids.append(tf.zeros_like(input_ids[-1]).numpy().tolist())\n",
    "    inputs = {\"input_ids\": input_ids}\n",
    "    inputs[\"input_mask\"] = input_mask\n",
    "    inputs['input_type_ids'] = input_type_ids\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation APIs (Seq2Seq)\n",
    "\n",
    "We will benchmark on gpt2 model with following approaches.\n",
    "\n",
    "1. Use the model in `saved_model_dir` with **`TextDecoder`** API. This API will consume the model,\n",
    "\n",
    "   **(suppprts `saved_model`, `tf.keras.Model`, `hub.KerasLayer`)** . Recommended is `saved_model` \n",
    "   \n",
    "   or `hub.KerasLayer` . **`TextDecoder`** API is **pure python function, which has for loops for decoding\n",
    "   \n",
    "   \n",
    "2. We will use **`TextDecoderSerializable`** . This `tf.while_loop` implementation . The advantage is\n",
    "\n",
    "   we can serialize the entire operation as it is, so you decoding method will be a part of `saved_model`\n",
    "   \n",
    "   graph.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TextDecoderSeq2Seq (For loop over saved model)\n",
    "\n",
    "# You can pass either tf.keras.Model or you can load saved_model and pass that also.\n",
    "# Recommended is for performance <model_pb>\n",
    "\n",
    "# This is saved model of T5\n",
    "loaded   = tf.saved_model.load(saved_model_dir)\n",
    "model_pb = loaded.signatures['serving_default']\n",
    "\n",
    "decoder = TextDecoder(\n",
    "    model = model_pb,\n",
    "    hidden_dimension = 768,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = ['Sachin Tendulkar is one of the finest',\n",
    "             'I like to walk with my dog']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Greedy Decoding (serialized model)\n",
    "\n",
    "1. We can do, `model=beam` and `model=top_k_top_p` for Beam search and top K nucleus sampling\n",
    "\n",
    "   respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:540: UserWarning: Input dict contained keys ['iterations'] which did not match any model input. They will be ignored by the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Save the model for greedy decoding\n",
    "saved_model_dir_strategy = 'model_pb_temp'\n",
    "\n",
    "decoder_layer = TextDecoderSerializable(\n",
    "    model = model_tf_transformers,\n",
    "    hidden_dimension = 768,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    max_iterations=None,\n",
    "    mode=\"greedy\",\n",
    "    do_sample=False,\n",
    "    eos_id=-100,\n",
    "    input_mask_ids = 1,\n",
    "    input_type_ids = 0\n",
    ")\n",
    "# Convert whole operation to a model\n",
    "decoder_model  = decoder_layer.get_model()\n",
    "\n",
    "\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(saved_model_dir_strategy)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Greedy Decoding\n",
    "\n",
    "Lets test, whether the results we obtained using **`TextDecoderSeq2Seq`** matches with **`TextDecoderSerializableSeq2Seq`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='greedy', \n",
    "               max_iterations=25, \n",
    "               eos_id=-100)\n",
    "\n",
    "# # # This is Albert Model saved along with greedy decoder (as it is tf.while loop)\n",
    "# # # we can save it together .\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir_strategy)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "\n",
    "inputs_for_decoder = inputs.copy()\n",
    "# We saved by passing max_iterations = None in TextDecoderSerializableSeq2Seq\n",
    "# So, we need to pass iterations everytime\n",
    "inputs_for_decoder['iterations'] = tf.constant([[25]])\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs_for_decoder)\n",
    "\n",
    "tf.assert_equal( tf.cast(decoder_results['predicted_ids'], tf.int32)\n",
    "                ,decoder_results_serialized['predicted_ids'])\n",
    "\n",
    "print(\"Sucess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Save the model for greedy decoding\n",
    "saved_model_dir_strategy = 'model_pb_temp'\n",
    "\n",
    "decoder_layer = TextDecoderSerializable(\n",
    "    model = model_tf_transformers,\n",
    "    hidden_dimension = 768,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    max_iterations=None,\n",
    "    mode=\"beam\",\n",
    "    beam_size = 2,\n",
    "    do_sample=False,\n",
    "    eos_id=-100,\n",
    "    input_mask_ids = 1,\n",
    "    input_type_ids = 0\n",
    ")\n",
    "# Convert whole operation to a model\n",
    "decoder_model  = decoder_layer.get_model()\n",
    "\n",
    "\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(saved_model_dir_strategy)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Beam Decoding\n",
    "\n",
    "Lets test, whether the results we obtained using **`TextDecoderSeq2Seq`** matches with **`TextDecoderSerializableSeq2Seq`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='beam', \n",
    "               max_iterations=25, \n",
    "               beam_size = 2,\n",
    "               eos_id=-100)\n",
    "\n",
    "# # # This is Albert Model saved along with greedy decoder (as it is tf.while loop)\n",
    "# # # we can save it together .\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir_strategy)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "\n",
    "inputs_for_decoder = inputs.copy()\n",
    "# We saved by passing max_iterations = None in TextDecoderSerializableSeq2Seq\n",
    "# So, we need to pass iterations everytime\n",
    "inputs_for_decoder['iterations'] = tf.constant([[25]])\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs_for_decoder)\n",
    "\n",
    "tf.assert_equal( tf.cast(decoder_results['predicted_ids'], tf.int32)\n",
    "                ,decoder_results_serialized['predicted_ids'])\n",
    "\n",
    "print(\"Sucess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top K top P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Save the model for greedy decoding\n",
    "saved_model_dir_strategy = 'model_pb_temp'\n",
    "\n",
    "decoder_layer = TextDecoderSerializable(\n",
    "    model = model_tf_transformers,\n",
    "    hidden_dimension = 768,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    max_iterations=None,\n",
    "    mode=\"top_k_top_p\",\n",
    "    top_k = 25,\n",
    "    top_p = 0.75,\n",
    "    do_sample=False,\n",
    "    eos_id=-100,\n",
    "    input_mask_ids = 1,\n",
    "    input_type_ids = 0\n",
    ")\n",
    "# Convert whole operation to a model\n",
    "decoder_model  = decoder_layer.get_model()\n",
    "\n",
    "\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(saved_model_dir_strategy)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test top K top P Decoding\n",
    "\n",
    "Lets test, whether the results we obtained using **`TextDecoderSeq2Seq`** matches with **`TextDecoderSerializableSeq2Seq`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='top_k_top_p', \n",
    "               max_iterations=25,\n",
    "               top_k = 25,\n",
    "               top_p =0.75,\n",
    "               eos_id=-100)\n",
    "\n",
    "# # # This is Albert Model saved along with greedy decoder (as it is tf.while loop)\n",
    "# # # we can save it together .\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir_strategy)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "\n",
    "inputs_for_decoder = inputs.copy()\n",
    "# We saved by passing max_iterations = None in TextDecoderSerializableSeq2Seq\n",
    "# So, we need to pass iterations everytime\n",
    "inputs_for_decoder['iterations'] = tf.constant([[25]])\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs_for_decoder)\n",
    "\n",
    "tf.assert_equal( tf.cast(decoder_results['predicted_ids'], tf.int32)\n",
    "                ,decoder_results_serialized['predicted_ids'])\n",
    "\n",
    "print(\"Sucess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
