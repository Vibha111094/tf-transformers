{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tf_transformers.models import T5Model\n",
    "from tf_transformers.core import LegacyModule\n",
    "from tf_transformers.data import pad_dataset\n",
    "\n",
    "from tf_transformers.text import TextDecoderSeq2Seq\n",
    "from tf_transformers.text import TextDecoderSerializableSeq2Seq\n",
    "\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tf_transformers Model\n",
    "\n",
    "Configs are in the `model_configs` folder in the root of the repo.\n",
    "\n",
    "\n",
    "We will be using `convert_albert_hf_to_tf_transformers` function.\n",
    "\n",
    "Always use `is_training=False` to load the model and pass this model for conversion.\n",
    "\n",
    "Do not enablle `pipeline_mode='auto-regressive` while converting. Because, variable name\n",
    "\n",
    "differs due to `tf.cond` usage. \n",
    "\n",
    "# Steps:\n",
    "\n",
    "1. Load a model using **`is_training=False`**\n",
    "\n",
    "2. Convert it using conversion functions from `tf_transformers.utils`\n",
    "\n",
    "3. Save the `checkpoint` .\n",
    "\n",
    "4. For auto-regressive tasks (text generation) use **`pipeline_mode='auto-regressive`**\n",
    "\n",
    "   along with **`is_training=False`** and load from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Default Way of loading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "model_layer, model, config = T5Model(model_name='t5-small', use_dropout=False)\n",
    "model.load_checkpoint(\"/Users/PRVATE/LegacyAI_models/t5-small/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for reference\n",
    "\n",
    "Have a look at `tf_transformers/extra/*.py` for reference values, to make sure model\n",
    "\n",
    "has loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embeddings tf.Tensor(-125.860725, shape=(), dtype=float32)\n",
      "token_logits tf.Tensor(-103934104.0, shape=(), dtype=float32)\n",
      "last_token_logits tf.Tensor(-19607434.0, shape=(), dtype=float32)\n",
      "encoder_hidden_states tf.Tensor(6.383378, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Please have a look at tf_transformers/extra/*.py for reference values\n",
    "\n",
    "inputs_sample = {\n",
    "    \"encoder_input_ids\": tf.constant([[8774, 6, 82, 1782, 19, 5295]]),\n",
    "    \"encoder_input_mask\": tf.constant([[1, 1, 1, 1, 1, 1]]),\n",
    "    \"decoder_input_ids\": tf.constant([[8774, 6, 82, 1782, 19, 5295]]),\n",
    "}\n",
    "\n",
    "res = model(inputs_sample)\n",
    "for k, v in res.items():\n",
    "    print(k, tf.reduce_sum(v))\n",
    "\n",
    "# Reference\n",
    "\n",
    "# token_embeddings tf.Tensor(-125.860725, shape=(), dtype=float32)\n",
    "# all_layer_token_embeddings tf.Tensor(-192280.42, shape=(), dtype=float32)\n",
    "# token_logits tf.Tensor(-103934104.0, shape=(), dtype=float32)\n",
    "# last_token_logits tf.Tensor(-19607434.0, shape=(), dtype=float32)\n",
    "# encoder_hidden_states tf.Tensor(6.383378, shape=(), dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model for Auto-Regressive tasks\n",
    "\n",
    "**Text generation / Auto regressive decoding*** requires caching of `K` and `V` values.\n",
    "\n",
    "This, means for the model to make use of serialization, thsese values has to be a part of the model.\n",
    "\n",
    "So, K and V are extra inputs required for inference **(only in the case of text generation)**.\n",
    "\n",
    "As a result, training and testing needs different pipleines (for auto regressive tasks)\n",
    "\n",
    "**`Note: We have necessary wrappers to do all these, user doesnt has to worry about any of these`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:We are overwriding `is_training` is False to `is_training` to True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 8, None, 64), dtype=float32)\n",
      "INFO:absl:decoder_all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 8, None, 64), dtype=float32)\n",
      "INFO:absl:encoder_hidden_states ---> Tensor(\"encoder_hidden_states:0\", shape=(None, None, 512), dtype=float32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 8, None, 64), dtype=float32)\n",
      "INFO:absl:decoder_all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 8, None, 64), dtype=float32)\n",
      "INFO:absl:encoder_hidden_states ---> Tensor(\"encoder_hidden_states:0\", shape=(None, None, 512), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.EncoderDecoder object at 0x14aec8130> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x14a961fa0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.EncoderDecoder object at 0x14aec8130> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x14a961fa0>).\n",
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "model_layer, model, config = T5Model(model_name='t5-small', is_training=False, pipeline_mode='auto-regressive')\n",
    "model.load_checkpoint(\"/Users/PRVATE/LegacyAI_models/t5-small/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model .pb (saved_model)\n",
    "\n",
    "To make use of the benefits of serialization, we have to save the model.\n",
    "\n",
    "Now, why don't `model_tf_transformers.save(\"model_pb\", save_format='tf')` . \n",
    "\n",
    "Reason is when we save the model using above, TF will somehow ignore the proper output node names.\n",
    "\n",
    "It will assign some random names like `['gpt_output1, ect...]`. \n",
    "\n",
    "To preserve the names in the `saved_model` , we have small wrapper function called `LegacyModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_legacyai/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_legacyai/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_legacyai/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_legacyai/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: temp_pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: temp_pb/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model to .pb for make use of proper serialization\n",
    "model.save_as_serialize_module(\"temp_pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer \n",
    "LegcayAI has in-built tokenizer. You can either use it. (Not recommended).\n",
    "\n",
    "The main difference is how we handle `SPECIAL TOKENS`. Apart from that its the same.\n",
    "\n",
    "Recommended use **HuggingFace tokenizer**\n",
    "\n",
    "For tf_transformers tokenizer usage check **`tf_transformers/tests/notebooks/tokenizers`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "# Convert text to tokens (for T5 Model)\n",
    "@pad_dataset\n",
    "def tokenizer_fn(tokenizer, text_list):\n",
    "    \"\"\"Tokenizer fn should return a dict (no padding is required).\n",
    "    Make sure, you pass all primary keys required to the model\n",
    "\n",
    "    text_list: a list of text\n",
    "\n",
    "    {'input_ids': tf.constant([[1, 2]]),\n",
    "     'input_mask': tf.constant([[1, 1]]),\n",
    "     'input_type_ids': tf.constant([[1, 0]])}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    input_mask = []\n",
    "    for text in text_list:\n",
    "        input_ids.append(tokenizer.encode(text))\n",
    "        input_mask.append(tf.ones_like(input_ids[-1]).numpy().tolist())\n",
    "    inputs = {\"encoder_input_ids\": input_ids}\n",
    "    inputs[\"encoder_input_mask\"] = input_mask\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation APIs (Seq2Seq)\n",
    "\n",
    "We will benchmark on gpt2 model with following approaches.\n",
    "\n",
    "1. Use the model in `saved_model_dir` with **`TextDecoder`** API. This API will consume the model,\n",
    "\n",
    "   **(suppprts `saved_model`, `tf.keras.Model`, `hub.KerasLayer`)** . Recommended is `saved_model` \n",
    "   \n",
    "   or `hub.KerasLayer` . **`TextDecoder`** API is **pure python function, which has for loops for decoding\n",
    "   \n",
    "   \n",
    "2. We will use **`TextDecoderSerializable`** . This `tf.while_loop` implementation . The advantage is\n",
    "\n",
    "   we can serialize the entire operation as it is, so you decoding method will be a part of `saved_model`\n",
    "   \n",
    "   graph.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TextDecoderSeq2Seq (For loop over saved model)\n",
    "\n",
    "# You can pass either tf.keras.Model or you can load saved_model and pass that also.\n",
    "# Recommended is for performance <model_pb>\n",
    "\n",
    "# This is saved model of T5\n",
    "loaded   = tf.saved_model.load(\"temp_pb/\")\n",
    "# model_pb = loaded.signatures['serving_default']\n",
    "\n",
    "decoder = TextDecoderSeq2Seq(\n",
    "    model = loaded,\n",
    "    decoder_start_token_id = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Greedy Decoding (serialized model)\n",
    "\n",
    "1. We can do, `model=beam` and `model=top_k_top_p` for Beam search and top K nucleus sampling\n",
    "\n",
    "   respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: temp_pb_decode/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: temp_pb_decode/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Save the model for greedy decoding\n",
    "saved_model_dir_strategy = 'temp_pb_decode'\n",
    "\n",
    "decoder_layer = TextDecoderSerializableSeq2Seq(\n",
    "    model = model,\n",
    "    decoder_start_token_id=0,\n",
    "    mode=\"greedy\",\n",
    "    do_sample=False,\n",
    ")\n",
    "# Convert whole operation to a model\n",
    "decoder_model  = decoder_layer.get_model()\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(saved_model_dir_strategy)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Greedy Decoding\n",
    "\n",
    "Lets test, whether the results we obtained using **`TextDecoderSeq2Seq`** matches with **`TextDecoderSerializableSeq2Seq`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess\n"
     ]
    }
   ],
   "source": [
    "text_list = [\"summarize: studies have shown that owning a dog is good for you\", \n",
    "             \"translate: I love you so much\"]\n",
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='greedy', \n",
    "               max_iterations=25)\n",
    "\n",
    "# # This is T5 model saved along with greedy decoder (as it is tf.while loop)\n",
    "# # we can save it together .\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir_strategy)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "\n",
    "inputs_for_decoder = inputs.copy()\n",
    "# We saved by passing max_iterations = None in TextDecoderSerializableSeq2Seq\n",
    "# So, we need to pass iterations everytime\n",
    "inputs_for_decoder['iterations'] = tf.constant([[25]])\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs_for_decoder)\n",
    "\n",
    "tf.assert_equal( tf.cast(decoder_results['predicted_ids'], tf.int32)\n",
    "                ,decoder_results_serialized['predicted_ids'])\n",
    "\n",
    "print(\"Sucess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: temp_pb_decode/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: temp_pb_decode/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Save the model for greedy decoding\n",
    "\n",
    "decoder_layer = TextDecoderSerializableSeq2Seq(\n",
    "    model = model,\n",
    "    decoder_start_token_id = 0,\n",
    "    mode=\"beam\",\n",
    "    beam_size = 2,\n",
    "    do_sample=False,\n",
    ")\n",
    "# Convert whole operation to a model\n",
    "decoder_model  = decoder_layer.get_model()\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(saved_model_dir_strategy)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test beam Decoding\n",
    "\n",
    "Lets test, whether the results we obtained using **`TextDecoderSeq2Seq`** matches with **`TextDecoderSerializableSeq2Seq`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess\n"
     ]
    }
   ],
   "source": [
    "text_list = [\"summarize: studies have shown that owning a dog is good for you\", \n",
    "             \"translate: I love you so much\"]\n",
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='beam', \n",
    "               max_iterations=25,\n",
    "               beam_size = 2\n",
    "              )\n",
    "\n",
    "# # This is T5 model saved along with greedy decoder (as it is tf.while loop)\n",
    "# # we can save it together .\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir_strategy)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "\n",
    "inputs_for_decoder = inputs.copy()\n",
    "# We saved by passing max_iterations = None in TextDecoderSerializableSeq2Seq\n",
    "# So, we need to pass iterations everytime\n",
    "inputs_for_decoder['iterations'] = tf.constant([[25]])\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs_for_decoder)\n",
    "\n",
    "tf.assert_equal( tf.cast(decoder_results['predicted_ids'], tf.int32)\n",
    "                ,decoder_results_serialized['predicted_ids'])\n",
    "\n",
    "print(\"Sucess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top K top P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Save the model for greedy decoding\n",
    "decoder_layer = TextDecoderSerializableSeq2Seq(\n",
    "    model = model,\n",
    "    decoder_start_token_id = 0,\n",
    "    mode=\"top_k_top_p\",\n",
    "    top_k = 25,\n",
    "    top_p = 0.75,\n",
    "    do_sample=False,\n",
    ")\n",
    "# Convert whole operation to a model\n",
    "decoder_model  = decoder_layer.get_model()\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(saved_model_dir_strategy)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test top k top p Decoding\n",
    "\n",
    "Lets test, whether the results we obtained using **`TextDecoderSeq2Seq`** matches with **`TextDecoderSerializableSeq2Seq`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess\n"
     ]
    }
   ],
   "source": [
    "text_list = [\"summarize: studies have shown that owning a dog is good for you\", \n",
    "             \"translate: I love you so much\"]\n",
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='top_k_top_p', \n",
    "               max_iterations=25,\n",
    "               top_k = 25,\n",
    "               top_p = 0.75,\n",
    "               eos_id=-100)\n",
    "\n",
    "# # This is T5 model saved along with greedy decoder (as it is tf.while loop)\n",
    "# # we can save it together .\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir_strategy)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "\n",
    "inputs_for_decoder = inputs.copy()\n",
    "# We saved by passing max_iterations = None in TextDecoderSerializableSeq2Seq\n",
    "# So, we need to pass iterations everytime\n",
    "inputs_for_decoder['iterations'] = tf.constant([[25]])\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs_for_decoder)\n",
    "\n",
    "tf.assert_equal( tf.cast(decoder_results['predicted_ids'], tf.int32)\n",
    "                ,decoder_results_serialized['predicted_ids'])\n",
    "\n",
    "print(\"Sucess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete everything from temp folder\n",
    "!rm -rf temp*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
