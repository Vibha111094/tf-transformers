{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tf_transformers.models import GPT2Encoder, GPT2Model\n",
    "from tf_transformers.core import LegacyModule\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from tf_transformers.data import pad_dataset\n",
    "from tf_transformers.text import TextDecoder\n",
    "from tf_transformers.text import TextDecoderSerializable\n",
    "\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tf_transformers Model\n",
    "\n",
    "Configs are in the `model_configs` folder in the root of the repo.\n",
    "\n",
    "\n",
    "We will be using `convert_albert_hf_to_tf_transformers` function.\n",
    "\n",
    "Always use `is_training=False` to load the model and pass this model for conversion.\n",
    "\n",
    "Do not enablle `pipeline_mode='auto-regressive` while converting. Because, variable name\n",
    "\n",
    "differs due to `tf.cond` usage. \n",
    "\n",
    "# Steps:\n",
    "\n",
    "1. Load a model using **`is_training=False`**\n",
    "\n",
    "2. Convert it using conversion functions from `tf_transformers.utils`\n",
    "\n",
    "3. Save the `checkpoint` .\n",
    "\n",
    "4. For auto-regressive tasks (text generation) use **`pipeline_mode='auto-regressive`**\n",
    "\n",
    "   along with **`is_training=False`** and load from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Initialized Variables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.gpt2.GPT2Encoder object at 0x109db96a0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x16f8a6430>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.gpt2.GPT2Encoder object at 0x109db96a0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x16f8a6430>).\n",
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "# Load tf_transformers model\n",
    "# Most config we will be providing\n",
    "\n",
    "# Default configs for the model\n",
    "from tf_transformers.models.model_configs.gpt2 import gpt2 as config\n",
    "config = config.config\n",
    "\n",
    "# Always do this\n",
    "\n",
    "\n",
    "# tf_transformers Layer (an extension of Keras Layer)\n",
    "# This is not Keras model, but extension of keras Layer\n",
    "\n",
    "# Save as saved_model\n",
    "# If you want to use the model for Auto Regressive tasks ( text-generation ),\n",
    "# you have to enable pipeline_mode='auto-regressive'.\n",
    "# Because TF needs extra cache inputs in the saved_model format for doing efficient caching\n",
    "\n",
    "model = GPT2Encoder(\n",
    "    config=config,\n",
    "    name=\"gpt2\",\n",
    "    mask_mode=config[\"mask_mode\"],\n",
    "    is_training=False,\n",
    "    pipeline_mode=\"auto-regressive\",\n",
    ")\n",
    "\n",
    "model = model.get_model()\n",
    "model.load_checkpoint(\"/Users/PRVATE/tf_transformers_models/gpt2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Initialized Variables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.gpt2.GPT2Encoder object at 0x14ce765e0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x15b5ab610>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.gpt2.GPT2Encoder object at 0x14ce765e0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x15b5ab610>).\n",
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "from tf_transformers.models import GPT2Model\n",
    "model_layer, model, model_config = GPT2Model(\n",
    "    model_name='gpt2',\n",
    "    is_training=False,\n",
    "    pipeline_mode=\"auto-regressive\",\n",
    ")\n",
    "\n",
    "model.load_checkpoint(\"/Users/PRVATE/tf_transformers_models/gpt2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model .pb (saved_model)\n",
    "\n",
    "To make use of the benefits of serialization, we have to save the model.\n",
    "\n",
    "Now, why don't `model_tf_transformers.save(\"model_pb\", save_format='tf')` . \n",
    "\n",
    "Reason is when we save the model using above, TF will somehow ignore the proper output node names.\n",
    "\n",
    "It will assign some random names like `['gpt_output1, ect...]`. \n",
    "\n",
    "To preserve the names in the `saved_model` , we have small wrapper function called `LegacyModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model to .pb for make use of proper serialization\n",
    "saved_model_dir = \"model_pb\"\n",
    "model.save_as_serialize_module(saved_model_dir, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer \n",
    "LegcayAI has in-built tokenizer. You can either use it. (Not recommended).\n",
    "\n",
    "The main difference is how we handle `SPECIAL TOKENS`. Apart from that its the same.\n",
    "\n",
    "Recommended use **HuggingFace tokenizer**\n",
    "\n",
    "For tf_transformers tokenizer usage check **`tf_transformers/tests/notebooks/tokenizers`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "# Convert text to tokens (for GPT2 Model)\n",
    "@pad_dataset\n",
    "def tokenizer_fn(tokenizer, text_list):\n",
    "    \"\"\"Tokenizer fn should return a dict (no padding is required).\n",
    "    Make sure, you pass all primary keys required to the model\n",
    "\n",
    "    text_list: a list of text\n",
    "\n",
    "    {'input_ids': tf.constant([[1, 2]]),\n",
    "     'input_mask': tf.constant([[1, 1]]),\n",
    "     'input_type_ids': tf.constant([[1, 0]])}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    input_mask = []\n",
    "    input_type_ids = []\n",
    "    for text in text_list:\n",
    "        input_ids.append(tokenizer.encode(text))\n",
    "    inputs = {\"input_ids\": input_ids}\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation APIs (Seq2Seq)\n",
    "\n",
    "We will benchmark on gpt2 model with following approaches.\n",
    "\n",
    "1. Use the model in `saved_model_dir` with **`TextDecoder`** API. This API will consume the model,\n",
    "\n",
    "   **(suppprts `saved_model`, `tf.keras.Model`, `hub.KerasLayer`)** . Recommended is `saved_model` \n",
    "   \n",
    "   or `hub.KerasLayer` . **`TextDecoder`** API is **pure python function, which has for loops for decoding\n",
    "   \n",
    "   \n",
    "2. We will use **`TextDecoderSerializable`** . This `tf.while_loop` implementation . The advantage is\n",
    "\n",
    "   we can serialize the entire operation as it is, so you decoding method will be a part of `saved_model`\n",
    "   \n",
    "   graph.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TextDecoderSeq2Seq (For loop over saved model)\n",
    "\n",
    "# You can pass either tf.keras.Model or you can load saved_model and pass that also.\n",
    "# Recommended is for performance <model_pb>\n",
    "\n",
    "# This is saved model of T5\n",
    "loaded   = tf.saved_model.load(saved_model_dir)\n",
    "\n",
    "decoder = TextDecoder(\n",
    "    model = loaded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Greedy Decoding (serialized model)\n",
    "\n",
    "1. We can do, `model=beam` and `model=top_k_top_p` for Beam search and top K nucleus sampling\n",
    "\n",
    "   respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Save the model for greedy decoding\n",
    "saved_model_dir_strategy = 'model_pb_temp'\n",
    "\n",
    "decoder_layer = TextDecoderSerializable(\n",
    "    model = model,\n",
    "    max_iterations=None,\n",
    "    mode=\"greedy\",\n",
    "    do_sample=False,\n",
    "    eos_id=-100\n",
    ")\n",
    "# Convert whole operation to a model\n",
    "decoder_model  = decoder_layer.get_model()\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(saved_model_dir_strategy)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = ['Sachin Tendulkar is one of the finest',\n",
    "             'I like to walk with my dog']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Greedy Decoding\n",
    "\n",
    "Lets test, whether the results we obtained using **`TextDecoderSeq2Seq`** matches with **`TextDecoderSerializableSeq2Seq`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='greedy', \n",
    "               max_iterations=25, \n",
    "               eos_id=-100)\n",
    "\n",
    "# # # This is GPT2 Model saved along with greedy decoder (as it is tf.while loop)\n",
    "# # # we can save it together .\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir_strategy)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "\n",
    "inputs_for_decoder = inputs.copy()\n",
    "# We saved by passing max_iterations = None in TextDecoderSerializableSeq2Seq\n",
    "# So, we need to pass iterations everytime\n",
    "inputs_for_decoder['iterations'] = tf.constant([[25]])\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs_for_decoder)\n",
    "\n",
    "tf.assert_equal( tf.cast(decoder_results['predicted_ids'], tf.int32)\n",
    "                ,decoder_results_serialized['predicted_ids'])\n",
    "\n",
    "print(\"Sucess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:540: UserWarning: Input dict contained keys ['iterations'] which did not match any model input. They will be ignored by the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Save the model for greedy decoding\n",
    "saved_model_dir_strategy = 'model_pb_temp'\n",
    "\n",
    "decoder_layer = TextDecoderSerializable(\n",
    "    model = model,\n",
    "    max_iterations=None,\n",
    "    mode=\"beam\",\n",
    "    beam_size = 2,\n",
    "    do_sample=False,\n",
    "    eos_id=-100\n",
    ")\n",
    "# Convert whole operation to a model\n",
    "decoder_model  = decoder_layer.get_model()\n",
    "\n",
    "\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(saved_model_dir_strategy)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Beam Decoding\n",
    "\n",
    "Lets test, whether the results we obtained using **`TextDecoderSeq2Seq`** matches with **`TextDecoderSerializableSeq2Seq`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='beam', \n",
    "               max_iterations=25, \n",
    "               beam_size = 2,\n",
    "               eos_id=-100)\n",
    "\n",
    "# # # This is GPT2 Model saved along with greedy decoder (as it is tf.while loop)\n",
    "# # # we can save it together .\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir_strategy)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "\n",
    "inputs_for_decoder = inputs.copy()\n",
    "# We saved by passing max_iterations = None in TextDecoderSerializableSeq2Seq\n",
    "# So, we need to pass iterations everytime\n",
    "inputs_for_decoder['iterations'] = tf.constant([[25]])\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs_for_decoder)\n",
    "\n",
    "tf.assert_equal( tf.cast(decoder_results['predicted_ids'], tf.int32)\n",
    "                ,decoder_results_serialized['predicted_ids'])\n",
    "\n",
    "print(\"Sucess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top K top P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Save the model for greedy decoding\n",
    "saved_model_dir_strategy = 'model_pb_temp'\n",
    "\n",
    "decoder_layer = TextDecoderSerializable(\n",
    "    model = model,\n",
    "    max_iterations=None,\n",
    "    mode=\"top_k_top_p\",\n",
    "    top_k = 35,\n",
    "    top_p = 0.79,\n",
    "    do_sample=False,\n",
    "    eos_id=-100\n",
    ")\n",
    "# Convert whole operation to a model\n",
    "decoder_model  = decoder_layer.get_model()\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(saved_model_dir_strategy)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test top K top P Decoding\n",
    "\n",
    "Lets test, whether the results we obtained using **`TextDecoderSeq2Seq`** matches with **`TextDecoderSerializableSeq2Seq`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='top_k_top_p', \n",
    "               max_iterations=25,\n",
    "               top_k = 35,\n",
    "               top_p =0.79,\n",
    "               eos_id=-100)\n",
    "\n",
    "# # # This is GPT2 Model saved along with greedy decoder (as it is tf.while loop)\n",
    "# # # we can save it together .\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir_strategy)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "\n",
    "inputs_for_decoder = inputs.copy()\n",
    "# We saved by passing max_iterations = None in TextDecoderSerializableSeq2Seq\n",
    "# So, we need to pass iterations everytime\n",
    "inputs_for_decoder['iterations'] = tf.constant([[25]])\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs_for_decoder)\n",
    "\n",
    "tf.assert_equal( tf.cast(decoder_results['predicted_ids'], tf.int32)\n",
    "                ,decoder_results_serialized['predicted_ids'])\n",
    "\n",
    "print(\"Sucess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r model_ckpt/\n",
    "# !rm -r model_pb/\n",
    "# !rm -r model_pb_temp/\n",
    "# !rm -rf dummy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
