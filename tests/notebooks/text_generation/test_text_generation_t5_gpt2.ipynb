{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/PRVATE/Documents/tf_transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a dummy Model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from transformers import TFT5Model\n",
    "from tf_transformers.models import T5Encoder\n",
    "from tf_transformers.models import EncoderDecoder\n",
    "\n",
    "from tf_transformers.core import LegacyModule\n",
    "\n",
    "from tf_transformers.utils import convert_t5_hf_to_tf_transformers\n",
    "from tf_transformers.data import pad_dataset\n",
    "\n",
    "from tf_transformers.text import TextDecoderSeq2Seq\n",
    "from tf_transformers.text import TextDecoderSerializableSeq2Seq\n",
    "\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tf_transformers Model\n",
    "\n",
    "Configs are in the `model_configs` folder in the root of the repo.\n",
    "\n",
    "\n",
    "We will be using `convert_albert_hf_to_tf_transformers` function.\n",
    "\n",
    "Always use `is_training=False` to load the model and pass this model for conversion.\n",
    "\n",
    "Do not enablle `pipeline_mode='auto-regressive` while converting. Because, variable name\n",
    "\n",
    "differs due to `tf.cond` usage. \n",
    "\n",
    "# Steps:\n",
    "\n",
    "1. Load a model using **`is_training=False`**\n",
    "\n",
    "2. Convert it using conversion functions from `tf_transformers.utils`\n",
    "\n",
    "3. Save the `checkpoint` .\n",
    "\n",
    "4. For auto-regressive tasks (text generation) use **`pipeline_mode='auto-regressive`**\n",
    "\n",
    "   along with **`is_training=False`** and load from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Specifics\n",
    "1. T5 is an **Seq2Seq ( Encoder Decoder ) Model** . It has an encoder part and decoder part.\n",
    "\n",
    "2. In tf_transformers, you can convert any **Encoder to Decoder** with few keyword arguments (`is_decoder=True,`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:We are overwriding `is_training` is False to `is_training` to                     True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:We are overwriding `is_training` is False to `is_training` to                     True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_hidden_states ---> Tensor(\"encoder_hidden_states:0\", shape=(None, None, 512), dtype=float32)\n",
      "INFO:absl:decoder_encoder_mask ---> Tensor(\"decoder_encoder_mask:0\", shape=(None, None, None), dtype=float32)\n",
      "INFO:absl:Initialized Variables\n"
     ]
    }
   ],
   "source": [
    "# Load tf_transformers model\n",
    "# Most config we will be providing\n",
    "\n",
    "# Default configs for the model\n",
    "\n",
    "model_config_dir = '/Users/PRVATE/Documents/tf_transformers/model_configs/'\n",
    "model_name = 't5_small'\n",
    "config_location = os.path.join(model_config_dir, model_name, 'config.json')\n",
    "config = json.load(open(config_location))\n",
    "\n",
    "# Always do this\n",
    "\n",
    "\n",
    "\n",
    "# Encoder\n",
    "\n",
    "config[\"bidirectional\"] = True\n",
    "config[\"mask_mode\"] = \"user_defined\"\n",
    "encoder_layer = T5Encoder(\n",
    "    config=config, mask_mode=config[\"mask_mode\"], is_training=False, name=\"t5_encoder\"\n",
    ")\n",
    "\n",
    "\n",
    "# Decoder\n",
    "\n",
    "# T5 needs bidirectional = False\n",
    "# Decoder mask mode has to be causal (auto-regressive)\n",
    "# encoder_embedding_layer (share embedding from encoder)\n",
    "config[\"bidirectional\"] = False\n",
    "config[\"mask_mode\"] = \"causal\"\n",
    "decoder_layer = T5Encoder(\n",
    "    config=config,\n",
    "    name=\"t5_decoder\",\n",
    "    mask_mode=config[\"mask_mode\"],\n",
    "    is_decoder=True,\n",
    "    is_training=False,\n",
    "    share_encoder_embeddings=True,\n",
    "    encoder_embedding_layer=encoder_layer._embedding_layer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create T5 (Encoder Decoder) model\n",
    "We will use above initialized (randomly) encoder and decoder layer and pass it to `EncoderDecoder` API\n",
    "\n",
    "**Note: If you want to use EncoderDecoder, for text-generation tasks, make sure `decoder_layer` must\n",
    "have `is_training=False` and `pipeline_mode='auto-regressive'` enabled. \n",
    "After that, set `is_training=False` in `EncoderDecoder` model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Train mode (Keras Layer/Legacy Layer)\n",
    "\n",
    "t5_enc_dec_model = EncoderDecoder(\n",
    "    encoder=encoder_layer,\n",
    "    decoder=decoder_layer,\n",
    "    is_training=True,\n",
    "    name=\"t5_small\",\n",
    "    use_dropout=False,\n",
    ")\n",
    "\n",
    "t5_enc_dec_model = t5_enc_dec_model.get_model()\n",
    "\n",
    "# Once this is loaded, we will delete decoder_layer , as well as t5_enc_dec_model\n",
    "# Because, we want to use Albert as the decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delte\n",
    "del t5_enc_dec_model\n",
    "del decoder_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GPT2 Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:We are overwriding `is_training` is False to `is_training`                     to True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_hidden_states ---> Tensor(\"encoder_hidden_states:0\", shape=(None, None, 768), dtype=float32)\n",
      "INFO:absl:decoder_encoder_mask ---> Tensor(\"decoder_encoder_mask:0\", shape=(None, None, None), dtype=float32)\n",
      "INFO:absl:Initialized Variables\n"
     ]
    }
   ],
   "source": [
    "from tf_transformers.models import GPT2Encoder\n",
    "# Load tf_transformers model\n",
    "# Most config we will be providing\n",
    "\n",
    "# Default configs for the model\n",
    "\n",
    "model_config_dir = '/Users/PRVATE/Documents/tf_transformers/model_configs/'\n",
    "model_name = 'gpt2_base'\n",
    "config_location = os.path.join(model_config_dir, model_name, 'config.json')\n",
    "config = json.load(open(config_location))\n",
    "\n",
    "# Always do this\n",
    "\n",
    "\n",
    "# tf_transformers Layer (an extension of Keras Layer)\n",
    "# This is not Keras model, but extension of keras Layer\n",
    "\n",
    "# Save as saved_model\n",
    "# If you want to use the model for Auto Regressive tasks ( text-generation ),\n",
    "# you have to enable pipeline_mode='auto-regressive'.\n",
    "# Because TF needs extra cache inputs in the saved_model format for doing efficient caching\n",
    "\n",
    "# Decoder requires causal masking\n",
    "config[\"mask_mode\"] = 'causal'\n",
    "model_layer = GPT2Encoder(\n",
    "    config=config,\n",
    "    name=\"gpt2\",\n",
    "    mask_mode=config[\"mask_mode\"],\n",
    "    is_training=False,\n",
    "    is_decoder=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Encoder Decoder model\n",
    "\n",
    "Here we use **GPT2** as decoder and **T5** as encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Train mode\n",
    "enc_dec_model = EncoderDecoder(\n",
    "    encoder=encoder_layer,\n",
    "    decoder=model_layer,\n",
    "    is_training=True,\n",
    "    name=\"t5_gpt2\",\n",
    "    use_dropout=False,\n",
    ")\n",
    "enc_dec_model = enc_dec_model.get_model()\n",
    "enc_dec_model.save_weights(\"dummy.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Encoder Decoder model (Auto Regressive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"decoder_input_ids_1:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_hidden_states ---> Tensor(\"encoder_hidden_states_1:0\", shape=(None, None, 768), dtype=float32)\n",
      "INFO:absl:decoder_encoder_mask ---> Tensor(\"decoder_encoder_mask_1:0\", shape=(None, None, None), dtype=float32)\n",
      "INFO:absl:all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:Initialized Variables\n"
     ]
    }
   ],
   "source": [
    "config[\"mask_mode\"] = 'causal'\n",
    "model_layer = GPT2Encoder(\n",
    "    config=config,\n",
    "    name=\"gpt2\",\n",
    "    mask_mode=config[\"mask_mode\"],\n",
    "    is_training=False,\n",
    "    is_decoder=True,\n",
    "    pipeline_mode = 'auto-regressive'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids_1:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:decoder_all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:encoder_hidden_states ---> Tensor(\"encoder_hidden_states_1:0\", shape=(None, None, 768), dtype=float32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids_1:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:decoder_all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:encoder_hidden_states ---> Tensor(\"encoder_hidden_states_1:0\", shape=(None, None, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Train mode\n",
    "enc_dec_model_test = EncoderDecoder(\n",
    "    encoder=encoder_layer,\n",
    "    decoder=model_layer,\n",
    "    is_training=False,\n",
    "    name=\"t5_gpt2\",\n",
    "    use_dropout=False,\n",
    ")\n",
    "enc_dec_model_test = enc_dec_model_test.get_model()\n",
    "enc_dec_model_test.load_weights(\"dummy.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer \n",
    "LegcayAI has in-built tokenizer. You can either use it. (Not recommended).\n",
    "\n",
    "The main difference is how we handle `SPECIAL TOKENS`. Apart from that its the same.\n",
    "\n",
    "Recommended use **HuggingFace tokenizer**\n",
    "\n",
    "For tf_transformers tokenizer usage check **`tf_transformers/tests/notebooks/tokenizers`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "# Convert text to tokens (for T5 Model)\n",
    "@pad_dataset\n",
    "def tokenizer_fn(tokenizer, text_list):\n",
    "    \"\"\"Tokenizer fn should return a dict (no padding is required).\n",
    "    Make sure, you pass all primary keys required to the model\n",
    "\n",
    "    text_list: a list of text\n",
    "\n",
    "    {'input_ids': tf.constant([[1, 2]]),\n",
    "     'input_mask': tf.constant([[1, 1]]),\n",
    "     'input_type_ids': tf.constant([[1, 0]])}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    input_mask = []\n",
    "    for text in text_list:\n",
    "        input_ids.append(tokenizer.encode(text))\n",
    "        input_mask.append(tf.ones_like(input_ids[-1]).numpy().tolist())\n",
    "    inputs = {\"encoder_input_ids\": input_ids}\n",
    "    inputs[\"encoder_input_mask\"] = input_mask\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = ['summarize : Hello How are you ?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.0183208, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.0183206, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.018321, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.0183206, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.018321, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.018321, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.018321, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.0183208, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.018321, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.018321, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tokenizer_fn(tokenizer, text_list)\n",
    "encoder_inputs['decoder_input_ids'] = tf.constant([[9]])\n",
    "for i in range(10):\n",
    "    result = enc_dec_model(encoder_inputs)\n",
    "    best_id = tf.argmax(result['last_token_logits'][0])\n",
    "    best_prob = tf.reduce_max(result['last_token_logits'][0])\n",
    "    print(best_id, '-->', best_prob)\n",
    "    encoder_inputs['decoder_input_ids'] = tf.concat([encoder_inputs['decoder_input_ids'], \n",
    "              tf.expand_dims(tf.expand_dims(tf.cast(best_id, tf.int32), 0), 1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_input_ids': <tf.Tensor 'input_ids:0' shape=(None, None) dtype=int32>,\n",
       " 'encoder_input_mask': <tf.Tensor 'input_mask:0' shape=(None, None) dtype=int32>,\n",
       " 'decoder_input_ids': <tf.Tensor 'decoder_input_ids_1:0' shape=(None, None) dtype=int32>,\n",
       " 'decoder_all_cache_key': <tf.Tensor 'all_cache_key:0' shape=(None, None, 12, None, 64) dtype=float32>,\n",
       " 'decoder_all_cache_value': <tf.Tensor 'all_cache_value:0' shape=(None, None, 12, None, 64) dtype=float32>,\n",
       " 'encoder_hidden_states': <tf.Tensor 'encoder_hidden_states_1:0' shape=(None, None, 768) dtype=float32>}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_dec_model_test.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.0183208, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.0183206, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.0183206, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.018321, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.0183206, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.0183206, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.0183208, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.0183206, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.018321, shape=(), dtype=float32)\n",
      "tf.Tensor(8310, shape=(), dtype=int64) --> tf.Tensor(2.018321, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tokenizer_fn(tokenizer, text_list)\n",
    "encoder_inputs['decoder_input_ids'] = tf.constant([[9]])\n",
    "encoder_inputs['decoder_all_cache_key'] = tf.zeros((12, 1, 12, 1, 64 ), dtype=tf.float32)\n",
    "encoder_inputs['decoder_all_cache_value'] = tf.zeros((12, 1, 12, 1, 64 ), dtype=tf.float32)\n",
    "encoder_inputs['encoder_hidden_states'] = tf.zeros((1, 10, 768), dtype=tf.float32)\n",
    "\n",
    "for i in range(10):\n",
    "    result = enc_dec_model_test(encoder_inputs)\n",
    "    best_id = tf.argmax(result['last_token_logits'][0])\n",
    "    best_prob = tf.reduce_max(result['last_token_logits'][0])\n",
    "    print(best_id, '-->', best_prob)\n",
    "    encoder_inputs['decoder_input_ids'] = tf.expand_dims(tf.expand_dims(tf.cast(best_id, tf.int32), 0), 1)\n",
    "    encoder_inputs['decoder_all_cache_key'] = result['decoder_all_cache_key']\n",
    "    encoder_inputs['decoder_all_cache_value'] = result['decoder_all_cache_value']\n",
    "    encoder_inputs['encoder_hidden_states'] = result['encoder_hidden_states']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf dummy.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
