{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tf_transformers.models import T5Encoder\n",
    "from tf_transformers.models import EncoderDecoder\n",
    "\n",
    "from tf_transformers.core import LegacyModule\n",
    "\n",
    "from tf_transformers.utils import convert_t5_hf_to_tf_transformers\n",
    "from tf_transformers.data import pad_dataset\n",
    "\n",
    "from tf_transformers.text import TextDecoderSeq2Seq\n",
    "from tf_transformers.text import TextDecoderSerializableSeq2Seq\n",
    "\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tf_transformers Model\n",
    "\n",
    "Configs are in the `model_configs` folder in the root of the repo.\n",
    "\n",
    "\n",
    "We will be using `convert_albert_hf_to_tf_transformers` function.\n",
    "\n",
    "Always use `is_training=False` to load the model and pass this model for conversion.\n",
    "\n",
    "Do not enablle `pipeline_mode='auto-regressive` while converting. Because, variable name\n",
    "\n",
    "differs due to `tf.cond` usage. \n",
    "\n",
    "# Steps:\n",
    "\n",
    "1. Load a model using **`is_training=False`**\n",
    "\n",
    "2. Convert it using conversion functions from `tf_transformers.utils`\n",
    "\n",
    "3. Save the `checkpoint` .\n",
    "\n",
    "4. For auto-regressive tasks (text generation) use **`pipeline_mode='auto-regressive`**\n",
    "\n",
    "   along with **`is_training=False`** and load from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Default Way of loading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'T5Encoder' object has no attribute 'bidirectional'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3382186d128b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mT5Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m't5-small'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/tf-transformers/src/tf_transformers/models/model_wrappers/t5_wrapper.py\u001b[0m in \u001b[0;36mT5Model\u001b[0;34m(model_name, is_training, use_dropout, pipeline_mode, model_checkpoint_dir, encoder_sequence_length)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mencoder_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     encoder_layer, encoder_model, encoder_config = encoder_class(\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     )\n",
      "\u001b[0;32m~/Documents/tf-transformers/src/tf_transformers/models/model_wrappers/t5_wrapper.py\u001b[0m in \u001b[0;36mmodelWrapper\u001b[0;34m(model_name, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmodel_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tf-transformers/src/tf_transformers/models/t5.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, mask_mode, name, use_dropout, is_training, batch_size, sequence_length, use_type_embeddings, use_positonal_embeddings, pipeline_mode, is_decoder, cross_attention_inside_encoder, share_attention_layers, share_encoder_embeddings, encoder_embedding_layer, encoder_type_embedding_layer, encoder_positional_embedding_layer, use_mlm_layer, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mintermediate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mintermediate_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_activation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0mbidirectional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m                 \u001b[0mcreate_positonal_embedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_positonal_embedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mpositional_buckets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_buckets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'T5Encoder' object has no attribute 'bidirectional'"
     ]
    }
   ],
   "source": [
    "from tf_transformers.models import T5Model\n",
    "model = T5Model(model_name='t5-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Specifics\n",
    "1. T5 is an **Seq2Seq ( Encoder Decoder ) Model** . It has an encoder part and decoder part.\n",
    "\n",
    "2. In tf_transformers, you can convert any **Encoder to Decoder** with few keyword arguments (`is_decoder=True,`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:We are overwriding `is_training` is False to `is_training` to                     True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:We are overwriding `is_training` is False to `is_training` to                     True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_hidden_states ---> Tensor(\"encoder_hidden_states:0\", shape=(None, None, 512), dtype=float32)\n",
      "INFO:absl:decoder_encoder_mask ---> Tensor(\"decoder_encoder_mask:0\", shape=(None, None, None), dtype=float32)\n",
      "INFO:absl:Initialized Variables\n"
     ]
    }
   ],
   "source": [
    "# Load tf_transformers model\n",
    "# Most config we will be providing\n",
    "\n",
    "# Default configs for the model\n",
    "\n",
    "model_config_dir = '/Users/PRVATE/Documents/tf_transformers/model_configs/'\n",
    "model_name = 't5_small'\n",
    "config_location = os.path.join(model_config_dir, model_name, 'config.json')\n",
    "config = json.load(open(config_location))\n",
    "\n",
    "# Always do this\n",
    "\n",
    "\n",
    "\n",
    "# Encoder\n",
    "\n",
    "config[\"bidirectional\"] = True\n",
    "config[\"mask_mode\"] = \"user_defined\"\n",
    "encoder_layer = T5Encoder(\n",
    "    config=config, mask_mode=config[\"mask_mode\"], is_training=False, name=\"t5_encoder\"\n",
    ")\n",
    "\n",
    "\n",
    "# Decoder\n",
    "\n",
    "# T5 needs bidirectional = False\n",
    "# Decoder mask mode has to be causal (auto-regressive)\n",
    "# encoder_embedding_layer (share embedding from encoder)\n",
    "config[\"bidirectional\"] = False\n",
    "config[\"mask_mode\"] = \"causal\"\n",
    "decoder_layer = T5Encoder(\n",
    "    config=config,\n",
    "    name=\"t5_decoder\",\n",
    "    mask_mode=config[\"mask_mode\"],\n",
    "    is_decoder=True,\n",
    "    is_training=False,\n",
    "    share_encoder_embeddings=True,\n",
    "    encoder_embedding_layer=encoder_layer._embedding_layer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create T5 (Encoder Decoder) model\n",
    "We will use above initialized (randomly) encoder and decoder layer and pass it to `EncoderDecoder` API\n",
    "\n",
    "**Note: If you want to use EncoderDecoder, for text-generation tasks, make sure `decoder_layer` must\n",
    "have `is_training=False` and `pipeline_mode='auto-regressive'` enabled. \n",
    "After that, set `is_training=False` in `EncoderDecoder` model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Train mode (Keras Layer/Legacy Layer)\n",
    "\n",
    "enc_dec_model = EncoderDecoder(\n",
    "    encoder=encoder_layer,\n",
    "    decoder=decoder_layer,\n",
    "    is_training=True,\n",
    "    name=\"t5_small\",\n",
    "    use_dropout=False,\n",
    ")\n",
    "\n",
    "enc_dec_model = enc_dec_model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Deleteing huggingface model for saving memory\n",
      "INFO:absl:Done assigning ENCODER variables weights 51\n",
      "INFO:absl:Deleteing huggingface model for saving memory\n",
      "INFO:absl:Done assigning DECODER variables weights 81\n"
     ]
    }
   ],
   "source": [
    "# Convert\n",
    "convert_t5_hf_to_tf_transformers(model_hf, enc_dec_model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved at model_ckpt/ckpt-1\n"
     ]
    }
   ],
   "source": [
    "# If you want to save the model as checkpoints\n",
    "\n",
    "checkpoint_dir = 'model_ckpt'\n",
    "checkpoint = tf.train.Checkpoint(model=enc_dec_model)\n",
    "manager = tf.train.CheckpointManager(\n",
    "    checkpoint, directory=checkpoint_dir, max_to_keep=1\n",
    ")\n",
    "manager.save()\n",
    "print(\"Saved at {}\".format(manager.latest_checkpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for reference\n",
    "\n",
    "Have a look at `tf_transformers/extra/*.py` for reference values, to make sure model\n",
    "\n",
    "has loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embeddings tf.Tensor(-126.0661, shape=(), dtype=float32)\n",
      "all_layer_token_embeddings tf.Tensor(-192505.6, shape=(), dtype=float32)\n",
      "token_logits tf.Tensor(-104059470.0, shape=(), dtype=float32)\n",
      "last_token_logits tf.Tensor(-19631904.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Please have a look at tf_transformers/extra/*.py for reference values\n",
    "\n",
    "inputs_sample = {\n",
    "    \"encoder_input_ids\": tf.constant([[8774, 6, 82, 1782, 19, 5295]]),\n",
    "    \"encoder_input_mask\": tf.constant([[1, 1, 1, 1, 1, 1]]),\n",
    "    \"decoder_input_ids\": tf.constant([[8774, 6, 82, 1782, 19, 5295]]),\n",
    "}\n",
    "\n",
    "res = enc_dec_model(inputs_sample)\n",
    "for k, v in res.items():\n",
    "    print(k, tf.reduce_sum(v))\n",
    "\n",
    "# Reference\n",
    "\n",
    "# token_embeddings tf.Tensor(-126.0661, shape=(), dtype=float32)\n",
    "# token_logits tf.Tensor(-104059470.0, shape=(), dtype=float32)\n",
    "# last_token_logits tf.Tensor(-19631904.0, shape=(), dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model for Auto-Regressive tasks\n",
    "\n",
    "**Text generation / Auto regressive decoding*** requires caching of `K` and `V` values.\n",
    "\n",
    "This, means for the model to make use of serialization, thsese values has to be a part of the model.\n",
    "\n",
    "So, K and V are extra inputs required for inference **(only in the case of text generation)**.\n",
    "\n",
    "As a result, training and testing needs different pipleines (for auto regressive tasks)\n",
    "\n",
    "**`Note: We have necessary wrappers to do all these, user doesnt has to worry about any of these`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:We are overwriding `is_training` is False to `is_training` to                     True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_hidden_states ---> Tensor(\"encoder_hidden_states:0\", shape=(None, None, 512), dtype=float32)\n",
      "INFO:absl:decoder_encoder_mask ---> Tensor(\"decoder_encoder_mask:0\", shape=(None, None, None), dtype=float32)\n",
      "INFO:absl:all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 8, None, 64), dtype=float32)\n",
      "INFO:absl:all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 8, None, 64), dtype=float32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 8, None, 64), dtype=float32)\n",
      "INFO:absl:decoder_all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 8, None, 64), dtype=float32)\n",
      "INFO:absl:encoder_hidden_states ---> Tensor(\"encoder_hidden_states:0\", shape=(None, None, 512), dtype=float32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 8, None, 64), dtype=float32)\n",
      "INFO:absl:decoder_all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 8, None, 64), dtype=float32)\n",
      "INFO:absl:encoder_hidden_states ---> Tensor(\"encoder_hidden_states:0\", shape=(None, None, 512), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.EncoderDecoder object at 0x14a3a4fd0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x14f3ee4f0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.EncoderDecoder object at 0x14a3a4fd0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x14f3ee4f0>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "# Encoder layer (Nothing to cache)\n",
    "config[\"mask_mode\"] = \"user_defined\"\n",
    "config[\"bidirectional\"] = True\n",
    "encoder_layer = T5Encoder(\n",
    "    config=config, mask_mode=config[\"mask_mode\"], is_training=False, name=\"t5_encoder\"\n",
    ")\n",
    "\n",
    "# Decoder\n",
    "# Set pipeline_mode = 'auto-regressive'\n",
    "# Only by that, we can enable caching in decoder side\n",
    "\n",
    "\n",
    "config[\"bidirectional\"] = False\n",
    "config[\"mask_mode\"] = \"causal\"\n",
    "decoder_layer = T5Encoder(\n",
    "    config=config,\n",
    "    name=\"t5_decoder\",\n",
    "    mask_mode=\"causal\",\n",
    "    is_decoder=True,\n",
    "    is_training=False,\n",
    "    use_dropout=False,\n",
    "    pipeline_mode=\"auto-regressive\",\n",
    "    share_encoder_embeddings=True,\n",
    "    encoder_embedding_layer=encoder_layer._embedding_layer,\n",
    ")\n",
    "\n",
    "\n",
    "# Train mode\n",
    "enc_dec_model = EncoderDecoder(\n",
    "    encoder=encoder_layer,\n",
    "    decoder=decoder_layer,\n",
    "    is_training=False,\n",
    "    name=\"t5_small\",\n",
    "    use_dropout=False,\n",
    ")\n",
    "enc_dec_model = enc_dec_model.get_model()\n",
    "\n",
    "# And now load the checkpints from previously saved model\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=enc_dec_model)\n",
    "manager = tf.train.CheckpointManager(\n",
    "    checkpoint, directory=checkpoint_dir, max_to_keep=1\n",
    ")\n",
    "status = checkpoint.restore(manager.latest_checkpoint)\n",
    "\n",
    "# Important\n",
    "if status.assert_existing_objects_matched():\n",
    "    print(\"Model checkpoints matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model .pb (saved_model)\n",
    "\n",
    "To make use of the benefits of serialization, we have to save the model.\n",
    "\n",
    "Now, why don't `model_tf_transformers.save(\"model_pb\", save_format='tf')` . \n",
    "\n",
    "Reason is when we save the model using above, TF will somehow ignore the proper output node names.\n",
    "\n",
    "It will assign some random names like `['gpt_output1, ect...]`. \n",
    "\n",
    "To preserve the names in the `saved_model` , we have small wrapper function called `LegacyModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model to .pb for make use of proper serialization\n",
    "saved_model_dir = \"model_pb\"\n",
    "tf_transformers_module = LegacyModule(enc_dec_model)\n",
    "tf_transformers_module.save(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer \n",
    "LegcayAI has in-built tokenizer. You can either use it. (Not recommended).\n",
    "\n",
    "The main difference is how we handle `SPECIAL TOKENS`. Apart from that its the same.\n",
    "\n",
    "Recommended use **HuggingFace tokenizer**\n",
    "\n",
    "For tf_transformers tokenizer usage check **`tf_transformers/tests/notebooks/tokenizers`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "# Convert text to tokens (for T5 Model)\n",
    "@pad_dataset\n",
    "def tokenizer_fn(tokenizer, text_list):\n",
    "    \"\"\"Tokenizer fn should return a dict (no padding is required).\n",
    "    Make sure, you pass all primary keys required to the model\n",
    "\n",
    "    text_list: a list of text\n",
    "\n",
    "    {'input_ids': tf.constant([[1, 2]]),\n",
    "     'input_mask': tf.constant([[1, 1]]),\n",
    "     'input_type_ids': tf.constant([[1, 0]])}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    input_mask = []\n",
    "    for text in text_list:\n",
    "        input_ids.append(tokenizer.encode(text))\n",
    "        input_mask.append(tf.ones_like(input_ids[-1]).numpy().tolist())\n",
    "    inputs = {\"encoder_input_ids\": input_ids}\n",
    "    inputs[\"encoder_input_mask\"] = input_mask\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation APIs (Seq2Seq)\n",
    "\n",
    "We will benchmark on gpt2 model with following approaches.\n",
    "\n",
    "1. Use the model in `saved_model_dir` with **`TextDecoder`** API. This API will consume the model,\n",
    "\n",
    "   **(suppprts `saved_model`, `tf.keras.Model`, `hub.KerasLayer`)** . Recommended is `saved_model` \n",
    "   \n",
    "   or `hub.KerasLayer` . **`TextDecoder`** API is **pure python function, which has for loops for decoding\n",
    "   \n",
    "   \n",
    "2. We will use **`TextDecoderSerializable`** . This `tf.while_loop` implementation . The advantage is\n",
    "\n",
    "   we can serialize the entire operation as it is, so you decoding method will be a part of `saved_model`\n",
    "   \n",
    "   graph.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TextDecoderSeq2Seq (For loop over saved model)\n",
    "\n",
    "# You can pass either tf.keras.Model or you can load saved_model and pass that also.\n",
    "# Recommended is for performance <model_pb>\n",
    "\n",
    "# This is saved model of T5\n",
    "loaded   = tf.saved_model.load(saved_model_dir)\n",
    "model_pb = loaded.signatures['serving_default']\n",
    "\n",
    "decoder = TextDecoderSeq2Seq(\n",
    "    model = model_pb,\n",
    "    decode_start_token_id = 0,\n",
    "    encoder_hidden_size = 512,\n",
    "    decoder_hidden_size = 512,\n",
    "    decoder_num_attention_heads=8,\n",
    "    decoder_num_layers=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Greedy Decoding (serialized model)\n",
    "\n",
    "1. We can do, `model=beam` and `model=top_k_top_p` for Beam search and top K nucleus sampling\n",
    "\n",
    "   respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:540: UserWarning: Input dict contained keys ['iterations'] which did not match any model input. They will be ignored by the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Save the model for greedy decoding\n",
    "saved_model_dir_strategy = 'model_pb_temp'\n",
    "\n",
    "decoder_layer = TextDecoderSerializableSeq2Seq(\n",
    "    model = enc_dec_model,\n",
    "    decode_start_token_id = 0,\n",
    "    encoder_hidden_size = 512,\n",
    "    decoder_hidden_size = 512,\n",
    "    max_iterations=None,\n",
    "    decoder_num_attention_heads=8,\n",
    "    decoder_num_layers=6,\n",
    "    mode=\"greedy\",\n",
    "    do_sample=False,\n",
    "    eos_id=-100,\n",
    ")\n",
    "# Convert whole operation to a model\n",
    "decoder_model  = decoder_layer.get_model()\n",
    "\n",
    "\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(saved_model_dir_strategy)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Greedy Decoding\n",
    "\n",
    "Lets test, whether the results we obtained using **`TextDecoderSeq2Seq`** matches with **`TextDecoderSerializableSeq2Seq`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess\n"
     ]
    }
   ],
   "source": [
    "text_list = [\"summarize: studies have shown that owning a dog is good for you\", \n",
    "             \"translate: I love you so much\"]\n",
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='greedy', \n",
    "               max_iterations=25, \n",
    "               eos_id=-100)\n",
    "\n",
    "# # This is T5 model saved along with greedy decoder (as it is tf.while loop)\n",
    "# # we can save it together .\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir_strategy)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "\n",
    "inputs_for_decoder = inputs.copy()\n",
    "# We saved by passing max_iterations = None in TextDecoderSerializableSeq2Seq\n",
    "# So, we need to pass iterations everytime\n",
    "inputs_for_decoder['iterations'] = tf.constant([[25]])\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs_for_decoder)\n",
    "\n",
    "tf.assert_equal( tf.cast(decoder_results['predicted_ids'], tf.int32)\n",
    "                ,decoder_results_serialized['predicted_ids'])\n",
    "\n",
    "print(\"Sucess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:540: UserWarning: Input dict contained keys ['iterations'] which did not match any model input. They will be ignored by the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Save the model for greedy decoding\n",
    "saved_model_dir_strategy = 'model_pb_temp'\n",
    "\n",
    "decoder_layer = TextDecoderSerializableSeq2Seq(\n",
    "    model = enc_dec_model,\n",
    "    decode_start_token_id = 0,\n",
    "    encoder_hidden_size = 512,\n",
    "    decoder_hidden_size = 512,\n",
    "    max_iterations=None,\n",
    "    decoder_num_attention_heads=8,\n",
    "    decoder_num_layers=6,\n",
    "    mode=\"beam\",\n",
    "    beam_size = 2,\n",
    "    do_sample=False,\n",
    "    eos_id=-100,\n",
    ")\n",
    "# Convert whole operation to a model\n",
    "decoder_model  = decoder_layer.get_model()\n",
    "\n",
    "\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(saved_model_dir_strategy)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test beam Decoding\n",
    "\n",
    "Lets test, whether the results we obtained using **`TextDecoderSeq2Seq`** matches with **`TextDecoderSerializableSeq2Seq`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess\n"
     ]
    }
   ],
   "source": [
    "text_list = [\"summarize: studies have shown that owning a dog is good for you\", \n",
    "             \"translate: I love you so much\"]\n",
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='beam', \n",
    "               max_iterations=25,\n",
    "               beam_size = 2,\n",
    "               eos_id=-100)\n",
    "\n",
    "# # This is T5 model saved along with greedy decoder (as it is tf.while loop)\n",
    "# # we can save it together .\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir_strategy)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "\n",
    "inputs_for_decoder = inputs.copy()\n",
    "# We saved by passing max_iterations = None in TextDecoderSerializableSeq2Seq\n",
    "# So, we need to pass iterations everytime\n",
    "inputs_for_decoder['iterations'] = tf.constant([[25]])\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs_for_decoder)\n",
    "\n",
    "tf.assert_equal( tf.cast(decoder_results['predicted_ids'], tf.int32)\n",
    "                ,decoder_results_serialized['predicted_ids'])\n",
    "\n",
    "print(\"Sucess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top K top P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb_temp/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Save the model for greedy decoding\n",
    "saved_model_dir_strategy = 'model_pb_temp'\n",
    "\n",
    "decoder_layer = TextDecoderSerializableSeq2Seq(\n",
    "    model = enc_dec_model,\n",
    "    decode_start_token_id = 0,\n",
    "    encoder_hidden_size = 512,\n",
    "    decoder_hidden_size = 512,\n",
    "    max_iterations=None,\n",
    "    decoder_num_attention_heads=8,\n",
    "    decoder_num_layers=6,\n",
    "    mode=\"top_k_top_p\",\n",
    "    top_k = 25,\n",
    "    top_p = 0.75,\n",
    "    do_sample=False,\n",
    "    eos_id=-100,\n",
    ")\n",
    "# Convert whole operation to a model\n",
    "decoder_model  = decoder_layer.get_model()\n",
    "\n",
    "\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(saved_model_dir_strategy)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test top k top p Decoding\n",
    "\n",
    "Lets test, whether the results we obtained using **`TextDecoderSeq2Seq`** matches with **`TextDecoderSerializableSeq2Seq`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess\n"
     ]
    }
   ],
   "source": [
    "text_list = [\"summarize: studies have shown that owning a dog is good for you\", \n",
    "             \"translate: I love you so much\"]\n",
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='top_k_top_p', \n",
    "               max_iterations=25,\n",
    "               top_k = 25,\n",
    "               top_p = 0.75,\n",
    "               eos_id=-100)\n",
    "\n",
    "# # This is T5 model saved along with greedy decoder (as it is tf.while loop)\n",
    "# # we can save it together .\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir_strategy)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "\n",
    "inputs_for_decoder = inputs.copy()\n",
    "# We saved by passing max_iterations = None in TextDecoderSerializableSeq2Seq\n",
    "# So, we need to pass iterations everytime\n",
    "inputs_for_decoder['iterations'] = tf.constant([[25]])\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs_for_decoder)\n",
    "\n",
    "tf.assert_equal( tf.cast(decoder_results['predicted_ids'], tf.int32)\n",
    "                ,decoder_results_serialized['predicted_ids'])\n",
    "\n",
    "print(\"Sucess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
