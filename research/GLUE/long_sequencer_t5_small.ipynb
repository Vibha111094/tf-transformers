{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b6e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7f9f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daba6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/home/TF_NEW/tf-transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801234ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "647d34cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 25 09:41:23 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.126.02   Driver Version: 418.126.02   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    42W / 300W |      0MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    43W / 300W |      0MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac5085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4c578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f1d0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tf_transformers.models import T5Model, EncoderDecoder\n",
    "from transformers import T5TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1be145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da7a7107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.core import LegacyModel, LegacyLayer\n",
    "class Long_Model(LegacyLayer):\n",
    "    def __init__(\n",
    "        self, model_layer, num_splits,\n",
    "        gru_units,\n",
    "        activation=None, is_training=False, use_dropout=False, **kwargs\n",
    "    ):\n",
    "        super(Long_Model, self).__init__(\n",
    "            is_training=is_training, use_dropout=use_dropout, name=model_layer.name, **kwargs\n",
    "        )\n",
    "        self.model_layer = model_layer\n",
    "        self.num_splits = num_splits\n",
    "        self.gru_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(gru_units, return_sequences=True,\n",
    "                                                                           name='gru_for_logits', trainable=True))\n",
    "        # self.gru_layer_token_embeddings = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(gru_units, return_sequences=True))\n",
    "        \n",
    "        self._config_dict = model_layer._config_dict\n",
    "        self._mask_mode   = model_layer._mask_mode\n",
    "        self._sequence_length = model_layer._sequence_length\n",
    "        self.model_inputs, self.model_outputs = self.get_model(initialize_only=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        all_outputs_token_embeddings = []\n",
    "        inputs_splitted = {}\n",
    "        input_names = []\n",
    "        for k, v in inputs.items():\n",
    "            inputs_splitted[k] = tf.split(v, self.num_splits, axis=1)\n",
    "            input_names.append(k)\n",
    "            \n",
    "        for i in range(self.num_splits):\n",
    "            inputs_main = {}\n",
    "            for name in input_names:\n",
    "                inputs_main[name] = inputs_splitted[name][i]\n",
    "            model_outputs = self.model_layer(inputs_main)\n",
    "            # all_outputs_token_logits.append(model_outputs[\"token_logits\"])\n",
    "            all_outputs_token_embeddings.append(model_outputs['token_embeddings'])\n",
    "            \n",
    "        # token_logits_concatanted = tf.concat(all_outputs_token_logits, axis=1) # over sequence length\n",
    "\n",
    "        token_embeddings_concatanted = tf.concat(all_outputs_token_embeddings, axis=1) # over sequence length\n",
    "        token_embeddings_concatanted = self.gru_layer(token_embeddings_concatanted)\n",
    "        return {'token_embeddings': token_embeddings_concatanted}\n",
    "    \n",
    " \n",
    "    def get_model(self, initialize_only=False):\n",
    "        inputs = {}\n",
    "        for k, v in self.model_layer.model_inputs.items():\n",
    "            shape = v.shape\n",
    "            inputs[k] = tf.keras.layers.Input(\n",
    "                shape[1:], batch_size=shape[0], name= k, dtype=v.dtype\n",
    "            )\n",
    "        layer_output = self(inputs)\n",
    "        if initialize_only:\n",
    "            return inputs, layer_output\n",
    "        model = LegacyModel(inputs=inputs, outputs=layer_output, name=\"long_span_selection\")\n",
    "        model.model_config = self.model_layer._config_dict\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef51ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    gru_units = 384 # half of hidden dimension\n",
    "    model = T5Model.from_pretrained(\"t5-small\", \n",
    "                                           return_layer=True)\n",
    "    encoder = model._encoder\n",
    "    decoder = model._decoder\n",
    "    del model\n",
    "    long_model = Long_Model(encoder, num_splits=8, gru_units=gru_units)\n",
    "    \n",
    "    # long_model._layers[0]._embedding_layer = decoder._embedding_layer\n",
    "    #long_model._layers[0]._type_embeddings_layer = albert_decoder._type_embeddings_layer\n",
    "    #long_model._layers[0]._positional_embedding_layer = albert_decoder._positional_embedding_layer\n",
    "    \n",
    "    model_encoder = EncoderDecoder(encoder=encoder, decoder=decoder) \n",
    "    \n",
    "    model_encoder = model_encoder.get_model()\n",
    "    return model_encoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd1deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2432ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de64defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to features using specific length\n",
    "# into a temp dir (and log it as well for monitoring)\n",
    "\n",
    "def write_tfrecord(data, \n",
    "                    batch_size, \n",
    "                    tokenizer, \n",
    "                    encoder_max_length, \n",
    "                    decoder_max_length, \n",
    "                    mode, \n",
    "                    tfrecord_dir, \n",
    "                    take_sample=False, \n",
    "                    verbose=10000):\n",
    "    \n",
    "    if mode not in [\"train\", \"eval\"]:\n",
    "        raise ValueError(\"Inavlid mode `{}` specified. Available mode is ['train', 'eval']\".format(mode))\n",
    "    \n",
    "    def get_tfrecord_example(data):\n",
    "        for f in data:            \n",
    "            inputs_hf = tokenizer('long summarize:' + f['article'], \n",
    "                                  truncation=True, \n",
    "                                  max_length=encoder_max_length)\n",
    "\n",
    "            input_ids  = inputs_hf['input_ids'][:-1] # skip sep\n",
    "            input_mask = inputs_hf['attention_mask'][:-1] # skip sep\n",
    "            input_type_ids = [0] * len(input_ids)\n",
    "\n",
    "            decoder_input_ids = tokenizer(f['abstract'], \n",
    "                                  truncation=True, \n",
    "                                  max_length=decoder_max_seq_length)['input_ids']\n",
    "            \n",
    "            decoder_input_ids = [tokenizer.pad_token_id] + decoder_input_ids\n",
    "            # decoder_input_type_ids = [0] * len(decoder_input_ids)\n",
    "\n",
    "            result = {}\n",
    "            result['encoder_input_ids'] = input_ids\n",
    "            result['encoder_input_mask'] = input_mask\n",
    "            #result['encoder_input_type_ids'] = input_type_ids\n",
    "            result['decoder_input_ids'] = decoder_input_ids[:-1] # except last word\n",
    "            #result['decoder_input_type_ids'] = decoder_input_type_ids[:-1] # except last word\n",
    "\n",
    "            result['labels'] = decoder_input_ids[1:] # not including first word\n",
    "            result['labels_mask'] = [1] * len(result['labels'])\n",
    "\n",
    "                # Decoder doesnt need input_mask because by default decoder has causal mask mode\n",
    "\n",
    "            yield result\n",
    "\n",
    "    schema = {\n",
    "        \"encoder_input_ids\": (\"var_len\", \"int\"),\n",
    "        \"encoder_input_mask\": (\"var_len\", \"int\"),\n",
    "        \"decoder_input_ids\": (\"var_len\", \"int\"),\n",
    "        \"labels\": (\"var_len\", \"int\"),\n",
    "        \"labels_mask\": (\"var_len\", \"int\"),\n",
    "    }\n",
    "    \n",
    "    # Create a temp dir\n",
    "    if mode == \"train\":\n",
    "        # Write tf records\n",
    "        train_data_dir = os.path.join(tfrecord_dir,\"train\")        \n",
    "        tfrecord_filename = 'pubmed'\n",
    "        tfwriter = TFWriter(schema=schema, \n",
    "                            file_name=tfrecord_filename, \n",
    "                            model_dir=train_data_dir,\n",
    "                            tag='train',\n",
    "                            overwrite=True,\n",
    "                            verbose_counter=verbose\n",
    "                     )\n",
    "        data_train = data\n",
    "        # Take sample\n",
    "        if take_sample:\n",
    "            data_train = data_train.select(range(500))\n",
    "            \n",
    "        tfwriter.process(parse_fn=get_tfrecord_example(data_train))\n",
    "    if mode == \"eval\":\n",
    "        # Write tfrecords\n",
    "        eval_data_dir = os.path.join(tfrecord_dir,\"eval\")\n",
    "        tfrecord_filename = 'pubmed'\n",
    "        tfwriter = TFWriter(schema=schema, \n",
    "                            file_name=tfrecord_filename, \n",
    "                            model_dir=eval_data_dir,\n",
    "                            tag='eval',\n",
    "                            overwrite=True,\n",
    "                            verbose_counter=verbose\n",
    "                            )\n",
    "        data_eval = data\n",
    "        # Take sample\n",
    "        if take_sample:\n",
    "            data_eval = data_eval.select(range(500))\n",
    "        tfwriter.process(parse_fn=get_tfrecord_example(data_eval))\n",
    "        \n",
    "def read_tfrecord(tfrecord_dir, max_seq_length, batch_size, shuffle=False, drop_remainder=False):\n",
    "    \n",
    "        padded_shapes = {'encoder_input_ids': [max_seq_length,], \n",
    "                        'encoder_input_mask':[max_seq_length,],\n",
    "                        'decoder_input_ids': [None,],\n",
    "                        'labels': [None,], \n",
    "                        'labels_mask': [None,]\n",
    "                }\n",
    "        # Read tfrecord to dataset\n",
    "        schema = json.load(open(\"{}/schema.json\".format(tfrecord_dir)))\n",
    "        stats  = json.load(open('{}/stats.json'.format(tfrecord_dir)))\n",
    "        all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_dir))\n",
    "        tf_reader = TFReader(schema=schema, \n",
    "                            tfrecord_files=all_files)\n",
    "\n",
    "        x_keys = ['encoder_input_ids', 'encoder_input_mask', 'decoder_input_ids']\n",
    "        y_keys = ['labels', 'labels_mask']\n",
    "        dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                           keys=x_keys,\n",
    "                                           padded_shapes=padded_shapes,\n",
    "                                           batch_size=batch_size, \n",
    "                                           x_keys = x_keys, \n",
    "                                           y_keys = y_keys,\n",
    "                                           shuffle=shuffle, \n",
    "                                           drop_remainder=drop_remainder\n",
    "                                          )\n",
    "        return dataset, stats['total_records']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399faf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e287e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data specific configuration\n",
    "encoder_max_seq_length = 4096\n",
    "decoder_max_seq_length = 256\n",
    "\n",
    "take_sample = False\n",
    "train_batch_size = 6\n",
    "eval_batch_size  = 8\n",
    "\n",
    "# Trainer specifics\n",
    "device = \"gpu\"\n",
    "num_gpus = 2\n",
    "tpu_address = None\n",
    "dtype = \"fp32\"\n",
    "epochs = 3\n",
    "strategy = \"mirrored\"\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 3e-5\n",
    "loss_type = None\n",
    "return_all_layer_outputs = False\n",
    "if loss_type and loss_type == 'joint':\n",
    "    return_all_layer_outputs = True\n",
    "\n",
    "# Core data specifics\n",
    "data_name = \"scientific_papers\"\n",
    "#num_classes = cfg.glue.data.num_classes\n",
    "\n",
    "# Model specific\n",
    "is_training = False\n",
    "use_dropout = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "265988e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "from tf_transformers.data import TFWriter, TFReader\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2465ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7905f51d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaa3417b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scientific_papers (/home/jovyan/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"scientific_papers\", \"pubmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3ad69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFrecords\n",
    "# tfrecord_dir = tempfile.mkdtemp()\n",
    "tfrecord_dir = '/tmp/t5_pubmed_long_tfrecords_t5/'\n",
    "\n",
    "# # Train Tfrecords\n",
    "# write_tfrecord(dataset['train'], \n",
    "#                train_batch_size,\n",
    "#                tokenizer, \n",
    "#                encoder_max_seq_length, \n",
    "#                decoder_max_seq_length, \n",
    "#                \"train\", \n",
    "#                tfrecord_dir, \n",
    "#                take_sample, \n",
    "#                verbose=1000)\n",
    "\n",
    "# # # Eval Tfrecords\n",
    "# write_tfrecord(dataset['validation'], \n",
    "#                eval_batch_size,\n",
    "#                tokenizer, \n",
    "#                encoder_max_seq_length, \n",
    "#                decoder_max_seq_length, \n",
    "#                \"eval\", \n",
    "#                tfrecord_dir, \n",
    "#                take_sample, \n",
    "#               verbose=1000)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset, total_train_examples = read_tfrecord(tfrecord_dir + 'train', encoder_max_seq_length, train_batch_size, shuffle=True, drop_remainder=True)\n",
    "eval_dataset, total_eval_examples   = read_tfrecord(tfrecord_dir + 'eval', encoder_max_seq_length,  eval_batch_size,  shuffle=False, drop_remainder=False)\n",
    "\n",
    "# original_summaries = [item['summary'] for item in dataset['validation']]\n",
    "# callback = RougeCallback( model_ar, eval_dataset, original_summaries,\n",
    "#                          tokenizer, tokenizer.sep_token_id, tokenizer.cls_token_id, decoder_max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02333a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79d63ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decoder_input_ids': <tf.Tensor: shape=(6, 256), dtype=int32, numpy=\n",
      "array([[    0,     8,   778, ...,     3,     9, 12226],\n",
      "       [    0,     8,  8209, ...,     5,   175,   386],\n",
      "       [    0,    62,     3, ...,     0,     0,     0],\n",
      "       [    0,    46,     3, ...,     0,     0,     0],\n",
      "       [    0,   167,     3, ...,     0,     0,     0],\n",
      "       [    0,    62,  5530, ...,     0,     0,     0]], dtype=int32)>, 'encoder_input_ids': <tf.Tensor: shape=(6, 4096), dtype=int32, numpy=\n",
      "array([[  307, 21603,    10, ...,     0,     0,     0],\n",
      "       [  307, 21603,    10, ...,     0,     0,     0],\n",
      "       [  307, 21603,    10, ...,     0,     0,     0],\n",
      "       [  307, 21603,    10, ...,     0,     0,     0],\n",
      "       [  307, 21603,    10, ...,     0,     0,     0],\n",
      "       [  307, 21603,    10, ...,     0,     0,     0]], dtype=int32)>, 'encoder_input_mask': <tf.Tensor: shape=(6, 4096), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>} {'labels': <tf.Tensor: shape=(6, 256), dtype=int32, numpy=\n",
      "array([[    8,   778,  5786, ...,     9, 12226,     1],\n",
      "       [    8,  8209,    13, ...,   175,   386,     1],\n",
      "       [   62,     3,  2172, ...,     0,     0,     0],\n",
      "       [   46,     3,  4608, ...,     0,     0,     0],\n",
      "       [  167,     3,    53, ...,     0,     0,     0],\n",
      "       [   62,  5530,     3, ...,     0,     0,     0]], dtype=int32)>, 'labels_mask': <tf.Tensor: shape=(6, 256), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for (batch_inputs, batch_labels) in train_dataset:\n",
    "    print(batch_inputs, batch_labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f49b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af1385d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimizer fn\n",
    "\n",
    "from tf_transformers.optimization.adam_weighted import AdamWeightDecay\n",
    "def get_optimizer(learning_rate, examples, batch_size, epochs, learning_rate_type=\"polynomial\"):\n",
    "    steps_per_epoch = int(examples / batch_size)\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    warmup_steps = int(0.1 * num_train_steps)\n",
    "    \n",
    "    def optimizer_fn():\n",
    "        optimizer = AdamWeightDecay(learning_rate = learning_rate)\n",
    "        return optimizer\n",
    "    return optimizer_fn\n",
    "\n",
    "learning_rate = 0.0001\n",
    "optimizer_fn = get_optimizer(learning_rate, total_train_examples, train_batch_size, epochs, learning_rate_type=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24f215ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.losses import cross_entropy_loss, cross_entropy_loss_label_smoothing\n",
    "\n",
    "# Loss fn\n",
    "def get_loss(y_true_dict, y_pred_dict):\n",
    "    \n",
    "    loss = cross_entropy_loss_label_smoothing(labels=y_true_dict['labels'], \n",
    "                                   logits=y_pred_dict['token_logits'], \n",
    "                                      label_weights=y_true_dict['labels_mask'])\n",
    "    return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5df063ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "# Load trainer\n",
    "from tf_transformers.core import GPUTrainer\n",
    "strategy = 'mirrored'\n",
    "num_gpus = 2\n",
    "dtype = 'fp32'\n",
    "trainer = GPUTrainer(distribution_strategy=strategy, \n",
    "                    num_gpus=num_gpus, \n",
    "                    dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4868e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09659cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = total_train_examples//train_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493135a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a16fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Make sure `steps_per_epoch` should be less than or equal to number of batches in dataset.\n",
      "INFO:absl:Policy: ----> float32\n",
      "INFO:absl:Strategy: ---> <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f255c3e5c40>\n",
      "INFO:absl:Num GPU Devices: ---> 2\n",
      "You are using a model of type t5 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/t5-small/ckpt-1\n",
      "INFO:absl:No checkpoint found in /tmp/roberta2robera_pubmed_long_sequencet5/\n",
      "Epoch 1/3 --- Step 1/19987 --- total examples 0:   0%|          | 0/19987 [00:00<?, ?batch /s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 132 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 132 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 132 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 132 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1/3 --- Step 2405/19987 --- total examples 14424:  12%|█▏        | 2404/19987 [34:30<4:09:59,  1.17batch /s, learning_rate=1e-04, loss=1.13] "
     ]
    }
   ],
   "source": [
    "model_checkpoint_dir = \"/tmp/roberta2robera_pubmed_long_sequencet5/\"\n",
    "history = trainer.run(\n",
    "    model_fn = get_model,\n",
    "    optimizer_fn = optimizer_fn,\n",
    "    train_dataset = train_dataset,\n",
    "    train_loss_fn = get_loss,\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    model_checkpoint_dir=model_checkpoint_dir,\n",
    "    batch_size=train_batch_size,\n",
    "    training_loss_names=None,\n",
    "    validation_loss_names=None,\n",
    "    validation_dataset=None,\n",
    "    validation_loss_fn=None,\n",
    "    validation_interval_steps=None,\n",
    "    steps_per_call=1,\n",
    "    enable_xla=False,\n",
    "    callbacks=None,\n",
    "    callbacks_interval_steps=None,\n",
    "    overwrite_checkpoint_dir=True,\n",
    "    max_number_of_models=10,\n",
    "    model_save_interval_steps=None,\n",
    "    repeat_dataset=False,\n",
    "    latest_checkpoint=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2534bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd239ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861cb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
