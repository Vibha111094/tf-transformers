{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfda9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda116cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jovyan/TF_NEW/tf-transformers/src/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab8ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "import glob\n",
    "import datasets\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_transformers.data import TFReader, TFWriter\n",
    "from tf_transformers.models import RobertaModel, EncoderDecoder\n",
    "from tf_transformers.losses import cross_entropy_loss, cross_entropy_loss_label_smoothing\n",
    "\n",
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed256f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a810fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to features using specific length\n",
    "# into a temp dir (and log it as well for monitoring)\n",
    "\n",
    "def write_tfrecords(data, \n",
    "                    batch_size, \n",
    "                    tokenizer, \n",
    "                    encoder_max_length, \n",
    "                    decoder_max_length, \n",
    "                    mode, \n",
    "                    tfrecord_dir, \n",
    "                    take_sample=False, \n",
    "                    verbose=10000):\n",
    "    \n",
    "    if mode not in [\"train\", \"eval\"]:\n",
    "        raise ValueError(\"Inavlid mode `{}` specified. Available mode is ['train', 'eval']\".format(mode))\n",
    "    \n",
    "    def get_tfrecord_example(data):\n",
    "        result = {}\n",
    "        for f in data:\n",
    "            input_ids = [tokenizer.cls_token] + tokenizer.tokenize(f['article'])[: encoder_max_length-2] + [tokenizer.sep_token] # -2 to add CLS and SEP\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(input_ids)\n",
    "            input_mask = [1] * len(input_ids)\n",
    "            input_type_ids = [0] * len(input_ids)\n",
    "\n",
    "            decoder_input_ids = [tokenizer.cls_token] + tokenizer.tokenize(f['abstract'])[: decoder_max_length-2] + [tokenizer.sep_token]\n",
    "            decoder_input_ids = tokenizer.convert_tokens_to_ids(decoder_input_ids)\n",
    "            decoder_input_type_ids = [0] * len(decoder_input_ids)\n",
    "\n",
    "            result = {}\n",
    "            result['encoder_input_ids'] = input_ids\n",
    "            result['encoder_input_mask'] = input_mask\n",
    "            result['encoder_input_type_ids'] = input_type_ids\n",
    "            result['decoder_input_ids'] = decoder_input_ids[:-1] # except last word\n",
    "            result['decoder_input_type_ids'] = decoder_input_type_ids[:-1] # except last word\n",
    "\n",
    "            result['labels'] = decoder_input_ids[1:] # not including first word\n",
    "            result['labels_mask'] = [1] * len(decoder_input_ids[1:])\n",
    "\n",
    "            # Decoder doesnt need input_mask because by default decoder has causal mask mode\n",
    "\n",
    "            yield result\n",
    "\n",
    "    schema = {\n",
    "        \"encoder_input_ids\": (\"var_len\", \"int\"),\n",
    "        \"encoder_input_mask\": (\"var_len\", \"int\"),\n",
    "        \"encoder_input_type_ids\": (\"var_len\", \"int\"),\n",
    "        \"decoder_input_ids\": (\"var_len\", \"int\"),\n",
    "        \"decoder_input_type_ids\": (\"var_len\", \"int\"),\n",
    "        \"labels\": (\"var_len\", \"int\"),\n",
    "        \"labels_mask\": (\"var_len\", \"int\"),\n",
    "    }\n",
    "    \n",
    "    # Create a temp dir\n",
    "    if mode == \"train\":\n",
    "        # Write tf records\n",
    "        train_data_dir = os.path.join(tfrecord_dir,\"train\")        \n",
    "        tfrecord_filename = 'pubmed'\n",
    "        tfwriter = TFWriter(schema=schema, \n",
    "                            file_name=tfrecord_filename, \n",
    "                            model_dir=train_data_dir,\n",
    "                            tag='train',\n",
    "                            overwrite=False\n",
    "                     )\n",
    "        data_train = data['train']\n",
    "        # Take sample\n",
    "        if take_sample:\n",
    "            data_train = data_train.select(range(500))\n",
    "            \n",
    "        tfwriter.process(parse_fn=get_tfrecord_example(data_train), verbose=verbose)\n",
    "    if mode == \"eval\":\n",
    "        # Write tfrecords\n",
    "        eval_data_dir = os.path.join(tfrecord_dir,\"eval\")\n",
    "        tfrecord_filename = 'pubmed'\n",
    "        tfwriter = TFWriter(schema=schema, \n",
    "                            file_name=tfrecord_filename, \n",
    "                            model_dir=eval_data_dir,\n",
    "                            tag='dev',\n",
    "                            overwrite=False\n",
    "                            )\n",
    "        data_eval = data['validation']\n",
    "        # Take sample\n",
    "        if take_sample:\n",
    "            data_eval = data_eval.select(range(500))\n",
    "        tfwriter.process(parse_fn=get_tfrecord_example(data_eval), verbose=verbose)\n",
    "        \n",
    "def read_tfrecord(tfrecord_dir, batch_size, shuffle=False, drop_remainder=False):\n",
    "        # Read tfrecord to dataset\n",
    "        schema = json.load(open(\"{}/schema.json\".format(tfrecord_dir)))\n",
    "        stats  = json.load(open('{}/stats.json'.format(tfrecord_dir)))\n",
    "        all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_dir))\n",
    "        tf_reader = TFReader(schema=schema, \n",
    "                            tfrecord_files=all_files)\n",
    "\n",
    "        x_keys = ['encoder_input_ids', 'encoder_input_type_ids', 'encoder_input_mask', 'decoder_input_ids', 'decoder_input_type_ids']\n",
    "        y_keys = ['labels', 'labels_mask']\n",
    "        dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                           keys=x_keys,\n",
    "                                           batch_size=batch_size, \n",
    "                                           x_keys = x_keys, \n",
    "                                           y_keys = y_keys,\n",
    "                                           shuffle=shuffle, \n",
    "                                           drop_remainder=drop_remainder\n",
    "                                          )\n",
    "        return dataset, stats['total_records']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8229894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0109c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "def get_model():\n",
    "    encoder = RobertaModel.from_pretrained(\"roberta-base\", return_layer=True)\n",
    "    decoder = RobertaModel.from_pretrained(\"roberta-base\",use_decoder=True, return_layer=True, mask_mode='causal')\n",
    "\n",
    "    # Assign all possible encoder variables to decoder\n",
    "    encoder_dict = {var.name: var for var in encoder.variables}\n",
    "    assigned_counter = 0\n",
    "    for var in decoder.variables:\n",
    "        if var.name in encoder_dict:\n",
    "            var.assign(encoder_dict[var.name])\n",
    "            assigned_counter += 1\n",
    "    print(\"Assigned {} variables from encoder to decoder .\".format(assigned_counter))\n",
    "    del encoder_dict\n",
    "    print(\"ENncoder variables {} and Decoder variables {}\".format(len(encoder.variables), len(decoder.variables)))\n",
    "    model = EncoderDecoder(encoder=encoder, decoder=decoder, share_embeddings=True) \n",
    "    model = model.get_model()\n",
    "    print(\"Model variables {}\".format(len(model.variables)))\n",
    "\n",
    "    del encoder\n",
    "    del decoder\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1d653c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854de9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f7cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data specific configuration\n",
    "encoder_max_seq_length = 512\n",
    "decoder_max_seq_length = 64\n",
    "\n",
    "take_sample = False\n",
    "train_batch_size = 32\n",
    "eval_batch_size  = 32\n",
    "\n",
    "# Trainer specifics\n",
    "device = \"gpu\"\n",
    "num_gpus = 2\n",
    "tpu_address = None\n",
    "dtype = \"fp32\"\n",
    "epochs = 3\n",
    "strategy = \"mirrored\"\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 3e-5\n",
    "loss_type = None\n",
    "return_all_layer_outputs = False\n",
    "if loss_type and loss_type == 'joint':\n",
    "    return_all_layer_outputs = True\n",
    "\n",
    "# Core data specifics\n",
    "data_name = \"scientific_papers\"\n",
    "#num_classes = cfg.glue.data.num_classes\n",
    "\n",
    "# Model specific\n",
    "is_training = True\n",
    "use_dropout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d8386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18dc6534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/roberta-base/ckpt-1\n",
      "You are using a model of type roberta to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/roberta-base/ckpt-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned 132 variables from encoder to decoder .\n",
      "ENncoder variables 204 and Decoder variables 324\n",
      "Model variables 525\n"
     ]
    }
   ],
   "source": [
    "# Autoregressive model\n",
    "\n",
    "encoder = RobertaModel.from_pretrained(\"roberta-base\", return_layer=True)\n",
    "decoder = RobertaModel.from_pretrained(\"roberta-base\", mask_mode='causal', use_decoder=True, use_auto_regressive=True, return_layer=True)\n",
    "# Assign all possible encoder variables to decoder\n",
    "encoder_dict = {var.name: var for var in encoder.variables}\n",
    "assigned_counter = 0\n",
    "for var in decoder.variables:\n",
    "    if var.name in encoder_dict:\n",
    "        var.assign(encoder_dict[var.name])\n",
    "        assigned_counter += 1\n",
    "print(\"Assigned {} variables from encoder to decoder .\".format(assigned_counter))\n",
    "del encoder_dict\n",
    "print(\"ENncoder variables {} and Decoder variables {}\".format(len(encoder.variables), len(decoder.variables)))\n",
    "model_ar = EncoderDecoder(encoder=encoder, decoder=decoder, share_embeddings=True) \n",
    "print(\"Model variables {}\".format(len(model_ar.variables)))\n",
    "\n",
    "del encoder\n",
    "del decoder\n",
    "\n",
    "# Important\n",
    "model_ar = model_ar.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8e46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f66f41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tqdm\n",
    "from tf_transformers.text import TextDecoder\n",
    "\n",
    "class RougeCallback():\n",
    "    \n",
    "    def __init__(self, model, eval_dataset, original_summaries, tokenizer, eos_id, decoder_start_id, max_iterations):\n",
    "        \n",
    "        self.model = model\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.original_summaries = original_summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos_id = eos_id\n",
    "        self.decoder_start_id = decoder_start_id\n",
    "        self.max_iterations = max_iterations\n",
    "        \n",
    "    def __call__(self, train_kwargs):\n",
    "        \n",
    "        self.model.set_weights(train_kwargs['model'].get_weights())\n",
    "        decoder = TextDecoder(self.model, decoder_start_token_id=self.decoder_start_id, input_type_ids=0)\n",
    "        \n",
    "        # Predictions\n",
    "        predicted_summaries = []\n",
    "        for (batch_inputs, batch_labels) in tqdm.tqdm(eval_dataset):\n",
    "            del batch_inputs['decoder_input_ids']\n",
    "            decoder_outputs = decoder.decode(batch_inputs, mode='greedy', max_iterations=self.max_iterations, eos_id=self.eos_id)\n",
    "            predicted_ids = [item[0] for item in decoder_outputs['predicted_ids'].numpy().tolist()]\n",
    "\n",
    "            predicted_ids_sliced = []\n",
    "            for p_id in predicted_ids:\n",
    "                if self.eos_id in p_id:\n",
    "                    index = p_id.index(self.eos_id)\n",
    "                    p_id = p_id[:index]\n",
    "                predicted_ids_sliced.append(p_id)\n",
    "                predicted_summaries.append(self.tokenizer.decode(p_id))\n",
    "                \n",
    "        rouge = datasets.load_metric(\"rouge\")\n",
    "        rouge_output2 = rouge.compute(predictions=predicted_summaries, references=self.original_summaries, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "        rouge_output1 = rouge.compute(predictions=predicted_summaries, references=self.original_summaries, rouge_types=[\"rouge1\"])[\"rouge1\"].mid\n",
    "        rouge_outputL = rouge.compute(predictions=predicted_summaries, references=self.original_summaries, rouge_types=[\"rougeL\"])[\"rougeL\"].mid\n",
    "\n",
    "        rouge2 = {'rouge2_precision': rouge_output2.precision,\n",
    "                  'rouge2_recall': rouge_output2.recall,\n",
    "                  'rouge2_f1': rouge_output2.fmeasure}\n",
    "        rouge2['rouge1_precision'] = rouge_output1.precision\n",
    "        rouge2['rouge1_recall'] = rouge_output1.recall\n",
    "        rouge2['rouge1_f1'] = rouge_output1.fmeasure\n",
    "\n",
    "        rouge2['rougeL_precision'] = rouge_outputL.precision\n",
    "        rouge2['rougeL_recall'] = rouge_outputL.recall\n",
    "        rouge2['rougeL_f1'] = rouge_outputL.fmeasure\n",
    "        return rouge2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a1eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe0d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7f37752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset scientific_papers (/home/jovyan/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"scientific_papers\", \"pubmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb5d84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFrecords\n",
    "# tfrecord_dir = tempfile.mkdtemp()\n",
    "tfrecord_dir = '/tmp/roberta2robera_tfrecords/'\n",
    "\n",
    "# # Train Tfrecords\n",
    "# write_tfrecord(dataset, \n",
    "#                train_batch_size,\n",
    "#                tokenizer, \n",
    "#                encoder_max_seq_length, \n",
    "#                decoder_max_seq_length, \n",
    "#                \"train\", \n",
    "#                tfrecord_dir, \n",
    "#                take_sample)\n",
    "\n",
    "# # Eval Tfrecords\n",
    "# write_tfrecord(dataset, \n",
    "#                eval_atch_size,\n",
    "#                tokenizer, \n",
    "#                encoder_max_seq_length, \n",
    "#                decoder_max_seq_length, \n",
    "#                \"eval\", \n",
    "#                tfrecord_dir, \n",
    "#                take_sample)\n",
    "\n",
    "train_dataset, total_train_examples = read_tfrecord(tfrecord_dir + 'train', train_batch_size, shuffle=False, drop_remainder=False)\n",
    "eval_dataset, total_eval_examples   = read_tfrecord(tfrecord_dir + 'eval', eval_batch_size,  shuffle=False, drop_remainder=False)\n",
    "\n",
    "original_summaries = [item['abstract'] for item in dataset['validation']]\n",
    "callback = RougeCallback( model_ar, eval_dataset, original_summaries,\n",
    "                         tokenizer, tokenizer.sep_token_id, tokenizer.cls_token_id, decoder_max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a2823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2947cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_summaries = [item['abstract'] for item in dataset['validation']]\n",
    "# eval_dataset = eval_dataset.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e221284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = RougeCallback( model_ar, eval_dataset, original_summaries,\n",
    "                         tokenizer, tokenizer.sep_token_id, tokenizer.cls_token_id, decoder_max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13e16a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdf47961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimizer fn\n",
    "\n",
    "from tf_transformers.optimization import create_optimizer\n",
    "def get_optimizer(learning_rate, examples, batch_size, epochs, learning_rate_type=\"polynomial\"):\n",
    "    steps_per_epoch = int(examples / batch_size)\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    warmup_steps = int(0.1 * num_train_steps)\n",
    "    \n",
    "    def optimizer_fn():\n",
    "        optimizer, learning_rate_fn = create_optimizer(learning_rate,\n",
    "                                                   num_train_steps,\n",
    "                                                   num_train_steps, \n",
    "                                                      learning_rate_type=learning_rate_type)\n",
    "        return optimizer\n",
    "    return optimizer_fn\n",
    "\n",
    "optimizer_fn = get_optimizer(learning_rate, total_train_examples, train_batch_size, epochs, learning_rate_type=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89c13c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss fn\n",
    "\n",
    "def get_loss(y_true_dict, y_pred_dict):\n",
    "    \n",
    "    loss = cross_entropy_loss(labels=y_true_dict['labels'], \n",
    "                                   logits=y_pred_dict['token_logits'], \n",
    "                                      label_weights=y_true_dict['labels_mask'])\n",
    "    return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b915ec83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4761ed6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "# Load trainer\n",
    "from tf_transformers.core import GPUTrainer\n",
    "trainer = GPUTrainer(distribution_strategy=strategy, \n",
    "                    num_gpus=num_gpus, \n",
    "                    dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164db72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cff6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = get_model\n",
    "train_loss_fn = get_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0e552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Make sure `steps_per_epoch` should be less than or equal to number of batches in dataset.\n",
      "INFO:absl:Policy: ----> float32\n",
      "INFO:absl:Strategy: ---> <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fd7502dea90>\n",
      "INFO:absl:Num GPU Devices: ---> 2\n",
      "You are using a model of type roberta to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/roberta-base/ckpt-1\n",
      "You are using a model of type roberta to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/roberta-base/ckpt-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned 204 variables from encoder to decoder .\n",
      "ENncoder variables 204 and Decoder variables 324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using Adamw optimizer\n",
      "INFO:absl:No checkpoint found in /tmp/roberta2robera_pubmed_short/\n",
      "Epoch 1/3 --- Step 100/3000 --- total examples 0:   0%|          | 0/30 [00:00<?, ?batch /s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model variables 525\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 515 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 515 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 515 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 515 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1/3 --- Step 3000/3000 --- total examples 92800: 100%|██████████| 30/30 [48:59<00:00, 97.99s/batch , learning_rate=7.87e-6, loss=2.4] \n",
      "INFO:absl:Model saved at epoch 1\n",
      "INFO:absl:Callbacks in progress at epoch end 1 . . . .\n",
      "2it [00:21, 10.88s/it]"
     ]
    }
   ],
   "source": [
    "model_checkpoint_dir = \"/tmp/roberta2robera_pubmed_short/\"\n",
    "history = trainer.run(\n",
    "    model_fn = model_fn,\n",
    "    optimizer_fn = optimizer_fn,\n",
    "    train_dataset = train_dataset,\n",
    "    train_loss_fn = train_loss_fn,\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch = 3000,\n",
    "    model_checkpoint_dir=model_checkpoint_dir,\n",
    "    batch_size=train_batch_size,\n",
    "    training_loss_names=None,\n",
    "    validation_loss_names=None,\n",
    "    validation_dataset=None,\n",
    "    validation_loss_fn=None,\n",
    "    validation_interval_steps=None,\n",
    "    steps_per_call=100,\n",
    "    enable_xla=False,\n",
    "    callbacks=[callback],\n",
    "    callbacks_interval_steps=None,\n",
    "    overwrite_checkpoint_dir=True,\n",
    "    max_number_of_models=10,\n",
    "    model_save_interval_steps=None,\n",
    "    repeat_dataset=False,\n",
    "    latest_checkpoint=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c5490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce1f8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "with open(\"{}/history.json\".format(model_checkpoint_dir), \"w\") as f:\n",
    "    json.dump(str(history),f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "732aeeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(model_checkpoint_dir)\n",
    "shutil.rmtree(tfrecord_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87c1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ecb28f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.encoder_decoder.EncoderDecoder object at 0x7fdb97a1bca0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7fdb803d8a60>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.encoder_decoder.EncoderDecoder object at 0x7fdb97a1bca0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7fdb803d8a60>).\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/roberta2robera_pubmed_short/ckpt-3\n"
     ]
    }
   ],
   "source": [
    "model_ar.load_checkpoint(model_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd48994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
