{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfda9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda116cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jovyan/TF_NEW/tf-transformers/src/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab8ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "import glob\n",
    "import datasets\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_transformers.data import TFReader, TFWriter\n",
    "from tf_transformers.models import RobertaModel, EncoderDecoder\n",
    "from tf_transformers.losses import cross_entropy_loss, cross_entropy_loss_label_smoothing\n",
    "\n",
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed256f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a810fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to features using specific length\n",
    "# into a temp dir (and log it as well for monitoring)\n",
    "\n",
    "def write_tfrecords(data, \n",
    "                    batch_size, \n",
    "                    tokenizer, \n",
    "                    encoder_max_length, \n",
    "                    decoder_max_length, \n",
    "                    mode, \n",
    "                    tfrecord_dir, \n",
    "                    take_sample=False, \n",
    "                    verbose=10000):\n",
    "    \n",
    "    if mode not in [\"train\", \"eval\"]:\n",
    "        raise ValueError(\"Inavlid mode `{}` specified. Available mode is ['train', 'eval']\".format(mode))\n",
    "    \n",
    "    def get_tfrecord_example(data):\n",
    "        result = {}\n",
    "        for f in data:\n",
    "            input_ids = [tokenizer.cls_token] + tokenizer.tokenize(f['article'])[: encoder_max_length-2] + [tokenizer.sep_token] # -2 to add CLS and SEP\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(input_ids)\n",
    "            input_mask = [1] * len(input_ids)\n",
    "            input_type_ids = [0] * len(input_ids)\n",
    "\n",
    "            decoder_input_ids = [tokenizer.cls_token] + tokenizer.tokenize(f['abstract'])[: decoder_max_length-2] + [tokenizer.sep_token]\n",
    "            decoder_input_ids = tokenizer.convert_tokens_to_ids(decoder_input_ids)\n",
    "            decoder_input_type_ids = [0] * len(decoder_input_ids)\n",
    "\n",
    "            result = {}\n",
    "            result['encoder_input_ids'] = input_ids\n",
    "            result['encoder_input_mask'] = input_mask\n",
    "            result['encoder_input_type_ids'] = input_type_ids\n",
    "            result['decoder_input_ids'] = decoder_input_ids[:-1] # except last word\n",
    "            result['decoder_input_type_ids'] = decoder_input_type_ids[:-1] # except last word\n",
    "\n",
    "            result['labels'] = decoder_input_ids[1:] # not including first word\n",
    "            result['labels_mask'] = [1] * len(decoder_input_ids[1:])\n",
    "\n",
    "            # Decoder doesnt need input_mask because by default decoder has causal mask mode\n",
    "\n",
    "            yield result\n",
    "\n",
    "    schema = {\n",
    "        \"encoder_input_ids\": (\"var_len\", \"int\"),\n",
    "        \"encoder_input_mask\": (\"var_len\", \"int\"),\n",
    "        \"encoder_input_type_ids\": (\"var_len\", \"int\"),\n",
    "        \"decoder_input_ids\": (\"var_len\", \"int\"),\n",
    "        \"decoder_input_type_ids\": (\"var_len\", \"int\"),\n",
    "        \"labels\": (\"var_len\", \"int\"),\n",
    "        \"labels_mask\": (\"var_len\", \"int\"),\n",
    "    }\n",
    "    \n",
    "    # Create a temp dir\n",
    "    if mode == \"train\":\n",
    "        # Write tf records\n",
    "        train_data_dir = os.path.join(tfrecord_dir,\"train\")        \n",
    "        tfrecord_filename = 'pubmed'\n",
    "        tfwriter = TFWriter(schema=schema, \n",
    "                            file_name=tfrecord_filename, \n",
    "                            model_dir=train_data_dir,\n",
    "                            tag='train',\n",
    "                            overwrite=False\n",
    "                     )\n",
    "        data_train = data['train']\n",
    "        # Take sample\n",
    "        if take_sample:\n",
    "            data_train = data_train.select(range(500))\n",
    "            \n",
    "        tfwriter.process(parse_fn=get_tfrecord_example(data_train), verbose=verbose)\n",
    "    if mode == \"eval\":\n",
    "        # Write tfrecords\n",
    "        eval_data_dir = os.path.join(tfrecord_dir,\"eval\")\n",
    "        tfrecord_filename = 'pubmed'\n",
    "        tfwriter = TFWriter(schema=schema, \n",
    "                            file_name=tfrecord_filename, \n",
    "                            model_dir=eval_data_dir,\n",
    "                            tag='dev',\n",
    "                            overwrite=False\n",
    "                            )\n",
    "        data_eval = data['validation']\n",
    "        # Take sample\n",
    "        if take_sample:\n",
    "            data_eval = data_eval.select(range(500))\n",
    "        tfwriter.process(parse_fn=get_tfrecord_example(data_eval), verbose=verbose)\n",
    "        \n",
    "def read_tfrecord(tfrecord_dir, batch_size, shuffle=False, drop_remainder=False):\n",
    "        # Read tfrecord to dataset\n",
    "        schema = json.load(open(\"{}/schema.json\".format(tfrecord_dir)))\n",
    "        stats  = json.load(open('{}/stats.json'.format(tfrecord_dir)))\n",
    "        all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_dir))\n",
    "        tf_reader = TFReader(schema=schema, \n",
    "                            tfrecord_files=all_files)\n",
    "\n",
    "        x_keys = ['encoder_input_ids', 'encoder_input_type_ids', 'encoder_input_mask', 'decoder_input_ids', 'decoder_input_type_ids']\n",
    "        y_keys = ['labels', 'labels_mask']\n",
    "        dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                           keys=x_keys,\n",
    "                                           batch_size=batch_size, \n",
    "                                           x_keys = x_keys, \n",
    "                                           y_keys = y_keys,\n",
    "                                           shuffle=shuffle, \n",
    "                                           drop_remainder=drop_remainder\n",
    "                                          )\n",
    "        return dataset, stats['total_records']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8229894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0109c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "def get_model():\n",
    "    encoder = RobertaModel.from_pretrained(\"roberta-base\", return_layer=True)\n",
    "    decoder = RobertaModel.from_pretrained(\"roberta-base\",use_decoder=True, return_layer=True, mask_mode='causal')\n",
    "\n",
    "    # Assign all possible encoder variables to decoder\n",
    "    encoder_dict = {var.name: var for var in encoder.variables}\n",
    "    assigned_counter = 0\n",
    "    for var in decoder.variables:\n",
    "        if var.name in encoder_dict:\n",
    "            var.assign(encoder_dict[var.name])\n",
    "            assigned_counter += 1\n",
    "    print(\"Assigned {} variables from encoder to decoder .\".format(assigned_counter))\n",
    "    del encoder_dict\n",
    "    print(\"ENncoder variables {} and Decoder variables {}\".format(len(encoder.variables), len(decoder.variables)))\n",
    "    model = EncoderDecoder(encoder=encoder, decoder=decoder, share_embeddings=True) \n",
    "    model = model.get_model()\n",
    "    print(\"Model variables {}\".format(len(model.variables)))\n",
    "\n",
    "    del encoder\n",
    "    del decoder\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1d653c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854de9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f7cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data specific configuration\n",
    "encoder_max_seq_length = 512\n",
    "decoder_max_seq_length = 64\n",
    "\n",
    "take_sample = False\n",
    "train_batch_size = 32\n",
    "eval_batch_size  = 32\n",
    "\n",
    "# Trainer specifics\n",
    "device = \"gpu\"\n",
    "num_gpus = 2\n",
    "tpu_address = None\n",
    "dtype = \"fp32\"\n",
    "epochs = 3\n",
    "strategy = \"mirrored\"\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 3e-5\n",
    "loss_type = None\n",
    "return_all_layer_outputs = False\n",
    "if loss_type and loss_type == 'joint':\n",
    "    return_all_layer_outputs = True\n",
    "\n",
    "# Core data specifics\n",
    "data_name = \"scientific_papers\"\n",
    "#num_classes = cfg.glue.data.num_classes\n",
    "\n",
    "# Model specific\n",
    "is_training = True\n",
    "use_dropout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d8386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18dc6534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/roberta-base/ckpt-1\n",
      "You are using a model of type roberta to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/roberta-base/ckpt-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned 132 variables from encoder to decoder .\n",
      "ENncoder variables 204 and Decoder variables 324\n",
      "Model variables 525\n"
     ]
    }
   ],
   "source": [
    "# Autoregressive model\n",
    "\n",
    "encoder = RobertaModel.from_pretrained(\"roberta-base\", return_layer=True)\n",
    "decoder = RobertaModel.from_pretrained(\"roberta-base\", mask_mode='causal', use_decoder=True, use_auto_regressive=True, return_layer=True)\n",
    "# Assign all possible encoder variables to decoder\n",
    "encoder_dict = {var.name: var for var in encoder.variables}\n",
    "assigned_counter = 0\n",
    "for var in decoder.variables:\n",
    "    if var.name in encoder_dict:\n",
    "        var.assign(encoder_dict[var.name])\n",
    "        assigned_counter += 1\n",
    "print(\"Assigned {} variables from encoder to decoder .\".format(assigned_counter))\n",
    "del encoder_dict\n",
    "print(\"ENncoder variables {} and Decoder variables {}\".format(len(encoder.variables), len(decoder.variables)))\n",
    "model_ar = EncoderDecoder(encoder=encoder, decoder=decoder, share_embeddings=True) \n",
    "print(\"Model variables {}\".format(len(model_ar.variables)))\n",
    "\n",
    "del encoder\n",
    "del decoder\n",
    "\n",
    "# Important\n",
    "model_ar = model_ar.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8e46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f66f41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tqdm\n",
    "from tf_transformers.text import TextDecoder\n",
    "\n",
    "class RougeCallback():\n",
    "    \n",
    "    def __init__(self, model, eval_dataset, original_summaries, tokenizer, eos_id, decoder_start_id, max_iterations):\n",
    "        \n",
    "        self.model = model\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.original_summaries = original_summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos_id = eos_id\n",
    "        self.decoder_start_id = decoder_start_id\n",
    "        self.max_iterations = max_iterations\n",
    "        \n",
    "    def __call__(self, train_kwargs):\n",
    "        \n",
    "        self.model.set_weights(train_kwargs['model'].get_weights())\n",
    "        decoder = TextDecoder(self.model, decoder_start_token_id=self.decoder_start_id, input_type_ids=0)\n",
    "        \n",
    "        # Predictions\n",
    "        predicted_summaries = []\n",
    "        for (batch_inputs, batch_labels) in tqdm.tqdm(eval_dataset):\n",
    "            del batch_inputs['decoder_input_ids']\n",
    "            decoder_outputs = decoder.decode(batch_inputs, mode='greedy', max_iterations=self.max_iterations, eos_id=self.eos_id)\n",
    "            predicted_ids = [item[0] for item in decoder_outputs['predicted_ids'].numpy().tolist()]\n",
    "\n",
    "            predicted_ids_sliced = []\n",
    "            for p_id in predicted_ids:\n",
    "                if self.eos_id in p_id:\n",
    "                    index = p_id.index(self.eos_id)\n",
    "                    p_id = p_id[:index]\n",
    "                predicted_ids_sliced.append(p_id)\n",
    "                predicted_summaries.append(self.tokenizer.decode(p_id))\n",
    "                \n",
    "        rouge = datasets.load_metric(\"rouge\")\n",
    "        rouge_output2 = rouge.compute(predictions=predicted_summaries, references=self.original_summaries, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "        rouge_output1 = rouge.compute(predictions=predicted_summaries, references=self.original_summaries, rouge_types=[\"rouge1\"])[\"rouge1\"].mid\n",
    "        rouge_outputL = rouge.compute(predictions=predicted_summaries, references=self.original_summaries, rouge_types=[\"rougeL\"])[\"rougeL\"].mid\n",
    "\n",
    "        rouge2 = {'rouge2_precision': rouge_output2.precision,\n",
    "                  'rouge2_recall': rouge_output2.recall,\n",
    "                  'rouge2_f1': rouge_output2.fmeasure}\n",
    "        rouge2['rouge1_precision'] = rouge_output1.precision\n",
    "        rouge2['rouge1_recall'] = rouge_output1.recall\n",
    "        rouge2['rouge1_f1'] = rouge_output1.fmeasure\n",
    "\n",
    "        rouge2['rougeL_precision'] = rouge_outputL.precision\n",
    "        rouge2['rougeL_recall'] = rouge_outputL.recall\n",
    "        rouge2['rougeL_f1'] = rouge_outputL.fmeasure\n",
    "        return rouge2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a1eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe0d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7f37752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset scientific_papers (/home/jovyan/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"scientific_papers\", \"pubmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb5d84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFrecords\n",
    "# tfrecord_dir = tempfile.mkdtemp()\n",
    "tfrecord_dir = '/tmp/roberta2robera_tfrecords/'\n",
    "\n",
    "# # Train Tfrecords\n",
    "# write_tfrecord(dataset, \n",
    "#                train_batch_size,\n",
    "#                tokenizer, \n",
    "#                encoder_max_seq_length, \n",
    "#                decoder_max_seq_length, \n",
    "#                \"train\", \n",
    "#                tfrecord_dir, \n",
    "#                take_sample)\n",
    "\n",
    "# # Eval Tfrecords\n",
    "# write_tfrecord(dataset, \n",
    "#                eval_atch_size,\n",
    "#                tokenizer, \n",
    "#                encoder_max_seq_length, \n",
    "#                decoder_max_seq_length, \n",
    "#                \"eval\", \n",
    "#                tfrecord_dir, \n",
    "#                take_sample)\n",
    "\n",
    "train_dataset, total_train_examples = read_tfrecord(tfrecord_dir + 'train', train_batch_size, shuffle=False, drop_remainder=False)\n",
    "eval_dataset, total_eval_examples   = read_tfrecord(tfrecord_dir + 'eval', eval_batch_size,  shuffle=False, drop_remainder=False)\n",
    "\n",
    "original_summaries = [item['abstract'] for item in dataset['validation']]\n",
    "callback = RougeCallback( model_ar, eval_dataset, original_summaries,\n",
    "                         tokenizer, tokenizer.sep_token_id, tokenizer.cls_token_id, decoder_max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8bb5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33f228d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_summaries = [item['abstract'] for item in dataset['validation']]\n",
    "# eval_dataset = eval_dataset.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d5e57e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = RougeCallback( model_ar, eval_dataset, original_summaries,\n",
    "                         tokenizer, tokenizer.sep_token_id, tokenizer.cls_token_id, decoder_max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13e16a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdf47961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimizer fn\n",
    "\n",
    "from tf_transformers.optimization import create_optimizer\n",
    "def get_optimizer(learning_rate, examples, batch_size, epochs, learning_rate_type=\"polynomial\"):\n",
    "    steps_per_epoch = int(examples / batch_size)\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    warmup_steps = int(0.1 * num_train_steps)\n",
    "    \n",
    "    def optimizer_fn():\n",
    "        optimizer, learning_rate_fn = create_optimizer(learning_rate,\n",
    "                                                   num_train_steps,\n",
    "                                                   num_train_steps, \n",
    "                                                      learning_rate_type=learning_rate_type)\n",
    "        return optimizer\n",
    "    return optimizer_fn\n",
    "\n",
    "optimizer_fn = get_optimizer(learning_rate, total_train_examples, train_batch_size, epochs, learning_rate_type=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89c13c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss fn\n",
    "\n",
    "def get_loss(y_true_dict, y_pred_dict):\n",
    "    \n",
    "    loss = cross_entropy_loss(labels=y_true_dict['labels'], \n",
    "                                   logits=y_pred_dict['token_logits'], \n",
    "                                      label_weights=y_true_dict['labels_mask'])\n",
    "    return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b915ec83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4761ed6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "# Load trainer\n",
    "from tf_transformers.core import GPUTrainer\n",
    "trainer = GPUTrainer(distribution_strategy=strategy, \n",
    "                    num_gpus=num_gpus, \n",
    "                    dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164db72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cff6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = get_model\n",
    "train_loss_fn = get_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31c0e552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Make sure `steps_per_epoch` should be less than or equal to number of batches in dataset.\n",
      "INFO:absl:Policy: ----> float32\n",
      "INFO:absl:Strategy: ---> <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fd7502dea90>\n",
      "INFO:absl:Num GPU Devices: ---> 2\n",
      "You are using a model of type roberta to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/roberta-base/ckpt-1\n",
      "You are using a model of type roberta to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/roberta-base/ckpt-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned 204 variables from encoder to decoder .\n",
      "ENncoder variables 204 and Decoder variables 324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using Adamw optimizer\n",
      "INFO:absl:No checkpoint found in /tmp/roberta2robera_pubmed_short/\n",
      "Epoch 1/3 --- Step 100/3000 --- total examples 0:   0%|          | 0/30 [00:00<?, ?batch /s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model variables 525\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 515 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 515 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 515 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 515 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1/3 --- Step 3000/3000 --- total examples 92800: 100%|██████████| 30/30 [48:59<00:00, 97.99s/batch , learning_rate=7.87e-6, loss=2.4] \n",
      "INFO:absl:Model saved at epoch 1\n",
      "INFO:absl:Callbacks in progress at epoch end 1 . . . .\n",
      "208it [36:58, 10.66s/it]\n",
      "INFO:absl:Call back score {'rouge2_precision': 4.675481473829081e-05, 'rouge2_recall': 8.224765618668125e-06, 'rouge2_f1': 1.295005661227129e-05, 'rouge1_precision': 0.030340200582526004, 'rouge1_recall': 0.005445612480301799, 'rouge1_f1': 0.008295224104245928, 'rougeL_precision': 0.029599881402409503, 'rougeL_recall': 0.005258862329837096, 'rougeL_f1': 0.008012075240641786}\n",
      "Epoch 2/3 --- Step 3000/3000 --- total examples 188800: 100%|██████████| 30/30 [47:19<00:00, 94.64s/batch , learning_rate=1.59e-5, loss=1.94]\n",
      "INFO:absl:Model saved at epoch 2\n",
      "INFO:absl:Callbacks in progress at epoch end 2 . . . .\n",
      "208it [36:53, 10.64s/it]\n",
      "INFO:absl:Call back score {'rouge2_precision': 0.00014394939938071474, 'rouge2_recall': 4.374418267220742e-05, 'rouge2_f1': 6.348429743557746e-05, 'rouge1_precision': 0.09668412957318645, 'rouge1_recall': 0.017957755256252143, 'rouge1_f1': 0.026695307258902394, 'rougeL_precision': 0.09364899523802503, 'rougeL_recall': 0.0170526299145652, 'rougeL_f1': 0.02537773463237484}\n",
      "Epoch 3/3 --- Step 300/3000 --- total examples 198400:   7%|▋         | 2/30 [04:43<1:06:04, 141.59s/batch , learning_rate=1.64e-5, loss=1.93]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-858ecd35345f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_checkpoint_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/tmp/roberta2robera_pubmed_short/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = trainer.run(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moptimizer_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TF_NEW/tf-transformers/src/tf_transformers/core/trainer_gpu_cpu.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, model_fn, optimizer_fn, train_dataset, train_loss_fn, epochs, steps_per_epoch, model_checkpoint_dir, batch_size, training_loss_names, validation_loss_names, validation_dataset, validation_loss_fn, validation_interval_steps, steps_per_call, enable_xla, callbacks, callbacks_interval_steps, overwrite_checkpoint_dir, max_number_of_models, model_save_interval_steps, repeat_dataset, latest_checkpoint)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         training_history, validation_history, callback_scores = train_and_eval(\n\u001b[0m\u001b[1;32m    472\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TF_NEW/tf-transformers/src/tf_transformers/core/trainer_gpu_cpu.py\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(model, optimizer, strategy, epochs, steps_per_epoch, steps_per_call, train_dataset_iter, train_loss_fn, GLOBAL_BATCH_SIZE, training_loss_dict_metric, validation_dataset_distributed, validation_loss_fn, validation_loss_dict_metric, validation_interval_steps, mixed_precision, callbacks, callbacks_interval_steps, trainer_kwargs, checkpoint_manager, model_checkpoint_dir, model_save_interval_steps)\u001b[0m\n\u001b[1;32m    301\u001b[0m                 )\n\u001b[1;32m    302\u001b[0m                 \u001b[0;31m# Call Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                 \u001b[0mdo_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m                 \u001b[0mtotal_examples_processed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msteps_per_call\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mGLOBAL_BATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_checkpoint_dir = \"/tmp/roberta2robera_pubmed_short/\"\n",
    "history = trainer.run(\n",
    "    model_fn = model_fn,\n",
    "    optimizer_fn = optimizer_fn,\n",
    "    train_dataset = train_dataset,\n",
    "    train_loss_fn = train_loss_fn,\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch = 3000,\n",
    "    model_checkpoint_dir=model_checkpoint_dir,\n",
    "    batch_size=train_batch_size,\n",
    "    training_loss_names=None,\n",
    "    validation_loss_names=None,\n",
    "    validation_dataset=None,\n",
    "    validation_loss_fn=None,\n",
    "    validation_interval_steps=None,\n",
    "    steps_per_call=100,\n",
    "    enable_xla=False,\n",
    "    callbacks=[callback],\n",
    "    callbacks_interval_steps=None,\n",
    "    overwrite_checkpoint_dir=True,\n",
    "    max_number_of_models=10,\n",
    "    model_save_interval_steps=None,\n",
    "    repeat_dataset=False,\n",
    "    latest_checkpoint=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b2513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f62c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ar.load_checkpoint(model_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a57bd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TextDecoder(model=model_ar, \n",
    "                     decoder_start_token_id=tokenizer.cls_token_id, \n",
    "                     input_type_ids=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0967c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "620b9e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:10, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-cf58930460a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mp_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mpredicted_ids_sliced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mpredicted_summaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "predicted_summaries = []\n",
    "for (batch_inputs, batch_labels) in tqdm.tqdm(eval_dataset):\n",
    "    del batch_inputs['decoder_input_ids']\n",
    "    decoder_outputs = decoder.decode(batch_inputs, mode='greedy', max_iterations=64, eos_id=tokenizer.sep_token_id)\n",
    "    predicted_ids = [item[0] for item in decoder_outputs['predicted_ids'].numpy().tolist()]\n",
    "    \n",
    "    predicted_ids_sliced = []\n",
    "    for p_id in predicted_ids:\n",
    "        if tokenizer.sep_token_id in p_id:\n",
    "            index = p_id.index(self.eos_id)\n",
    "            p_id = p_id[:index]\n",
    "        predicted_ids_sliced.append(p_id)\n",
    "        predicted_summaries.append(self.tokenizer.decode(p_id))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce3f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34b77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa57f65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c5490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce1f8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "with open(\"{}/history.json\".format(model_checkpoint_dir), \"w\") as f:\n",
    "    json.dump(str(history),f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "732aeeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(model_checkpoint_dir)\n",
    "shutil.rmtree(tfrecord_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87c1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "20daa49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.encoder_decoder.EncoderDecoder object at 0x7fdb97a1bca0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7fdb803d8a60>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.encoder_decoder.EncoderDecoder object at 0x7fdb97a1bca0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7fdb803d8a60>).\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/roberta2robera_pubmed_short/ckpt-3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d982b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
