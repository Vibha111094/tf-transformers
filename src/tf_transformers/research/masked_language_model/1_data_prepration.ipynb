{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "812dfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/sidhu/Projects/tf-transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440a61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "from tf_transformers.text.sentencepiece_layer import extend_sentencepicemodel\n",
    "from tf_transformers.text import SentencepieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f90ffb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee10be28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae8d5955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init inputs ()\n",
      "init kwargs {'model_max_length': 512, 'vocab_file': '/home/sidhu/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d', 'special_tokens_map_file': None, 'tokenizer_file': '/home/sidhu/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529', 'name_or_path': 't5-base'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tmp/tmpnr05xtc6/t5-base/tokenizer_config.json',\n",
       " '/tmp/tmpnr05xtc6/t5-base/special_tokens_map.json',\n",
       " '/tmp/tmpnr05xtc6/t5-base/spiece.model',\n",
       " '/tmp/tmpnr05xtc6/t5-base/added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use T5 tokenizer , but we extend it with 2 more tokens\n",
    "\n",
    "# 1. [CLS]\n",
    "# 2. [MASK]\n",
    "# We will use </s> as SEP token\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "model_name = 't5-base'\n",
    "tokenizer_hf = T5Tokenizer.from_pretrained(model_name)\n",
    "save_path = \"{}/{}\".format(temp_dir, model_name)\n",
    "tokenizer_hf.save_pretrained(save_path)\n",
    "\n",
    "in_file  = '{}/spiece.model'.format(save_path)\n",
    "out_file = '{}/new_spiece.model'.format(save_path)\n",
    "special_tokens = ['[CLS]', '[MASK]']\n",
    "\n",
    "extend_sentencepicemodel(in_file, out_file, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1057408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f725932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use new sentencepiece model in T5 use like this\n",
    "t5_kwargs = {'bos_token': '[CLS]',\n",
    " 'eos_token': '</s>', \n",
    " 'unk_token': '<unk>', \n",
    " 'pad_token': '<pad>', \n",
    " 'mask_token': '[MASK]', \n",
    " 'name_or_path': '{}'.format(save_path), \n",
    " 'vocab_file': '{}/new_spiece.model'.format(save_path)}\n",
    "tokenizer_hf = T5Tokenizer(**t5_kwargs)\n",
    "tokenizer_hf.unique_no_split_tokens = tokenizer_hf.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b67c6e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total vocab is tf.Tensor(32002, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use this one for fast processinf\n",
    "tokenizer_tf = SentencepieceTokenizer(\n",
    "        model_file_path=out_file,\n",
    "        lower_case=False,\n",
    "        special_tokens=['[CLS]', '[MASK]', '</s>', '<unk>', '<pad>']\n",
    "    )\n",
    "print(\"total vocab is\", tokenizer_tf.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c374cd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b4b305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia\n",
    "\n",
    "from tf_transformers.data.utils import hf_dump_chars_to_textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7df9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('venv_tf2.4': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd0e43aa85e0d007ca602e04b3033c86d6af4f225b69c2da0d32fa6602213775d26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
