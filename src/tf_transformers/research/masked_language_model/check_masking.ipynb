{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa06794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/sidhu/Projects/tf-transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b37c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32a2ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_transformers.layers.mask import SelfAttentionMask\n",
    "from tf_transformers.data.processors.mlm import dynamic_masking_from_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9c1563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce3d1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Total individual observations/examples written is 1000 in 1.1995949745178223 seconds\n",
      "INFO:absl:All writer objects closed\n"
     ]
    }
   ],
   "source": [
    "# Create dummy tfrecord\n",
    "\n",
    "def parse_train():\n",
    "    for i in range(1000):\n",
    "        random_length = tf.random.uniform(minval=10, maxval=128, shape=(1,), dtype=tf.int32)[0]\n",
    "        vector = tf.random.uniform(minval=0, maxval=10000,shape=(random_length,), dtype=tf.int32)\n",
    "        vector = vector.numpy().tolist()\n",
    "        yield {\"input_ids\": vector}\n",
    "    \n",
    "    \n",
    "schema = {\n",
    "    \"input_ids\": (\"var_len\", \"int\"),\n",
    "}\n",
    "\n",
    "tfrecord_train_dir = 'tfrecord_dummy'\n",
    "tfrecord_filename = 'dummy'\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    n_files=1,\n",
    "                    overwrite=True\n",
    "                    )\n",
    "# Process\n",
    "tfwriter.process(parse_fn=parse_train())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5277ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae43315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tfrecord\n",
    "import glob\n",
    "def get_tfdataset_from_tfrecords(tfrecord_path_list):\n",
    "    \"\"\"Get tf dataset from tfrecords\"\"\"\n",
    "    all_files = []\n",
    "    for tfrecord_path in tfrecord_path_list:\n",
    "        all_files.extend(glob.glob(\"{}/*.tfrecord\".format(tfrecord_path)))\n",
    "    schema    = json.load(open(\"{}/schema.json\".format(tfrecord_path)))\n",
    "    tf_reader = TFReader(schema=schema, \n",
    "                        tfrecord_files=all_files)\n",
    "    train_dataset = tf_reader.read_record(\n",
    "                                      )\n",
    "    return train_dataset\n",
    "\n",
    "dataset = get_tfdataset_from_tfrecords([tfrecord_train_dir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839ee2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b2e9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(12,), dtype=int32, numpy=\n",
      "array([1012, 3744, 2457, 4470, 6325, 1196, 8150, 7410, 4806, 6899, 3046,\n",
      "       2511], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "da937643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.layers.mask import prefix_mask\n",
    "from tf_transformers.data.utils import auto_batch\n",
    "from tf_transformers.utils import tf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "6a0b4ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_prefix_lm_from_features(max_seq_len, \n",
    "                                    cls_id, sep_id):\n",
    "    \n",
    "    def dynamic_map_prefix(item):\n",
    "        input_ids = item['input_ids']\n",
    "        input_ids = input_ids[:max_seq_len-1] # we need -2 for cls and sep, but in causal LM we shift one pos\n",
    "                                              # so we use -1, length input_ids = max_seq_len + 1\n",
    "        # Add cls sep\n",
    "        input_ids = tf.concat([[cls_id], input_ids, [sep_id]], axis=0)\n",
    "        labels    = input_ids[1:] # exclude first word till last\n",
    "        input_ids = input_ids[:-1] # exclude last word\n",
    "        \n",
    "        input_seq_length = tf.shape(input_ids)[0]\n",
    "        sentence_length = tf.random.uniform(minval=1, maxval=input_seq_length, shape=(1,), dtype=tf.int32)[0]\n",
    "        remaining_length = input_seq_length - sentence_length\n",
    "        input_mask = tf.concat([tf.ones(shape=(sentence_length,), dtype=tf.int32),\n",
    "                   tf.zeros(shape=(remaining_length,), dtype=tf.int32)], axis=0)\n",
    "        # Opposite to input_mask\n",
    "        labels_mask = tf.concat([tf.zeros(shape=(sentence_length,), dtype=tf.int32),\n",
    "                   tf.ones(shape=(remaining_length,), dtype=tf.int32)], axis=0)\n",
    "        \n",
    "        # input type ids\n",
    "        input_type_ids = tf.zeros_like(input_ids)\n",
    "        mask = prefix_mask(input_mask)\n",
    "        inputs = {'input_ids': input_ids,\n",
    "                  'input_type_ids': input_type_ids, \n",
    "                  '3d_mask': mask, \n",
    "                  'input_mask': input_mask,\n",
    "                  'masked_lm_positions': tf.range(tf.shape(input_ids)[0])\n",
    "                 }\n",
    "        \n",
    "        outputs = {\n",
    "                  'masked_lm_labels': labels,\n",
    "                  'masked_lm_weights': labels_mask}\n",
    "        return inputs, outputs\n",
    "    return dynamic_map_prefix\n",
    "\n",
    "\n",
    "def dynamic_causal_lm_from_features(max_seq_len, \n",
    "                                    cls_id, sep_id):\n",
    "    \n",
    "    def attention_mask_square(nd):\n",
    "        \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
    "\n",
    "        Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
    "        \"\"\"\n",
    "        dtype = tf_utils.get_dtype()\n",
    "        ns = nd\n",
    "        i = tf.range(nd)[:, None]\n",
    "        j = tf.range(ns)\n",
    "        m = i >= j - ns + nd\n",
    "        return tf.cast(m, dtype)\n",
    "\n",
    "    def mask_causal_mask(input_ids):\n",
    "        input_ids = tf.expand_dims(input_ids, 0)\n",
    "        from_shape = tf_utils.get_shape_list(input_ids, expected_rank=[2, 3])\n",
    "        batch_size = from_shape[0]\n",
    "        from_seq_length = from_shape[1]\n",
    "\n",
    "        # 2D Lower Triangular Mask\n",
    "        from_mask = attention_mask_square(from_seq_length)\n",
    "\n",
    "        # Replicate 2D `N` times\n",
    "        mask = tf.cast(tf.ones([batch_size, 1, 1]), from_mask.dtype) * from_mask\n",
    "\n",
    "        return tf.cast(tf.squeeze(mask, axis=0), tf.float32)\n",
    "    \n",
    "    def dynamic_map_causal(item):\n",
    "        input_ids = item['input_ids']\n",
    "        input_ids = input_ids[:max_seq_len-1] # we need -2 for cls and sep, but in causal LM we shift one pos\n",
    "                                              # so we use -1, length input_ids = max_seq_len + 1\n",
    "        # Add cls sep\n",
    "        input_ids = tf.concat([[cls_id], input_ids, [sep_id]], axis=0)\n",
    "        labels    = input_ids[1:] # exclude first word till last\n",
    "        input_ids = input_ids[:-1] # exclude last word\n",
    "        labels_mask = tf.ones_like(input_ids)\n",
    "        input_mask = labels_mask\n",
    "        # input type ids\n",
    "        input_type_ids = tf.zeros_like(input_ids)\n",
    "        mask = mask_causal_mask(input_ids)\n",
    "        \n",
    "        inputs = {'input_ids': input_ids,\n",
    "                  'input_type_ids': input_type_ids, \n",
    "                  '3d_mask': mask, \n",
    "                  'input_mask': input_mask,\n",
    "                  'masked_lm_positions': tf.range(tf.shape(input_ids)[0])\n",
    "                 }\n",
    "        \n",
    "        outputs = {\n",
    "                  'masked_lm_labels': labels,\n",
    "                  'masked_lm_weights': labels_mask}\n",
    "        \n",
    "        return inputs, outputs\n",
    "    return dynamic_map_causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa950829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "f1839fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def attention_mask_square(nd):\n",
    "        \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
    "\n",
    "        Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
    "        \"\"\"\n",
    "        dtype = tf_utils.get_dtype()\n",
    "        ns = nd\n",
    "        i = tf.range(nd)[:, None]\n",
    "        j = tf.range(ns)\n",
    "        m = i >= j - ns + nd\n",
    "        return tf.cast(m, dtype)\n",
    "\n",
    "    def mask_causal_mask(input_ids):\n",
    "        input_ids = tf.expand_dims(input_ids, 0)\n",
    "        from_shape = tf_utils.get_shape_list(input_ids, expected_rank=[2, 3])\n",
    "        batch_size = from_shape[0]\n",
    "        from_seq_length = from_shape[1]\n",
    "\n",
    "        # 2D Lower Triangular Mask\n",
    "        from_mask = attention_mask_square(from_seq_length)\n",
    "\n",
    "        # Replicate 2D `N` times\n",
    "        mask = tf.cast(tf.ones([batch_size, 1, 1]), from_mask.dtype) * from_mask\n",
    "\n",
    "        return tf.cast(mask, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "7428aab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10, 10), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_causal_mask(tf.range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "5c0fffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "    tfrecord_path_list,\n",
    "    max_seq_len,\n",
    "    max_predictions_per_batch,\n",
    "    vocab_size,\n",
    "    cls_token_id,\n",
    "    sep_token_id,\n",
    "    unk_token_id,\n",
    "    pad_token_id,\n",
    "    mask_token_id,\n",
    "    batch_size,\n",
    "    min_sen_len,\n",
    "):\n",
    "    \"\"\"Get dataset after mlm from TFRecords\"\"\"\n",
    "\n",
    "    def filter_by_length(x, min_sen_len):\n",
    "        \"\"\"Filter by minimum sentence length (subwords)\"\"\"\n",
    "        return tf.squeeze(tf.greater_equal(tf.shape(x['input_ids']), tf.constant(min_sen_len)), axis=0)\n",
    "\n",
    "    def filter_by_batch(x, y, batch_size):\n",
    "        \"\"\"Filter by batch size\"\"\"\n",
    "        x_batch = tf.shape(x['input_ids'])[0]\n",
    "        return tf.equal(x_batch, tf.constant(batch_size))\n",
    "    \n",
    "    def prepare_3d_input_mask_mlm(input_mask):\n",
    "        \"\"\"Prepare 3D mask from 2D\"\"\"\n",
    "        batch_size = tf.shape(input_mask)[0]\n",
    "        seq_length = tf.shape(input_mask)[1]\n",
    "\n",
    "        to_mask = tf.cast(tf.reshape(input_mask, [batch_size, 1, seq_length]), dtype=input_mask.dtype)\n",
    "        broadcast_ones = tf.ones(shape=[batch_size, seq_length, 1], dtype=input_mask.dtype)\n",
    "\n",
    "        mask = broadcast_ones * to_mask\n",
    "\n",
    "        return tf.cast(mask, tf.float32)\n",
    "    \n",
    "    # Dynamic MLM\n",
    "    dynamic_mlm_fn = dynamic_masking_from_features(\n",
    "        max_seq_len,\n",
    "        max_predictions_per_batch,\n",
    "        vocab_size,\n",
    "        cls_token_id,\n",
    "        sep_token_id,\n",
    "        unk_token_id,\n",
    "        pad_token_id,\n",
    "        mask_token_id,\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Dynamic Prefix LM\n",
    "    dynamic_prefix_lm = dynamic_prefix_lm_from_features(max_seq_len, \n",
    "                                    cls_token_id, sep_token_id)\n",
    "    \n",
    "    # Dynamic Causal LM\n",
    "    dynamic_causal_lm = dynamic_causal_lm_from_features(max_seq_len, \n",
    "                                    cls_token_id, sep_token_id)\n",
    "    \n",
    "    train_dataset = get_tfdataset_from_tfrecords(tfrecord_path_list)\n",
    "\n",
    "    if min_sen_len and min_sen_len > 0:\n",
    "        train_dataset = train_dataset.filter(lambda x: filter_by_length(x, min_sen_len))\n",
    "    \n",
    "    # prob check has to be inside map\n",
    "    # otherwise things become deterministic\n",
    "    def get_dataset_based_on_prob(item):\n",
    "        \"\"\"Map function\"\"\"\n",
    "        \n",
    "        def add_mark(x, mode, prob):\n",
    "            \"\"\"Check are we getting all if conditions with equal probability\"\"\"\n",
    "            x['mode'] = mode\n",
    "            x['prob'] = prob\n",
    "            return x\n",
    "        \n",
    "        def map_mlm(x):\n",
    "            \"\"\"MLM\"\"\"\n",
    "            x['input_ids'] = tf.RaggedTensor.from_tensor(tf.expand_dims(x['input_ids'], axis=0))\n",
    "            x_copy , y_copy = dynamic_mlm_fn(x)\n",
    "            x = {}\n",
    "            for name, v_tensor in x_copy.items():\n",
    "                x[name] = tf.squeeze(v_tensor, axis=0)\n",
    "            y = {}\n",
    "            for name, v_tensor in y_copy.items():\n",
    "                y[name] = tf.squeeze(v_tensor, axis=0)\n",
    "            x['3d_mask']   = tf.squeeze(prepare_3d_input_mask_mlm(x_copy['input_mask']), axis=0)\n",
    "            \n",
    "            for name, v_tensor in y.items():\n",
    "                x[name] = v_tensor\n",
    "            return x\n",
    "        \n",
    "        def map_pcmlm(x):\n",
    "            \"\"\"Prefix Causal LM\"\"\"\n",
    "            x, y = dynamic_prefix_lm(x)\n",
    "            for name, v_tensor in y.items():\n",
    "                x[name] = v_tensor\n",
    "            return x\n",
    "        \n",
    "        def map_cmlm(x):\n",
    "            \"\"\"Causal LM\"\"\"\n",
    "            x, y = dynamic_causal_lm(x)\n",
    "            for name, v_tensor in y.items():\n",
    "                x[name] = v_tensor\n",
    "            return x\n",
    "    \n",
    "\n",
    "        prob = tf.random.uniform(shape=())\n",
    "        # Keep a copy like this importatnt\n",
    "        # otherwise transformation in first if cond might affect other\n",
    "        input_ids = item['input_ids']\n",
    "        \n",
    "        # Do MLM\n",
    "        if prob <= 0.33:\n",
    "            x = map_mlm(item)\n",
    "            x['masked_lm_positions'] = tf.cast(x['masked_lm_positions'], dtype=tf.int32)\n",
    "            x['masked_lm_weights']   = tf.cast(x['masked_lm_weights'], dtype=tf.int32)\n",
    "            del x['input_mask']\n",
    "            x = add_mark(x, \"mlm\", prob)\n",
    "            \n",
    "        # Prefix CLM\n",
    "        elif prob < 0.66:\n",
    "            x = map_pcmlm({\"input_ids\": input_ids})\n",
    "            del x['input_mask']\n",
    "            x = add_mark(x, \"prefix\", prob)\n",
    "            \n",
    "        else:\n",
    "            x = map_cmlm({\"input_ids\": input_ids})\n",
    "            del x['input_mask']\n",
    "            x = add_mark(x, \"causal\", prob)\n",
    "        return x\n",
    "    \n",
    "    train_dataset = train_dataset.map(get_dataset_based_on_prob, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_dataset = auto_batch(train_dataset, \n",
    "                              batch_size, \n",
    "                              x_keys=['input_ids', 'input_type_ids', '3d_mask', 'masked_lm_positions'],\n",
    "                              y_keys=['masked_lm_labels', 'masked_lm_weights', 'mode', 'prob'], \n",
    "                              shuffle=True\n",
    "                              )\n",
    "    train_dataset = train_dataset.filter(lambda x, y: filter_by_batch(x, y, batch_size))\n",
    "    train_dataset = train_dataset.shuffle(100)\n",
    "    train_dataset = train_dataset.prefetch(100)\n",
    "\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b4394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "e9317cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 128\n",
    "max_predictions_per_batch = 20\n",
    "vocab_size = 30200\n",
    "cls_token_id = 2\n",
    "sep_token_id = 3\n",
    "unk_token_id = 1\n",
    "pad_token_id = 0\n",
    "mask_token_id = 5\n",
    "batch_size = 5\n",
    "min_sen_len = None\n",
    "train_dataset = get_dataset(\n",
    "    [tfrecord_train_dir],\n",
    "    max_seq_len,\n",
    "    max_predictions_per_batch,\n",
    "    vocab_size,\n",
    "    cls_token_id,\n",
    "    sep_token_id,\n",
    "    unk_token_id,\n",
    "    pad_token_id,\n",
    "    mask_token_id,\n",
    "    batch_size,\n",
    "    min_sen_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "77d8d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_modes = []\n",
    "all_probs = []\n",
    "for (batch_inputs, batch_labels) in train_dataset:\n",
    "    all_modes.extend(batch_labels['mode'].numpy())\n",
    "    all_probs.extend(batch_labels['prob'].numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "d363ce4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({b'causal': 336, b'mlm': 337, b'prefix': 327})"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(all_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca0cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "1dd33328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.models import BertEncoder\n",
    "from tf_transformers.core import LegacyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f445a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "8b61f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixEncoder(BertEncoder):\n",
    "    \n",
    "    def __init__(self, config, **kwargs):\n",
    "        print(kwargs)\n",
    "        super(MixEncoder, self).__init__(config, **kwargs)\n",
    "        \n",
    "    def get_model(self, initialize_only=False):\n",
    "        \"\"\"Convert tf.keras.Layer to a tf.keras.Model/LegacyModel.\n",
    "        Args:\n",
    "            self: model (tf.keras.Layer) instance\n",
    "        \"\"\"\n",
    "\n",
    "        input_ids = tf.keras.layers.Input(\n",
    "            shape=(self._sequence_length,),\n",
    "            batch_size=self._batch_size,\n",
    "            dtype=tf.int32,\n",
    "            name=\"input_ids\",\n",
    "        )\n",
    "        input_mask = tf.keras.layers.Input(\n",
    "            shape=(self._sequence_length,self._sequence_length),\n",
    "            batch_size=self._batch_size,\n",
    "            dtype=tf.float32,\n",
    "            name=\"input_mask\",\n",
    "        )\n",
    "        input_type_ids = tf.keras.layers.Input(\n",
    "            shape=(self._sequence_length,),\n",
    "            batch_size=self._batch_size,\n",
    "            dtype=tf.int32,\n",
    "            name=\"input_type_ids\",\n",
    "        )\n",
    "        masked_lm_positions = tf.keras.layers.Input(\n",
    "            shape=(None,),\n",
    "            batch_size=self._batch_size,\n",
    "            dtype=tf.int32,\n",
    "            name=\"masked_lm_positions\",\n",
    "        )\n",
    "        inputs = {}\n",
    "        inputs[\"input_ids\"] = input_ids  # Default\n",
    "        # if mask_mode != 'causal', user has to provde mask\n",
    "        if self._mask_mode != \"causal\":\n",
    "            inputs[\"input_mask\"] = input_mask\n",
    "        # If type mebddings required\n",
    "        if self._type_embeddings_layer:\n",
    "            inputs[\"input_type_ids\"] = input_type_ids\n",
    "        # if masked_lm_positions\n",
    "        if self._use_masked_lm_positions:\n",
    "            inputs[\"masked_lm_positions\"] = masked_lm_positions\n",
    "\n",
    "\n",
    "        layer_outputs = self(inputs)\n",
    "        if initialize_only:\n",
    "            return inputs, layer_outputs\n",
    "\n",
    "        # Adding model_config is a hack\n",
    "        model = LegacyModel(inputs=inputs, outputs=layer_outputs, name=self._model_name)\n",
    "        model.model_config = self._config_dict\n",
    "        return model\n",
    "\n",
    "    def call_encoder(self, inputs):\n",
    "        \"\"\"Forward pass of an Encoder\n",
    "\n",
    "        Args:\n",
    "            inputs ([dict of tf.Tensor]): This is the input to the model.\n",
    "\n",
    "            'input_ids'         --> tf.int32 (b x s)\n",
    "            'input_mask'        --> tf.int32 (b x s) # optional\n",
    "            'input_type_ids'    --> tf.int32 (b x s) # optional\n",
    "\n",
    "        Returns:\n",
    "            [dict of tf.Tensor]: Output from the model\n",
    "\n",
    "            'cls_output'        --> tf.float32 (b x s) # optional\n",
    "            'token_embeddings'  --> tf.float32 (b x s x h)\n",
    "            'all_layer_token_embeddings' --> tf.float32 (List of (b x s x h)\n",
    "                                              from all layers)\n",
    "            'all_layer_cls_output'       --> tf.float32 (List of (b x s)\n",
    "                                              from all layers)\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Collect Word Embeddings\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        sequence_length = tf.shape(input_ids)[1]\n",
    "        embeddings = self._embedding_layer(input_ids)\n",
    "        # Add word_embeddings + position_embeddings + type_embeddings\n",
    "        if self._type_embeddings_layer:\n",
    "            input_type_ids = inputs[\"input_type_ids\"]\n",
    "            type_embeddings = self._type_embeddings_layer(input_type_ids)\n",
    "            embeddings = embeddings + type_embeddings\n",
    "        if self._positional_embedding_layer:\n",
    "            positional_embeddings = self._positional_embedding_layer(tf.range(sequence_length))\n",
    "            embeddings = embeddings + positional_embeddings\n",
    "\n",
    "        # 2. Norm + dropout\n",
    "        embeddings = self._embedding_norm(embeddings)\n",
    "        embeddings = self._embedding_dropout(embeddings, training=self._use_dropout)\n",
    "\n",
    "        # 3. Attention  Mask\n",
    "        attention_mask = inputs['input_mask']\n",
    "\n",
    "        # 4. Transformer Outputs\n",
    "        encoder_outputs = []\n",
    "        for i in range(self._config_dict[\"num_hidden_layers\"]):\n",
    "            layer = self._transformer_layers[i]\n",
    "            embeddings, _, _ = layer([embeddings, attention_mask])\n",
    "            encoder_outputs.append(embeddings)\n",
    "\n",
    "        # First word of last layer outputs [CLS]\n",
    "        cls_token_tensor = tf.keras.layers.Lambda(lambda x: tf.squeeze(x[:, 0:1, :], axis=1))(encoder_outputs[-1])\n",
    "        # batch_size x embedding_size\n",
    "        cls_output = self._pooler_layer(cls_token_tensor)\n",
    "        # batch_size x sequence_length x embedding_size\n",
    "        token_embeddings = encoder_outputs[-1]\n",
    "\n",
    "        # check for masked lm positions\n",
    "        # only for encoder forward pass. This is for MaskedLM training\n",
    "        if \"masked_lm_positions\" in inputs:\n",
    "            masked_lm_positions = inputs[\"masked_lm_positions\"]\n",
    "        else:\n",
    "            masked_lm_positions = None\n",
    "\n",
    "        # MaskedLM layer only project it and normalize (b x s x h)\n",
    "        token_embeddings_mlm = self._masked_lm_layer(token_embeddings, masked_lm_positions)\n",
    "        token_logits = tf.matmul(\n",
    "            token_embeddings_mlm, tf.cast(self.get_embedding_table(), dtype=tf_utils.get_dtype()), transpose_b=True\n",
    "        )\n",
    "        # token_logits         =  tf.nn.bias_add(token_logits, self._masked_lm_bias)\n",
    "        token_logits = self._masked_lm_bias(token_logits)\n",
    "        last_token_logits = tf.keras.layers.Lambda(lambda x: x[:, -1, :])(token_logits)\n",
    "\n",
    "        result = {\n",
    "            \"cls_output\": cls_output,\n",
    "            \"token_embeddings\": token_embeddings,\n",
    "            \"token_logits\": token_logits,\n",
    "            \"last_token_logits\": last_token_logits,\n",
    "        }\n",
    "\n",
    "        if self._return_all_layer_outputs:\n",
    "            all_cls_output = []\n",
    "            all_token_logits = []\n",
    "            for per_layer_token_embeddings in encoder_outputs:\n",
    "                per_cls_token_tensor = tf.keras.layers.Lambda(lambda x: tf.squeeze(x[:, 0:1, :], axis=1))(\n",
    "                    per_layer_token_embeddings\n",
    "                )\n",
    "                all_cls_output.append(self._pooler_layer(per_cls_token_tensor))\n",
    "\n",
    "                # token logits per layer\n",
    "                layer_token_embeddings_mlm = self._masked_lm_layer(per_layer_token_embeddings, masked_lm_positions)\n",
    "                layer_token_logits = tf.matmul(\n",
    "                    layer_token_embeddings_mlm,\n",
    "                    tf.cast(self.get_embedding_table(), dtype=tf_utils.get_dtype()),\n",
    "                    transpose_b=True,\n",
    "                )\n",
    "                layer_token_logits = self._masked_lm_bias(layer_token_logits)\n",
    "                all_token_logits.append(layer_token_logits)\n",
    "\n",
    "            result[\"all_layer_token_embeddings\"] = encoder_outputs\n",
    "            result[\"all_layer_cls_output\"] = all_cls_output\n",
    "            result[\"all_layer_token_logits\"] = all_token_logits\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc2b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "984a4dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_training': True, 'use_dropout': True, 'use_masked_lm_positions': True, 'return_all_layer_outputs': True}\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"intermediate_act\": \"gelu\",\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"embedding_size\": 768,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 3072,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"attention_head_size\": 64,\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"type_vocab_size\": 2,\n",
    "    \"vocab_size\": 30000,\n",
    "    \"layer_norm_epsilon\": 1e-12\n",
    "}\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = MixEncoder(config,\n",
    "                         is_training=True,\n",
    "                         use_dropout=True,\n",
    "                         use_masked_lm_positions=True,\n",
    "                         return_all_layer_outputs=True)\n",
    "model = model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "f037be4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <KerasTensor: shape=(None, None) dtype=int32 (created by layer 'input_ids')>,\n",
       " 'input_mask': <KerasTensor: shape=(None, None, None) dtype=float32 (created by layer 'input_mask')>,\n",
       " 'input_type_ids': <KerasTensor: shape=(None, None) dtype=int32 (created by layer 'input_type_ids')>,\n",
       " 'masked_lm_positions': <KerasTensor: shape=(None, None) dtype=int32 (created by layer 'masked_lm_positions')>}"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "439057e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cls_output': <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       " 'token_embeddings': <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       " 'token_logits': <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       " 'last_token_logits': <KerasTensor: shape=(None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       " 'all_layer_token_embeddings': [<KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>],\n",
       " 'all_layer_cls_output': [<KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>],\n",
       " 'all_layer_token_logits': [<KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>]}"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbcbb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('venv_tf2.4': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd0e43aa85e0d007ca602e04b3033c86d6af4f225b69c2da0d32fa6602213775d26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
