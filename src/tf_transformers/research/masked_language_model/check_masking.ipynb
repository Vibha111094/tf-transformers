{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa06794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/sidhu/Projects/tf-transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b37c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32a2ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_transformers.data.processors.mlm import (\n",
    "    dynamic_causal_lm_from_features,\n",
    "    dynamic_masking_from_features,\n",
    "    dynamic_prefix_lm_from_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c9c1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.data.utils import auto_batch\n",
    "from tf_transformers.utils import tf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce3d1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Total individual observations/examples written is 1000 in 2.317816972732544 seconds\n",
      "INFO:absl:All writer objects closed\n"
     ]
    }
   ],
   "source": [
    "# Create dummy tfrecord\n",
    "\n",
    "def parse_train():\n",
    "    for i in range(1000):\n",
    "        random_length = tf.random.uniform(minval=10, maxval=128, shape=(1,), dtype=tf.int32)[0]\n",
    "        vector = tf.random.uniform(minval=0, maxval=10000,shape=(random_length,), dtype=tf.int32)\n",
    "        vector = vector.numpy().tolist()\n",
    "        yield {\"input_ids\": vector}\n",
    "    \n",
    "    \n",
    "schema = {\n",
    "    \"input_ids\": (\"var_len\", \"int\"),\n",
    "}\n",
    "\n",
    "tfrecord_train_dir = 'tfrecord_dummy'\n",
    "tfrecord_filename = 'dummy'\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    n_files=1,\n",
    "                    overwrite=True\n",
    "                    )\n",
    "# Process\n",
    "tfwriter.process(parse_fn=parse_train())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5277ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae43315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tfrecord\n",
    "import glob\n",
    "def get_tfdataset_from_tfrecords(tfrecord_path_list):\n",
    "    \"\"\"Get tf dataset from tfrecords\"\"\"\n",
    "    all_files = []\n",
    "    for tfrecord_path in tfrecord_path_list:\n",
    "        all_files.extend(glob.glob(\"{}/*.tfrecord\".format(tfrecord_path)))\n",
    "    schema    = json.load(open(\"{}/schema.json\".format(tfrecord_path)))\n",
    "    tf_reader = TFReader(schema=schema, \n",
    "                        tfrecord_files=all_files)\n",
    "    train_dataset = tf_reader.read_record(\n",
    "                                      )\n",
    "    return train_dataset\n",
    "\n",
    "dataset = get_tfdataset_from_tfrecords([tfrecord_train_dir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839ee2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5b2e9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(93,), dtype=int32, numpy=\n",
      "array([5952, 3643, 7967, 6195, 3369, 2210, 9291, 2911, 5740, 3711, 6952,\n",
      "       1476, 2407, 5838, 6559, 2477, 4107, 2194,  120, 5211, 5620,  667,\n",
      "       1644, 9636, 1329, 3799, 5731, 7563, 7050, 1720, 6437, 6717, 1020,\n",
      "       2492, 9300,  948, 6357, 2033, 8648, 2489, 6808, 2274, 6029, 4729,\n",
      "       5366, 4358, 2885, 9429, 4410, 7672, 8602, 5081, 2272, 1845, 6607,\n",
      "       6031, 5429, 8123, 5859, 1106, 7327, 3914, 5751, 1728, 2399,   42,\n",
      "       4557, 6764, 1194, 4671, 1107, 2211, 5078, 6014, 8458,  408, 5195,\n",
      "       8810, 7499, 8514, 6822, 6580, 5549, 4560, 8958, 3485, 9272, 6463,\n",
      "        769, 5415, 4016, 4280,   57], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "da937643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cec28bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "    tfrecord_path_list,\n",
    "    max_seq_len,\n",
    "    max_predictions_per_batch,\n",
    "    vocab_size,\n",
    "    cls_token_id,\n",
    "    sep_token_id,\n",
    "    unk_token_id,\n",
    "    pad_token_id,\n",
    "    mask_token_id,\n",
    "    batch_size,\n",
    "    min_sen_len,\n",
    "):\n",
    "    \"\"\"Get dataset after mlm from TFRecords\"\"\"\n",
    "\n",
    "    def filter_by_length(x, min_sen_len):\n",
    "        \"\"\"Filter by minimum sentence length (subwords)\"\"\"\n",
    "        return tf.squeeze(tf.greater_equal(tf.shape(x['input_ids']), tf.constant(min_sen_len)), axis=0)\n",
    "\n",
    "    def filter_by_batch(x, y, batch_size):\n",
    "        \"\"\"Filter by batch size\"\"\"\n",
    "        x_batch = tf.shape(x['input_ids'])[0]\n",
    "        return tf.equal(x_batch, tf.constant(batch_size))\n",
    "    \n",
    "    def prepare_3d_input_mask_mlm(input_mask):\n",
    "        \"\"\"Prepare 3D mask from 2D\"\"\"\n",
    "        batch_size = tf.shape(input_mask)[0]\n",
    "        seq_length = tf.shape(input_mask)[1]\n",
    "\n",
    "        to_mask = tf.cast(tf.reshape(input_mask, [batch_size, 1, seq_length]), dtype=input_mask.dtype)\n",
    "        broadcast_ones = tf.ones(shape=[batch_size, seq_length, 1], dtype=input_mask.dtype)\n",
    "\n",
    "        mask = broadcast_ones * to_mask\n",
    "\n",
    "        return tf.cast(mask, tf.float32)\n",
    "    \n",
    "    # Dynamic MLM\n",
    "    dynamic_mlm_fn = dynamic_masking_from_features(\n",
    "        max_seq_len,\n",
    "        max_predictions_per_batch,\n",
    "        vocab_size,\n",
    "        cls_token_id,\n",
    "        sep_token_id,\n",
    "        unk_token_id,\n",
    "        pad_token_id,\n",
    "        mask_token_id,\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Dynamic Prefix LM\n",
    "    dynamic_prefix_lm = dynamic_prefix_lm_from_features(max_seq_len, \n",
    "                                    cls_token_id, sep_token_id)\n",
    "    \n",
    "    # Dynamic Causal LM\n",
    "    dynamic_causal_lm = dynamic_causal_lm_from_features(max_seq_len, \n",
    "                                    cls_token_id, sep_token_id)\n",
    "    \n",
    "    train_dataset = get_tfdataset_from_tfrecords(tfrecord_path_list)\n",
    "\n",
    "    if min_sen_len and min_sen_len > 0:\n",
    "        train_dataset = train_dataset.filter(lambda x: filter_by_length(x, min_sen_len))\n",
    "    \n",
    "    # prob check has to be inside map\n",
    "    # otherwise things become deterministic\n",
    "    def get_dataset_based_on_prob(item):\n",
    "        \"\"\"Map function\"\"\"\n",
    "        \n",
    "        def add_mark(x, mode, prob):\n",
    "            \"\"\"Check are we getting all if conditions with equal probability\"\"\"\n",
    "            x['mode'] = [mode]\n",
    "            x['prob'] = [prob]\n",
    "            return x\n",
    "        \n",
    "        def map_mlm(x):\n",
    "            \"\"\"MLM\"\"\"\n",
    "            x['input_ids'] = tf.RaggedTensor.from_tensor(tf.expand_dims(x['input_ids'], axis=0))\n",
    "            x_copy , y_copy = dynamic_mlm_fn(x)\n",
    "            x = {}\n",
    "            for name, v_tensor in x_copy.items():\n",
    "                x[name] = tf.squeeze(v_tensor, axis=0)\n",
    "            y = {}\n",
    "            for name, v_tensor in y_copy.items():\n",
    "                y[name] = tf.squeeze(v_tensor, axis=0)\n",
    "            x['3d_mask']   = tf.squeeze(prepare_3d_input_mask_mlm(x_copy['input_mask']), axis=0)\n",
    "            \n",
    "            for name, v_tensor in y.items():\n",
    "                x[name] = v_tensor\n",
    "            return x\n",
    "        \n",
    "        def map_pcmlm(x):\n",
    "            \"\"\"Prefix Causal LM\"\"\"\n",
    "            x, y = dynamic_prefix_lm(x)\n",
    "            for name, v_tensor in y.items():\n",
    "                x[name] = v_tensor\n",
    "            return x\n",
    "        \n",
    "        def map_cmlm(x):\n",
    "            \"\"\"Causal LM\"\"\"\n",
    "            x, y = dynamic_causal_lm(x)\n",
    "            for name, v_tensor in y.items():\n",
    "                x[name] = v_tensor\n",
    "            return x\n",
    "    \n",
    "\n",
    "        prob = tf.random.uniform(shape=())\n",
    "        # Keep a copy like this importatnt\n",
    "        # otherwise transformation in first if cond might affect other\n",
    "        input_ids = item['input_ids']\n",
    "        \n",
    "        # Do MLM\n",
    "        if prob <= 0.33:\n",
    "            x = map_mlm(item)\n",
    "            x['masked_lm_positions'] = tf.cast(x['masked_lm_positions'], dtype=tf.int32)\n",
    "            x['masked_lm_weights']   = tf.cast(x['masked_lm_weights'], dtype=tf.int32)\n",
    "            x['input_mask'] = x['3d_mask']\n",
    "            del x['3d_mask']\n",
    "            x = add_mark(x, \"mlm\", prob)\n",
    "            \n",
    "        # Prefix CLM\n",
    "        elif prob < 0.66:\n",
    "            x = map_pcmlm({\"input_ids\": input_ids})\n",
    "            x['input_mask'] = x['3d_mask']\n",
    "            del x['3d_mask']            \n",
    "            x = add_mark(x, \"prefix\", prob)\n",
    "            \n",
    "        else:\n",
    "            x = map_cmlm({\"input_ids\": input_ids})\n",
    "            x['input_mask'] = x['3d_mask']\n",
    "            del x['3d_mask']\n",
    "            x = add_mark(x, \"causal\", prob)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    train_dataset = train_dataset.map(get_dataset_based_on_prob, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_dataset = auto_batch(train_dataset, \n",
    "                              batch_size,\n",
    "                                x_keys=['input_ids', 'input_type_ids', 'input_mask', 'masked_lm_positions'],\n",
    "                                y_keys=['masked_lm_labels', 'masked_lm_weights', 'mode', 'prob'],\n",
    "                              shuffle=True\n",
    "                              )\n",
    "    train_dataset = train_dataset.filter(lambda x, y: filter_by_batch(x, y, batch_size))\n",
    "    train_dataset = train_dataset.shuffle(100)\n",
    "    train_dataset = train_dataset.prefetch(100)\n",
    "\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5196a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e9317cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded shapes {'input_ids': [None], 'input_type_ids': [None], 'input_mask': [None, None], 'masked_lm_positions': [None], 'masked_lm_labels': [None], 'masked_lm_weights': [None], 'mode': [None], 'prob': [None]}\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 128\n",
    "max_predictions_per_batch = 20\n",
    "vocab_size = 30200\n",
    "cls_token_id = 2\n",
    "sep_token_id = 3\n",
    "unk_token_id = 1\n",
    "pad_token_id = 0\n",
    "mask_token_id = 5\n",
    "batch_size = 5\n",
    "min_sen_len = None\n",
    "train_dataset = get_dataset(\n",
    "    [tfrecord_train_dir],\n",
    "    max_seq_len,\n",
    "    max_predictions_per_batch,\n",
    "    vocab_size,\n",
    "    cls_token_id,\n",
    "    sep_token_id,\n",
    "    unk_token_id,\n",
    "    pad_token_id,\n",
    "    mask_token_id,\n",
    "    batch_size,\n",
    "    min_sen_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4939745d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a06a69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_modes = []\n",
    "all_probs = []\n",
    "for (batch_inputs, batch_labels) in train_dataset:\n",
    "    all_modes.extend(batch_labels['mode'].numpy())\n",
    "    all_probs.extend(batch_labels['prob'].numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a9845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(all_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8448923a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([b'mlm'], dtype=object),\n",
       " array([b'causal'], dtype=object),\n",
       " array([b'prefix'], dtype=object),\n",
       " array([b'mlm'], dtype=object),\n",
       " array([b'prefix'], dtype=object)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df619209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4fad5380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(5, 128), dtype=int32, numpy=\n",
       " array([[    2,  8725,  8350,  9012,  9594,  4473,  2253,  7767,  6496,\n",
       "          3794,  3973,  6197,  3968,  1056,  4873,    81,  4731,  8508,\n",
       "             5,  6532,  4748,  1706,  6403,  8314,  7474,     5,  9873,\n",
       "          9950,  7376,     5,  1927,  2806,  7745,  1511,   982, 15391,\n",
       "          7896,     5,  7769,     5,  8273,   663,  7205,  6093,  4153,\n",
       "          6862,  3223,  5360, 20256,  5962,  9436,  7603, 17984,     5,\n",
       "          8310,  6297,  2921,  2230,  7995,     5,  8583,   783,  6228,\n",
       "             5,     5,  1044,  8177,  7197,  2025,  2296,  8295,  8599,\n",
       "             5,  4027,  2449,  7628,     3,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [    2,  1785,  4469,  6986,  6425,  6775,  2771,  7522,  3700,\n",
       "           473,  8818,  5121,  4625,  8205,  5937,  4222,  4782,  3911,\n",
       "          9841,  3213,  3251,  9766,  3954,  1370,  1852,  3460,  7083,\n",
       "          6233,  5109,  4558,   477,  5214,  6791,  9635,  9073,  8147,\n",
       "          5865,  5240,  7765,   199,   331,  6081,  3848,    51,  3180,\n",
       "          4590,   307,  5936,  2343,  2979,   658,  2027,  7819,  6698,\n",
       "          1828,  1784,  9979,  8300,  3561,  7748,   427,   423,  8108,\n",
       "          5119,  4037,  5001,  3483,  2987,  2728,  6656,  8737,  6664,\n",
       "          7311,  2188,  1847,  2135,  9700,  1304,  5952,  2698,  1925,\n",
       "          2473,  8649,  3263,  6692,  1646,  2194,  9970,  1098,  2307,\n",
       "          4783,  3514,  6149,  8104,  2328,  8981,  6075,  8384,  7694,\n",
       "          9691,  5183,  9187,  9203,  5235,  7770,  2442,  7613,  8837,\n",
       "          3068,  9040,  1024,  2935,  4627,  9173,  5029,   136,  3381,\n",
       "           471,  6687,  7926,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [    2,  8568,  2267,  7767,  9470,  7254,  4154,  6963,  6925,\n",
       "          9054,  8420,  7870,  1261,   332,  9669,   549,  4995,  9594,\n",
       "          7823,  7043,  4728,  8810,  8877,  3398,  6967,  1641,  1531,\n",
       "          9791,  7971,  5122,  7915,  9976,  9467,  1178,  5945,    49,\n",
       "          3014,  7154,  1129,   704,  7823,  7609,  4568,  8341,  6156,\n",
       "          2620,   257,  8124,  7665,  8807,  5813,  8061,  5653,  2039,\n",
       "          3679,  4245,  2283,  7537,  4041,  7385,  7874,  2331,  1263,\n",
       "          3094,  3114,  6544,  2218,  5296,  9329,  2938,  6206,  9082,\n",
       "          9091,  1594,  4179,  1220,  5045,  3694,  4878,  1803,  6587,\n",
       "          1091,  2027,  2727,  7778,  4699,  6507,  2165,  7739,   208,\n",
       "          2678,  7759,  2605,  4404,  8499,  9373,  6359,  2350,   132,\n",
       "          6263,  1492,  2260,   407,  5643,  7325,  8196,  2175,  3323,\n",
       "          6542,  1976,  5566,  2180,  8355,  7010,  9252,  3240,  9365,\n",
       "           332,  2115,  6390,  6013,  4250,  8162,  4983,    29,     0,\n",
       "             0,     0],\n",
       "        [    2,  4630,  8018,  8729,   125,  7575,  9894,  1440,  5149,\n",
       "          4823,     5,  9406,  8249,     5,  4595,  9741,     5,     5,\n",
       "          1363,    44,  7104,  8568,  5989,  2415,  2681,  5822,  7594,\n",
       "          8657,  7831,  3687,  2937,  4543,     5,  5254,  8890,     5,\n",
       "          6226,     3,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [    2,  5465,   200,  3459,  9697,  7933,  1630,  8564,  9476,\n",
       "           123,  4459,  2399,  5111,  9195,  5136,  5784,  2939,  5736,\n",
       "          3237,  8861,  1046,  7903,    10,  4612,  8967,   510,  6512,\n",
       "          5997,  8994,  9004,   148,  9451,  3977,  6904,    77,  6277,\n",
       "          2043,  6181,  5097,  5004,   277,  1554,  1454,  2419,  1579,\n",
       "          5743,  5733,  6105,   596,  8267,  7383,  1139,  4519,  5596,\n",
       "           520,   820,  8774,  6681,  5720,  1755,  5247,  7170,  7511,\n",
       "          7106,  1081,  6098,   949,  7932,  7399,  8933,  8028,  6685,\n",
       "          6905,  8798,  5138,   445,  4352,  2247,  3704,  5066,   306,\n",
       "          4438,  9311,  9338,  7694,  4180,  7804,  6769,  5319,  7764,\n",
       "          8626,  5217,  3001,  4000,  6364,  5031,  6314,  5260,  9537,\n",
       "          1378,  5352,  1602,  2772,   692,  1144,  6455,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>,\n",
       " 'input_type_ids': <tf.Tensor: shape=(5, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(5, 128, 128), dtype=float32, numpy=\n",
       " array([[[1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 1., 0., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 1., 0., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>,\n",
       " 'masked_lm_positions': <tf.Tensor: shape=(5, 125), dtype=int32, numpy=\n",
       " array([[ 18,  25,  29,  35,  37,  39,  47,  48,  52,  53,  59,  63,  64,\n",
       "          72,  74,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "          13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "          26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "          39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "          52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "          65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "          78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "          91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "         104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "         117, 118, 119,   0,   0,   0,   0,   0],\n",
       "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "          13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "          26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "          39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "          52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "          65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "          78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "          91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "         104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "         117, 118, 119, 120, 121, 122, 123, 124],\n",
       "        [ 10,  13,  16,  17,  23,  28,  32,  35,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "          13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "          26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "          39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "          52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "          65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "          78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "          91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "         104, 105,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0]], dtype=int32)>}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759af6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "045d2f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([120., 119., 118., 117., 116., 115., 114., 113., 112., 111., 110.,\n",
       "       109., 108., 107., 106., 105., 104., 103., 102., 101., 100.,  99.,\n",
       "        98.,  97.,  96.,  95.,  94.,  93.,  92.,  91.,  90.,  89.,  88.,\n",
       "        87.,  86.,  85.,  84.,  83.,  82.,  81.,  80.,  79.,  78.,  77.,\n",
       "        76.,  75.,  74.,  73.,  72.,  71.,  70.,  69.,  68.,  67.,  66.,\n",
       "        65.,  64.,  63.,  62.,  61.,  60.,  59.,  58.,  57.,  56.,  55.,\n",
       "        54.,  53.,  52.,  51.,  50.,  49.,  48.,  47.,  46.,  45.,  44.,\n",
       "        43.,  42.,  41.,  40.,  39.,  38.,  37.,  36.,  35.,  34.,  33.,\n",
       "        32.,  31.,  30.,  29.,  28.,  27.,  26.,  25.,  24.,  23.,  22.,\n",
       "        21.,  20.,  19.,  18.,  17.,  16.,  15.,  14.,  13.,  12.,  11.,\n",
       "        10.,   9.,   8.,   7.,   6.,   5.,   4.,   3.,   2.,   1.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.], dtype=float32)>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(batch_inputs['input_mask'][1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9a8be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "1dd33328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.models import BertEncoder\n",
    "from tf_transformers.core import LegacyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eda804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "91a7d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixEncoder(BertEncoder):\n",
    "    \n",
    "    def __init__(self, config, **kwargs):\n",
    "        print(kwargs)\n",
    "        super(MixEncoder, self).__init__(config, **kwargs)\n",
    "        \n",
    "    def get_model(self, initialize_only=False):\n",
    "        \"\"\"Convert tf.keras.Layer to a tf.keras.Model/LegacyModel.\n",
    "        Args:\n",
    "            self: model (tf.keras.Layer) instance\n",
    "        \"\"\"\n",
    "\n",
    "        input_ids = tf.keras.layers.Input(\n",
    "            shape=(self._sequence_length,),\n",
    "            batch_size=self._batch_size,\n",
    "            dtype=tf.int32,\n",
    "            name=\"input_ids\",\n",
    "        )\n",
    "        input_mask = tf.keras.layers.Input(\n",
    "            shape=(self._sequence_length,self._sequence_length),\n",
    "            batch_size=self._batch_size,\n",
    "            dtype=tf.float32,\n",
    "            name=\"input_mask\",\n",
    "        )\n",
    "        input_type_ids = tf.keras.layers.Input(\n",
    "            shape=(self._sequence_length,),\n",
    "            batch_size=self._batch_size,\n",
    "            dtype=tf.int32,\n",
    "            name=\"input_type_ids\",\n",
    "        )\n",
    "        masked_lm_positions = tf.keras.layers.Input(\n",
    "            shape=(None,),\n",
    "            batch_size=self._batch_size,\n",
    "            dtype=tf.int32,\n",
    "            name=\"masked_lm_positions\",\n",
    "        )\n",
    "        inputs = {}\n",
    "        inputs[\"input_ids\"] = input_ids  # Default\n",
    "        # if mask_mode != 'causal', user has to provde mask\n",
    "        if self._mask_mode != \"causal\":\n",
    "            inputs[\"input_mask\"] = input_mask\n",
    "        # If type mebddings required\n",
    "        if self._type_embeddings_layer:\n",
    "            inputs[\"input_type_ids\"] = input_type_ids\n",
    "        # if masked_lm_positions\n",
    "        if self._use_masked_lm_positions:\n",
    "            inputs[\"masked_lm_positions\"] = masked_lm_positions\n",
    "\n",
    "\n",
    "        layer_outputs = self(inputs)\n",
    "        if initialize_only:\n",
    "            return inputs, layer_outputs\n",
    "\n",
    "        # Adding model_config is a hack\n",
    "        model = LegacyModel(inputs=inputs, outputs=layer_outputs, name=self._model_name)\n",
    "        model.model_config = self._config_dict\n",
    "        return model\n",
    "\n",
    "    def call_encoder(self, inputs):\n",
    "        \"\"\"Forward pass of an Encoder\n",
    "\n",
    "        Args:\n",
    "            inputs ([dict of tf.Tensor]): This is the input to the model.\n",
    "\n",
    "            'input_ids'         --> tf.int32 (b x s)\n",
    "            'input_mask'        --> tf.int32 (b x s) # optional\n",
    "            'input_type_ids'    --> tf.int32 (b x s) # optional\n",
    "\n",
    "        Returns:\n",
    "            [dict of tf.Tensor]: Output from the model\n",
    "\n",
    "            'cls_output'        --> tf.float32 (b x s) # optional\n",
    "            'token_embeddings'  --> tf.float32 (b x s x h)\n",
    "            'all_layer_token_embeddings' --> tf.float32 (List of (b x s x h)\n",
    "                                              from all layers)\n",
    "            'all_layer_cls_output'       --> tf.float32 (List of (b x s)\n",
    "                                              from all layers)\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Collect Word Embeddings\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        sequence_length = tf.shape(input_ids)[1]\n",
    "        embeddings = self._embedding_layer(input_ids)\n",
    "        # Add word_embeddings + position_embeddings + type_embeddings\n",
    "        if self._type_embeddings_layer:\n",
    "            input_type_ids = inputs[\"input_type_ids\"]\n",
    "            type_embeddings = self._type_embeddings_layer(input_type_ids)\n",
    "            embeddings = embeddings + type_embeddings\n",
    "        if self._positional_embedding_layer:\n",
    "            positional_embeddings = self._positional_embedding_layer(tf.range(sequence_length))\n",
    "            embeddings = embeddings + positional_embeddings\n",
    "\n",
    "        # 2. Norm + dropout\n",
    "        embeddings = self._embedding_norm(embeddings)\n",
    "        embeddings = self._embedding_dropout(embeddings, training=self._use_dropout)\n",
    "\n",
    "        # 3. Attention  Mask\n",
    "        attention_mask = inputs['input_mask']\n",
    "\n",
    "        # 4. Transformer Outputs\n",
    "        encoder_outputs = []\n",
    "        for i in range(self._config_dict[\"num_hidden_layers\"]):\n",
    "            layer = self._transformer_layers[i]\n",
    "            embeddings, _, _ = layer([embeddings, attention_mask])\n",
    "            encoder_outputs.append(embeddings)\n",
    "\n",
    "        # First word of last layer outputs [CLS]\n",
    "        cls_token_tensor = tf.keras.layers.Lambda(lambda x: tf.squeeze(x[:, 0:1, :], axis=1))(encoder_outputs[-1])\n",
    "        # batch_size x embedding_size\n",
    "        cls_output = self._pooler_layer(cls_token_tensor)\n",
    "        # batch_size x sequence_length x embedding_size\n",
    "        token_embeddings = encoder_outputs[-1]\n",
    "\n",
    "        # check for masked lm positions\n",
    "        # only for encoder forward pass. This is for MaskedLM training\n",
    "        if \"masked_lm_positions\" in inputs:\n",
    "            masked_lm_positions = inputs[\"masked_lm_positions\"]\n",
    "        else:\n",
    "            masked_lm_positions = None\n",
    "\n",
    "        # MaskedLM layer only project it and normalize (b x s x h)\n",
    "        token_embeddings_mlm = self._masked_lm_layer(token_embeddings, masked_lm_positions)\n",
    "        token_logits = tf.matmul(\n",
    "            token_embeddings_mlm, tf.cast(self.get_embedding_table(), dtype=tf_utils.get_dtype()), transpose_b=True\n",
    "        )\n",
    "        # token_logits         =  tf.nn.bias_add(token_logits, self._masked_lm_bias)\n",
    "        token_logits = self._masked_lm_bias(token_logits)\n",
    "        last_token_logits = tf.keras.layers.Lambda(lambda x: x[:, -1, :])(token_logits)\n",
    "\n",
    "        result = {\n",
    "            \"cls_output\": cls_output,\n",
    "            \"token_embeddings\": token_embeddings,\n",
    "            \"token_logits\": token_logits,\n",
    "            \"last_token_logits\": last_token_logits,\n",
    "        }\n",
    "\n",
    "        if self._return_all_layer_outputs:\n",
    "            all_cls_output = []\n",
    "            all_token_logits = []\n",
    "            for per_layer_token_embeddings in encoder_outputs:\n",
    "                per_cls_token_tensor = tf.keras.layers.Lambda(lambda x: tf.squeeze(x[:, 0:1, :], axis=1))(\n",
    "                    per_layer_token_embeddings\n",
    "                )\n",
    "                all_cls_output.append(self._pooler_layer(per_cls_token_tensor))\n",
    "\n",
    "                # token logits per layer\n",
    "                layer_token_embeddings_mlm = self._masked_lm_layer(per_layer_token_embeddings, masked_lm_positions)\n",
    "                layer_token_logits = tf.matmul(\n",
    "                    layer_token_embeddings_mlm,\n",
    "                    tf.cast(self.get_embedding_table(), dtype=tf_utils.get_dtype()),\n",
    "                    transpose_b=True,\n",
    "                )\n",
    "                layer_token_logits = self._masked_lm_bias(layer_token_logits)\n",
    "                all_token_logits.append(layer_token_logits)\n",
    "\n",
    "            result[\"all_layer_token_embeddings\"] = encoder_outputs\n",
    "            result[\"all_layer_cls_output\"] = all_cls_output\n",
    "            result[\"all_layer_token_logits\"] = all_token_logits\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee0b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "fe8460a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_training': True, 'use_dropout': True, 'use_masked_lm_positions': True, 'return_all_layer_outputs': True}\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"intermediate_act\": \"gelu\",\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"embedding_size\": 768,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 3072,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"attention_head_size\": 64,\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"type_vocab_size\": 2,\n",
    "    \"vocab_size\": 30000,\n",
    "    \"layer_norm_epsilon\": 1e-12\n",
    "}\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = MixEncoder(config,\n",
    "                         is_training=True,\n",
    "                         use_dropout=True,\n",
    "                         use_masked_lm_positions=True,\n",
    "                         return_all_layer_outputs=True)\n",
    "model = model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "a30e78db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <KerasTensor: shape=(None, None) dtype=int32 (created by layer 'input_ids')>,\n",
       " 'input_mask': <KerasTensor: shape=(None, None, None) dtype=float32 (created by layer 'input_mask')>,\n",
       " 'input_type_ids': <KerasTensor: shape=(None, None) dtype=int32 (created by layer 'input_type_ids')>,\n",
       " 'masked_lm_positions': <KerasTensor: shape=(None, None) dtype=int32 (created by layer 'masked_lm_positions')>}"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "1069296c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cls_output': <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       " 'token_embeddings': <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       " 'token_logits': <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       " 'last_token_logits': <KerasTensor: shape=(None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       " 'all_layer_token_embeddings': [<KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>],\n",
       " 'all_layer_cls_output': [<KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_transformers/bert')>],\n",
       " 'all_layer_token_logits': [<KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>,\n",
       "  <KerasTensor: shape=(None, None, 30000) dtype=float32 (created by layer 'tf_transformers/bert')>]}"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "e26d2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(batch_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "f4c77bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cls_output', 'token_embeddings', 'token_logits', 'last_token_logits', 'all_layer_token_embeddings', 'all_layer_cls_output', 'all_layer_token_logits'])"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b2f70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('venv_tf2.4': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd0e43aa85e0d007ca602e04b3033c86d6af4f225b69c2da0d32fa6602213775d26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
