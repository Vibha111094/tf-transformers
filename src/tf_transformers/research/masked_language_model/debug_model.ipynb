{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad33929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3af36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "import hydra\n",
    "import tensorflow as tf\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "from tf_transformers.core import TPUTrainer\n",
    "from tf_transformers.data import TFReader\n",
    "from tf_transformers.data.callbacks.mlm_callback import MLMCallback\n",
    "from tf_transformers.data.processors.mlm import (\n",
    "    dynamic_causal_lm_from_features,\n",
    "    dynamic_masking_from_features,\n",
    "    dynamic_prefix_lm_from_features,\n",
    ")\n",
    "from tf_transformers.data.utils import auto_batch\n",
    "from tf_transformers.losses import cross_entropy_loss\n",
    "from tf_transformers.optimization import create_optimizer\n",
    "from tf_transformers.text import SentencepieceTokenizer\n",
    "from tf_transformers.utils import tf_utils\n",
    "\n",
    "\n",
    "def load_tokenizer(cfg):\n",
    "    \"\"\"Load tf text based tokenizer\"\"\"\n",
    "    model_file_path = cfg.tokenizer.model_file_path\n",
    "    do_lower_case = cfg.tokenizer.do_lower_case\n",
    "    special_tokens = cfg.tokenizer.special_tokens\n",
    "\n",
    "    tokenizer_layer = SentencepieceTokenizer(\n",
    "        model_file_path=model_file_path, lower_case=do_lower_case, special_tokens=special_tokens\n",
    "    )\n",
    "\n",
    "    return tokenizer_layer\n",
    "\n",
    "\n",
    "def get_tfdataset_from_tfrecords(tfrecord_path_list):\n",
    "    \"\"\"Get tf dataset from tfrecords\"\"\"\n",
    "    all_files = []\n",
    "    for tfrecord_path in tfrecord_path_list:\n",
    "        all_files.extend(glob.glob(\"{}/*.tfrecord\".format(tfrecord_path)))\n",
    "    schema = json.load(open(\"{}/schema.json\".format(tfrecord_path)))\n",
    "    tf_reader = TFReader(schema=schema, tfrecord_files=all_files)\n",
    "    train_dataset = tf_reader.read_record()\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    tfrecord_path_list,\n",
    "    max_seq_len,\n",
    "    max_predictions_per_batch,\n",
    "    vocab_size,\n",
    "    cls_token_id,\n",
    "    sep_token_id,\n",
    "    unk_token_id,\n",
    "    pad_token_id,\n",
    "    mask_token_id,\n",
    "    batch_size,\n",
    "    min_sen_len,\n",
    "):\n",
    "    \"\"\"Get dataset after mlm from TFRecords\"\"\"\n",
    "\n",
    "    def filter_by_length(x, min_sen_len):\n",
    "        \"\"\"Filter by minimum sentence length (subwords)\"\"\"\n",
    "        return tf.squeeze(tf.greater_equal(tf.shape(x['input_ids']), tf.constant(min_sen_len)), axis=0)\n",
    "\n",
    "    def filter_by_batch(x, y, batch_size):\n",
    "        \"\"\"Filter by batch size\"\"\"\n",
    "        x_batch = tf.shape(x['input_ids'])[0]\n",
    "        return tf.equal(x_batch, tf.constant(batch_size))\n",
    "\n",
    "    def prepare_3d_input_mask_mlm(input_mask):\n",
    "        \"\"\"Prepare 3D mask from 2D\"\"\"\n",
    "        batch_size = tf.shape(input_mask)[0]\n",
    "        seq_length = tf.shape(input_mask)[1]\n",
    "\n",
    "        to_mask = tf.cast(tf.reshape(input_mask, [batch_size, 1, seq_length]), dtype=input_mask.dtype)\n",
    "        broadcast_ones = tf.ones(shape=[batch_size, seq_length, 1], dtype=input_mask.dtype)\n",
    "\n",
    "        mask = broadcast_ones * to_mask\n",
    "\n",
    "        return tf.cast(mask, tf.float32)\n",
    "\n",
    "    # Dynamic MLM\n",
    "    dynamic_mlm_fn = dynamic_masking_from_features(\n",
    "        max_seq_len,\n",
    "        max_predictions_per_batch,\n",
    "        vocab_size,\n",
    "        cls_token_id,\n",
    "        sep_token_id,\n",
    "        unk_token_id,\n",
    "        pad_token_id,\n",
    "        mask_token_id,\n",
    "    )\n",
    "\n",
    "    # Dynamic Prefix LM\n",
    "    dynamic_prefix_lm = dynamic_prefix_lm_from_features(max_seq_len, cls_token_id, sep_token_id)\n",
    "\n",
    "    # Dynamic Causal LM\n",
    "    dynamic_causal_lm = dynamic_causal_lm_from_features(max_seq_len, cls_token_id, sep_token_id)\n",
    "\n",
    "    train_dataset = get_tfdataset_from_tfrecords(tfrecord_path_list)\n",
    "\n",
    "    if min_sen_len and min_sen_len > 0:\n",
    "        train_dataset = train_dataset.filter(lambda x: filter_by_length(x, min_sen_len))\n",
    "\n",
    "    # prob check has to be inside map\n",
    "    # otherwise things become deterministic\n",
    "    def get_dataset_based_on_prob(item):\n",
    "        \"\"\"Map function\"\"\"\n",
    "\n",
    "        def add_mark(x, mode, prob):\n",
    "            \"\"\"Check are we getting all if conditions with equal probability\"\"\"\n",
    "            x['mode'] = [mode]\n",
    "            x['prob'] = [prob]\n",
    "            return x\n",
    "\n",
    "        def map_mlm(x):\n",
    "            \"\"\"MLM\"\"\"\n",
    "            x['input_ids'] = tf.RaggedTensor.from_tensor(tf.expand_dims(x['input_ids'], axis=0))\n",
    "            x_copy, y_copy = dynamic_mlm_fn(x)\n",
    "            x = {}\n",
    "            for name, v_tensor in x_copy.items():\n",
    "                x[name] = tf.squeeze(v_tensor, axis=0)\n",
    "            y = {}\n",
    "            for name, v_tensor in y_copy.items():\n",
    "                y[name] = tf.squeeze(v_tensor, axis=0)\n",
    "            x['3d_mask'] = tf.squeeze(prepare_3d_input_mask_mlm(x_copy['input_mask']), axis=0)\n",
    "            for name, v_tensor in y.items():\n",
    "                x[name] = v_tensor\n",
    "            return x\n",
    "\n",
    "        def map_pcmlm(x):\n",
    "            \"\"\"Prefix Causal LM\"\"\"\n",
    "            x, y = dynamic_prefix_lm(x)\n",
    "            for name, v_tensor in y.items():\n",
    "                x[name] = v_tensor\n",
    "            return x\n",
    "\n",
    "        def map_cmlm(x):\n",
    "            \"\"\"Causal LM\"\"\"\n",
    "            x, y = dynamic_causal_lm(x)\n",
    "            for name, v_tensor in y.items():\n",
    "                x[name] = v_tensor\n",
    "            return x\n",
    "\n",
    "        prob = tf.random.uniform(shape=())\n",
    "        # Keep a copy like this importatnt\n",
    "        # otherwise transformation in first if cond might affect other\n",
    "        input_ids = item['input_ids']\n",
    "        \n",
    "        x = map_mlm(item)\n",
    "        x['masked_lm_positions'] = tf.cast(x['masked_lm_positions'], dtype=tf.int32)\n",
    "        x['masked_lm_weights'] = tf.cast(x['masked_lm_weights'], dtype=tf.int32)\n",
    "        # x['input_mask'] = x['3d_mask']\n",
    "        # del x['3d_mask']\n",
    "        \n",
    "        return x\n",
    "\n",
    "#         # Do MLM\n",
    "#         if prob <= 0.33:\n",
    "#             x = map_mlm(item)\n",
    "#             x['masked_lm_positions'] = tf.cast(x['masked_lm_positions'], dtype=tf.int32)\n",
    "#             x['masked_lm_weights'] = tf.cast(x['masked_lm_weights'], dtype=tf.int32)\n",
    "#             x['input_mask'] = x['3d_mask']\n",
    "#             del x['3d_mask']\n",
    "#             # x = add_mark(x, \"mlm\", prob)\n",
    "\n",
    "#         # Prefix CLM\n",
    "#         elif prob < 0.66:\n",
    "#             x = map_pcmlm({\"input_ids\": input_ids})\n",
    "#             del x['input_mask']\n",
    "#             x['input_mask'] = x['3d_mask']\n",
    "#             del x['3d_mask']\n",
    "#             # x = add_mark(x, \"prefix\", prob)\n",
    "#         # Causal LM\n",
    "#         else:\n",
    "#             x = map_cmlm({\"input_ids\": input_ids})\n",
    "#             del x['input_mask']\n",
    "#             x['input_mask'] = x['3d_mask']\n",
    "#             del x['3d_mask']\n",
    "#             # x = add_mark(x, \"causal\", prob)\n",
    "#          return x\n",
    "\n",
    "    train_dataset = train_dataset.map(get_dataset_based_on_prob, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_dataset = auto_batch(\n",
    "        train_dataset,\n",
    "        batch_size,\n",
    "        x_keys=['input_ids', 'input_type_ids', 'input_mask', 'masked_lm_positions'],\n",
    "        y_keys=['masked_lm_labels', 'masked_lm_weights'],\n",
    "        shuffle=True,\n",
    "    )\n",
    "    train_dataset = train_dataset.filter(lambda x, y: filter_by_batch(x, y, batch_size))\n",
    "    # train_dataset = train_dataset.shuffle(100)\n",
    "    # train_dataset = train_dataset.prefetch(100)\n",
    "\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a7a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b186130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(vocab_size):\n",
    "    \"\"\"Model\"\"\"\n",
    "\n",
    "    def model_fn():\n",
    "        config = {\n",
    "            \"attention_probs_dropout_prob\": 0.1,\n",
    "            \"hidden_act\": \"gelu\",\n",
    "            \"intermediate_act\": \"gelu\",\n",
    "            \"hidden_dropout_prob\": 0.1,\n",
    "            \"embedding_size\": 768,\n",
    "            \"initializer_range\": 0.02,\n",
    "            \"intermediate_size\": 3072,\n",
    "            \"max_position_embeddings\": 512,\n",
    "            \"num_attention_heads\": 12,\n",
    "            \"attention_head_size\": 64,\n",
    "            \"num_hidden_layers\": 12,\n",
    "            \"type_vocab_size\": 1,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"layer_norm_epsilon\": 1e-12,\n",
    "        }\n",
    "\n",
    "        from tf_transformers.core import LegacyModel\n",
    "        from tf_transformers.models import BertEncoder\n",
    "\n",
    "        class MixEncoder(BertEncoder):\n",
    "            def __init__(self, config, **kwargs):\n",
    "                print(kwargs)\n",
    "                super(MixEncoder, self).__init__(config, **kwargs)\n",
    "\n",
    "            def get_model(self, initialize_only=False):\n",
    "                \"\"\"Convert tf.keras.Layer to a tf.keras.Model/LegacyModel.\n",
    "                Args:\n",
    "                    self: model (tf.keras.Layer) instance\n",
    "                \"\"\"\n",
    "\n",
    "                input_ids = tf.keras.layers.Input(\n",
    "                    shape=(self._sequence_length,),\n",
    "                    batch_size=self._batch_size,\n",
    "                    dtype=tf.int32,\n",
    "                    name=\"input_ids\",\n",
    "                )\n",
    "                input_mask = tf.keras.layers.Input(\n",
    "                    shape=(self._sequence_length, self._sequence_length),\n",
    "                    batch_size=self._batch_size,\n",
    "                    dtype=tf.float32,\n",
    "                    name=\"input_mask\",\n",
    "                )\n",
    "                input_type_ids = tf.keras.layers.Input(\n",
    "                    shape=(self._sequence_length,),\n",
    "                    batch_size=self._batch_size,\n",
    "                    dtype=tf.int32,\n",
    "                    name=\"input_type_ids\",\n",
    "                )\n",
    "                masked_lm_positions = tf.keras.layers.Input(\n",
    "                    shape=(None,),\n",
    "                    batch_size=self._batch_size,\n",
    "                    dtype=tf.int32,\n",
    "                    name=\"masked_lm_positions\",\n",
    "                )\n",
    "                inputs = {}\n",
    "                inputs[\"input_ids\"] = input_ids  # Default\n",
    "                # if mask_mode != 'causal', user has to provde mask\n",
    "                if self._mask_mode != \"causal\":\n",
    "                    inputs[\"input_mask\"] = input_mask\n",
    "                # If type mebddings required\n",
    "                if self._type_embeddings_layer:\n",
    "                    inputs[\"input_type_ids\"] = input_type_ids\n",
    "                # if masked_lm_positions\n",
    "                if self._use_masked_lm_positions:\n",
    "                    inputs[\"masked_lm_positions\"] = masked_lm_positions\n",
    "\n",
    "                layer_outputs = self(inputs)\n",
    "                if initialize_only:\n",
    "                    return inputs, layer_outputs\n",
    "\n",
    "                # Adding model_config is a hack\n",
    "                model = LegacyModel(inputs=inputs, outputs=layer_outputs, name=self._model_name)\n",
    "                model.model_config = self._config_dict\n",
    "                return model\n",
    "\n",
    "            def call_encoder(self, inputs):\n",
    "                \"\"\"Forward pass of an Encoder\n",
    "\n",
    "                Args:\n",
    "                    inputs ([dict of tf.Tensor]): This is the input to the model.\n",
    "\n",
    "                    'input_ids'         --> tf.int32 (b x s)\n",
    "                    'input_mask'        --> tf.int32 (b x s) # optional\n",
    "                    'input_type_ids'    --> tf.int32 (b x s) # optional\n",
    "\n",
    "                Returns:\n",
    "                    [dict of tf.Tensor]: Output from the model\n",
    "\n",
    "                    'cls_output'        --> tf.float32 (b x s) # optional\n",
    "                    'token_embeddings'  --> tf.float32 (b x s x h)\n",
    "                    'all_layer_token_embeddings' --> tf.float32 (List of (b x s x h)\n",
    "                                                    from all layers)\n",
    "                    'all_layer_cls_output'       --> tf.float32 (List of (b x s)\n",
    "                                                    from all layers)\n",
    "                \"\"\"\n",
    "\n",
    "                # 1. Collect Word Embeddings\n",
    "                input_ids = inputs[\"input_ids\"]\n",
    "                sequence_length = tf.shape(input_ids)[1]\n",
    "                embeddings = self._embedding_layer(input_ids)\n",
    "                # Add word_embeddings + position_embeddings + type_embeddings\n",
    "                if self._type_embeddings_layer:\n",
    "                    input_type_ids = inputs[\"input_type_ids\"]\n",
    "                    type_embeddings = self._type_embeddings_layer(input_type_ids)\n",
    "                    embeddings = embeddings + type_embeddings\n",
    "                if self._positional_embedding_layer:\n",
    "                    positional_embeddings = self._positional_embedding_layer(tf.range(sequence_length))\n",
    "                    embeddings = embeddings + positional_embeddings\n",
    "\n",
    "                # 2. Norm + dropout\n",
    "                embeddings = self._embedding_norm(embeddings)\n",
    "                embeddings = self._embedding_dropout(embeddings, training=self._use_dropout)\n",
    "\n",
    "                # 3. Attention  Mask\n",
    "                attention_mask = inputs['input_mask']\n",
    "\n",
    "                # 4. Transformer Outputs\n",
    "                encoder_outputs = []\n",
    "                for i in range(self._config_dict[\"num_hidden_layers\"]):\n",
    "                    layer = self._transformer_layers[i]\n",
    "                    embeddings, _, _ = layer([embeddings, attention_mask])\n",
    "                    encoder_outputs.append(embeddings)\n",
    "\n",
    "                # First word of last layer outputs [CLS]\n",
    "                cls_token_tensor = tf.keras.layers.Lambda(lambda x: tf.squeeze(x[:, 0:1, :], axis=1))(\n",
    "                    encoder_outputs[-1]\n",
    "                )\n",
    "                # batch_size x embedding_size\n",
    "                cls_output = self._pooler_layer(cls_token_tensor)\n",
    "                # batch_size x sequence_length x embedding_size\n",
    "                token_embeddings = encoder_outputs[-1]\n",
    "\n",
    "                # check for masked lm positions\n",
    "                # only for encoder forward pass. This is for MaskedLM training\n",
    "                if \"masked_lm_positions\" in inputs:\n",
    "                    masked_lm_positions = inputs[\"masked_lm_positions\"]\n",
    "                else:\n",
    "                    masked_lm_positions = None\n",
    "\n",
    "                # MaskedLM layer only project it and normalize (b x s x h)\n",
    "                token_embeddings_mlm = self._masked_lm_layer(token_embeddings, masked_lm_positions)\n",
    "                token_logits = tf.matmul(\n",
    "                    token_embeddings_mlm,\n",
    "                    tf.cast(self.get_embedding_table(), dtype=tf_utils.get_dtype()),\n",
    "                    transpose_b=True,\n",
    "                )\n",
    "                # token_logits         =  tf.nn.bias_add(token_logits, self._masked_lm_bias)\n",
    "                token_logits = self._masked_lm_bias(token_logits)\n",
    "                last_token_logits = tf.keras.layers.Lambda(lambda x: x[:, -1, :])(token_logits)\n",
    "\n",
    "                result = {\n",
    "                    \"cls_output\": cls_output,\n",
    "                    \"token_embeddings\": token_embeddings,\n",
    "                    \"token_logits\": token_logits,\n",
    "                    \"last_token_logits\": last_token_logits,\n",
    "                }\n",
    "\n",
    "                if self._return_all_layer_outputs:\n",
    "                    all_cls_output = []\n",
    "                    all_token_logits = []\n",
    "                    for per_layer_token_embeddings in encoder_outputs:\n",
    "                        per_cls_token_tensor = tf.keras.layers.Lambda(lambda x: tf.squeeze(x[:, 0:1, :], axis=1))(\n",
    "                            per_layer_token_embeddings\n",
    "                        )\n",
    "                        all_cls_output.append(self._pooler_layer(per_cls_token_tensor))\n",
    "\n",
    "                        # token logits per layer\n",
    "                        layer_token_embeddings_mlm = self._masked_lm_layer(\n",
    "                            per_layer_token_embeddings, masked_lm_positions\n",
    "                        )\n",
    "                        layer_token_logits = tf.matmul(\n",
    "                            layer_token_embeddings_mlm,\n",
    "                            tf.cast(self.get_embedding_table(), dtype=tf_utils.get_dtype()),\n",
    "                            transpose_b=True,\n",
    "                        )\n",
    "                        layer_token_logits = self._masked_lm_bias(layer_token_logits)\n",
    "                        all_token_logits.append(layer_token_logits)\n",
    "\n",
    "                    result[\"all_layer_token_embeddings\"] = encoder_outputs\n",
    "                    result[\"all_layer_cls_output\"] = all_cls_output\n",
    "                    result[\"all_layer_token_logits\"] = all_token_logits\n",
    "\n",
    "                return result\n",
    "\n",
    "#         model = MixEncoder(\n",
    "#             config, is_training=True, use_dropout=True, use_masked_lm_positions=True, return_all_layer_outputs=False\n",
    "#         )\n",
    "#         model = model.get_model()\n",
    "        \n",
    "        model = BertEncoder(\n",
    "            config, is_training=True, use_dropout=True, use_masked_lm_positions=True, return_all_layer_outputs=False\n",
    "        )\n",
    "        model = model.get_model()\n",
    "\n",
    "        print(\"Model inputs\", model.input)\n",
    "        print(\"Model outputs\", model.output)\n",
    "        return model\n",
    "\n",
    "    return model_fn\n",
    "\n",
    "\n",
    "def get_optimizer(learning_rate, train_steps, warmup_steps, optimizer_type):\n",
    "    def optimizer_fn():\n",
    "        optimizer, learning_rate_fn = create_optimizer(\n",
    "            init_lr=learning_rate,\n",
    "            num_train_steps=train_steps,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            optimizer_type=optimizer_type,\n",
    "        )\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    return optimizer_fn\n",
    "\n",
    "\n",
    "def get_loss(loss_type):\n",
    "\n",
    "    if loss_type and loss_type == 'joint':\n",
    "\n",
    "        def lm_loss(y_true_dict, y_pred_dict):\n",
    "            \"\"\"Joint loss over all layers\"\"\"\n",
    "            loss_dict = {}\n",
    "            loss_holder = []\n",
    "            for layer_count, per_layer_output in enumerate(y_pred_dict['all_layer_token_logits']):\n",
    "\n",
    "                loss = cross_entropy_loss(\n",
    "                    labels=y_true_dict['masked_lm_labels'],\n",
    "                    logits=per_layer_output,\n",
    "                    label_weights=y_true_dict['masked_lm_weights'],\n",
    "                )\n",
    "                loss_dict['loss_{}'.format(layer_count + 1)] = loss\n",
    "                loss_holder.append(loss)\n",
    "            loss_dict['loss'] = tf.reduce_mean(loss_holder, axis=0)\n",
    "            return loss_dict\n",
    "\n",
    "    else:\n",
    "        print(\"Normal loss -------------------------------------------------------\")\n",
    "        def lm_loss(y_true_dict, y_pred_dict):\n",
    "            \"\"\"Joint loss over all layers\"\"\"\n",
    "            loss_dict = {}\n",
    "            loss = cross_entropy_loss(\n",
    "                labels=y_true_dict['masked_lm_labels'],\n",
    "                logits=y_pred_dict['token_logits'],\n",
    "                label_weights=y_true_dict['masked_lm_weights'],\n",
    "            )\n",
    "            loss_dict['loss'] = loss\n",
    "            return loss_dict\n",
    "\n",
    "    return lm_loss\n",
    "\n",
    "\n",
    "def get_trainer(device_type, device_address, dtype):\n",
    "\n",
    "    if device_type == 'tpu':\n",
    "        trainer = TPUTrainer(tpu_address=device_address, dtype=dtype)\n",
    "        return trainer\n",
    "    if device_type == 'gpu':\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac03aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa557b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87093af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "with initialize():\n",
    "    # compose from config.yaml, this composes a bunch of defaults in:\n",
    "    cfg=compose(config_name=\"config/train_config.yaml\")\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "    cfg = cfg.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478b2bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5788e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Load tokenizer from tf text SentencePieceTokenizer\n",
    "    tokenizer_sp = load_tokenizer(cfg)\n",
    "\n",
    "    # Vocab and tokens\n",
    "    model_file_path = cfg.tokenizer.model_file_path\n",
    "    vocab_size = cfg.tokenizer.vocab_size\n",
    "    cls_id = tokenizer_sp._vocab[cfg.tokenizer.cls_token]\n",
    "    mask_id = tokenizer_sp._vocab[cfg.tokenizer.mask_token]\n",
    "    sep_id = tokenizer_sp._vocab[cfg.tokenizer.sep_token]\n",
    "    unk_id = tokenizer_sp._vocab[cfg.tokenizer.unk_token]\n",
    "    pad_id = tokenizer_sp._vocab[cfg.tokenizer.pad_token]\n",
    "\n",
    "    # Data\n",
    "    max_seq_len = cfg.data.max_seq_len\n",
    "    max_predictions_per_batch = cfg.data.max_predictions_per_batch\n",
    "    batch_size = cfg.data.batch_size\n",
    "    min_sen_len = cfg.data.min_sen_len\n",
    "\n",
    "    # Train Dataset\n",
    "    tfrecord_path_list = cfg.data.tfrecord_path_list\n",
    "    train_dataset = get_dataset(\n",
    "        tfrecord_path_list,\n",
    "        max_seq_len,\n",
    "        max_predictions_per_batch,\n",
    "        vocab_size,\n",
    "        cls_id,\n",
    "        sep_id,\n",
    "        unk_id,\n",
    "        pad_id,\n",
    "        mask_id,\n",
    "        batch_size,\n",
    "        min_sen_len,\n",
    "    )\n",
    "\n",
    "    # Get Model\n",
    "    model_fn = get_model(vocab_size)\n",
    "\n",
    "    # Get Optimizer\n",
    "    optimizer_fn = get_optimizer(\n",
    "        cfg.model.optimizer.learning_rate,\n",
    "        cfg.model.optimizer.train_steps,\n",
    "        cfg.model.optimizer.warmup_steps,\n",
    "        cfg.model.optimizer.optimizer_type,\n",
    "    )\n",
    "\n",
    "    # Get loss\n",
    "    loss_fn = get_loss(cfg.model.loss.loss_type)\n",
    "    training_loss_names = None\n",
    "    if cfg.model.loss.loss_type == 'joint':\n",
    "        training_loss_names = ['loss_{}'.format(i + 1) for i in range(12)]  # 12 num of hidden layers\n",
    "\n",
    "    # Model params\n",
    "    epochs = cfg.model.epochs\n",
    "    steps_per_epoch = cfg.model.steps_per_epoch\n",
    "    model_save_dir = cfg.model.model_save_dir\n",
    "    callback_steps = cfg.model.callback_steps\n",
    "\n",
    "    # Set callback\n",
    "    # To use new sentencepiece model in T5 use like this\n",
    "    t5_kwargs = {\n",
    "        'bos_token': '[CLS]',\n",
    "        'eos_token': '</s>',\n",
    "        'unk_token': '<unk>',\n",
    "        'pad_token': '<pad>',\n",
    "        'mask_token': '[MASK]',\n",
    "        'vocab_file': '{}'.format(model_file_path),\n",
    "    }\n",
    "    tokenizer_hf = T5Tokenizer(**t5_kwargs)\n",
    "    tokenizer_hf.unique_no_split_tokens = tokenizer_hf.all_special_tokens\n",
    "    mlm_callback = MLMCallback(tokenizer_hf)\n",
    "\n",
    "    # Get trainer\n",
    "    trainer = get_trainer(cfg.trainer.device_type, cfg.trainer.device_address, cfg.trainer.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d197bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "dummy_inputs = []\n",
    "dummy_labels = []\n",
    "for (batch_inputs, batch_labels) in tqdm.tqdm(train_dataset):\n",
    "    \n",
    "    for k,v in batch_inputs.items():\n",
    "        print(k, '--->', v.shape)\n",
    "    for k, v in batch_labels.items():\n",
    "        print(k, '--->', v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08d59eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f19331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5948980",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = trainer.distribution_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbba27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_dist = iter(strategy.experimental_distribute_dataset(train_dataset))\n",
    "with strategy.scope():\n",
    "    model = model_fn()\n",
    "    optimizer = optimizer_fn()\n",
    "    \n",
    "train_loss_fn = loss_fn\n",
    "GLOBAL_BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(batch_labels, model_outputs):\n",
    "    \"\"\"Loss computation which takes care of loss reduction based on GLOBAL_BATCH_SIZE\"\"\"\n",
    "    per_example_loss = train_loss_fn(batch_labels, model_outputs)\n",
    "    per_example_loss_averaged = {}\n",
    "    # Inplace update\n",
    "    # Avergae loss per global batch size , recommended\n",
    "    for name, loss in per_example_loss.items():\n",
    "        per_example_loss_averaged[name] = tf.nn.compute_average_loss(loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "    return per_example_loss_averaged\n",
    "    \n",
    "# Train Functions\n",
    "@tf.function\n",
    "def do_train(iterator):\n",
    "    \"\"\"The step function for one training step\"\"\"\n",
    "\n",
    "    def train_step(dist_inputs):\n",
    "        \"\"\"The computation to run on each device.\"\"\"\n",
    "        batch_inputs, batch_labels = dist_inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            model_outputs = model(batch_inputs)\n",
    "            loss = compute_loss(batch_labels, model_outputs)\n",
    "\n",
    "            # tf.debugging.check_numerics(loss['loss'], message='Loss value is either NaN or inf')\n",
    "\n",
    "            # TODO\n",
    "            # Scales down the loss for gradients to be invariant from replicas.\n",
    "            # loss = loss / strategy.num_replicas_in_sync\n",
    "        grads = tape.gradient(loss[\"loss\"], model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        # training_loss.update_state(loss * strategy.num_replicas_in_sync)\n",
    "        return loss\n",
    "    \n",
    "    dist_inputs = next(iterator)\n",
    "    loss = strategy.run(train_step, args=(dist_inputs,))\n",
    "    return loss\n",
    "\n",
    "l = do_train(train_dataset_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697f9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de821f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beae7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269adce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521ac20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7acd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f1d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (batch_inputs, batch_labels) in train_dataset:\n",
    "    print(batch_inputs, batch_labels)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('venv_tf2.4': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd0e43aa85e0d007ca602e04b3033c86d6af4f225b69c2da0d32fa6602213775d26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
