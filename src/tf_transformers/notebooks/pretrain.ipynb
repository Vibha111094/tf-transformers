{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9869f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aca8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect dataset\n",
    "\n",
    "\n",
    "def hf_dump_chars_to_textfile(file, dataset, data_keys, max_char=-1):\n",
    "    \"\"\"Write part of a TFDS sentence dataset to lines in a text file.\n",
    "\n",
    "  Args:\n",
    "    dataset: tf.dataset containing string-data.\n",
    "    data_keys: what keys in dataset to dump from.\n",
    "    max_char: max character to dump to text file.\n",
    "\n",
    "  Returns:\n",
    "    name of temp file with dataset bytes, exact number of characters dumped.\n",
    "  \"\"\"\n",
    "    line_count = 0\n",
    "    with open(file, \"a+\") as outfp:\n",
    "        char_count = 0\n",
    "        for example in tqdm.tqdm(dataset):\n",
    "            for k in data_keys:\n",
    "                line = example[k]\n",
    "                if len(line) < 10 or line == \"\\n\":  # 50 chars\n",
    "                    continue\n",
    "                # line = line + b\"\\n\"\n",
    "                char_count += len(line)\n",
    "                line_count += 1\n",
    "                outfp.write(line)\n",
    "\n",
    "    print(\"Total lines {}, chars {}\".format(line_count, char_count))\n",
    "\n",
    "\n",
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikipedia\", \"20200501.en\")\n",
    "hf_dump_chars_to_textfile(\"/home/sidhu/Datasets/wikipedia.txt\", \n",
    "                          dataset[\"train\"].select(range(1000)),\n",
    "                          \n",
    "                          (\"text\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08142154",
   "metadata": {},
   "outputs": [],
   "source": [
    "212763665\n",
    "\n",
    "3187609 # After filtering length of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210d00f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Sentencepiece tokenizer (Albert)\n",
    "\n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train(input='DATA/wikipedia.txt',\n",
    "                               model_prefix='bert-joint',\n",
    "                               vocab_size=32000,\n",
    "                               pad_id=0,\n",
    "                               unk_id=1,\n",
    "                               bos_id=-1,\n",
    "                               user_defined_symbols=['(', ')', '\"', '-', '.', '–', '£', '€'],\n",
    "                               control_symbols=['[CLS]','[SEP]','[MASK]'],\n",
    "                               shuffle_input_sentence=True,\n",
    "                               input_sentence_size=10000000,\n",
    "                               character_coverage=0.99995,\n",
    "                               model_type='unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dfb449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54566f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone tft\n",
    "\n",
    "git clone -b modification https://github.com/legacyai/tf-transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571dc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd02a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import tqdm\n",
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f76bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tokenizer\n",
    "model_file_path = '/home/sidhu/Datasets/PRETRAIN_DATA/vocab/bert-joint.model'\n",
    "dtype = tf.int32\n",
    "nbest_size = 0\n",
    "alpha = 1.0\n",
    "\n",
    "def _create_tokenizer(model_serialized_proto, dtype, nbest_size, alpha):\n",
    "    return tf_text.SentencepieceTokenizer(\n",
    "        model=model_serialized_proto,\n",
    "        out_type=dtype,\n",
    "        nbest_size=nbest_size,\n",
    "        alpha=alpha)\n",
    "\n",
    "model_serialized_proto = tf.io.gfile.GFile(model_file_path,\n",
    "                                                       \"rb\").read()\n",
    "\n",
    "tokenizer = _create_tokenizer(model_serialized_proto, \n",
    "                             dtype,\n",
    "                             nbest_size,\n",
    "                             alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21f2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read wikipedia data\n",
    "\n",
    "file_paths = ['DATA/wikipedia.txt']\n",
    "\n",
    "#file_buffer = open(file_paths[0])\n",
    "\n",
    "# file_paths = ['/tmp/tmpip9jwekj']\n",
    "dataset = tf.data.TextLineDataset(file_paths)\n",
    "BATCH_SIZE = 1024\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "\n",
    "def parse_train():\n",
    "    for batch_input in tqdm.tqdm(dataset):\n",
    "        batch_tokenized = tokenizer.tokenize(batch_input).merge_dims(-1,1).to_list()\n",
    "        for input_ids in batch_tokenized:\n",
    "            \n",
    "            yield {\"input_ids\": input_ids}\n",
    "        \n",
    "            \n",
    "# Lets write using TF Writer\n",
    "# Use TFProcessor for smalled data\n",
    "\n",
    "schema = {\n",
    "    \"input_ids\": (\"var_len\", \"int\"),\n",
    "}\n",
    "\n",
    "tfrecord_train_dir = 'TFRECORD'\n",
    "tfrecord_filename = 'wikipedia'\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    n_files=100,\n",
    "                    overwrite=True\n",
    "                    )\n",
    "tfwriter.process(parse_fn=parse_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c11701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca046e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF way\n",
    "# from transformers import AlbertTokenizer\n",
    "\n",
    "# tokenizer_hf = AlbertTokenizer(vocab_file='vocab/bert-joint.model')\n",
    "\n",
    "# def parse_train_hf():\n",
    "#     for batch_input in tqdm.tqdm(dataset):\n",
    "#         batch_input = [item.decode() for item in batch_input.numpy()]\n",
    "#         batch_tokenized = tokenizer_hf(batch_input)['input_ids']\n",
    "#         for input_ids in batch_tokenized:\n",
    "            \n",
    "#             yield {\"input_ids\": input_ids}\n",
    "        \n",
    "            \n",
    "# schema = {\n",
    "#     \"input_ids\": (\"var_len\", \"int\"),\n",
    "# }\n",
    "\n",
    "# tfrecord_train_dir = 'DUMMY'\n",
    "# tfrecord_filename = 'wikipedia'\n",
    "# tfwriter = TFWriter(schema=schema, \n",
    "#                     file_name=tfrecord_filename, \n",
    "#                     model_dir=tfrecord_train_dir,\n",
    "#                     tag='train',\n",
    "#                     n_files=100,\n",
    "#                     overwrite=True\n",
    "#                     )\n",
    "# tfwriter.process(parse_fn=parse_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa1ca48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b84414",
   "metadata": {},
   "outputs": [],
   "source": [
    "_MAX_SEQ_LEN = 128\n",
    "_MAX_PREDICTIONS_PER_BATCH = 20\n",
    "_VOCAB_SIZE = 32000\n",
    "_MIN_SEN_LEN = 5\n",
    "\n",
    "_START_TOKEN = tokenizer.string_to_id('[CLS]')\n",
    "_END_TOKEN = tokenizer.string_to_id('[SEP]')\n",
    "_MASK_TOKEN = tokenizer.string_to_id('[MASK]')\n",
    "#_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\n",
    "_UNK_TOKEN = tokenizer.string_to_id('<unk>')\n",
    "_PAD_TOKEN = tokenizer.string_to_id('<pad>')\n",
    "\n",
    "\n",
    "_START_TOKEN = 3\n",
    "_END_TOKEN = 4\n",
    "_MASK_TOKEN = 5\n",
    "#_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\n",
    "_UNK_TOKEN = 1\n",
    "_PAD_TOKEN = 0\n",
    "\n",
    "# Truncate inputs to a maximum length.\n",
    "trimmer = tf_text.RoundRobinTrimmer(max_seq_length=_MAX_SEQ_LEN)\n",
    "\n",
    "# Random Selector\n",
    "random_selector = tf_text.RandomItemSelector(\n",
    "    max_selections_per_batch=_MAX_PREDICTIONS_PER_BATCH,\n",
    "    selection_rate=0.2,\n",
    "    unselectable_ids=[_START_TOKEN, _END_TOKEN, _UNK_TOKEN, _PAD_TOKEN]\n",
    ")\n",
    "\n",
    "# Mask Value chooser (Encapsulates the BERT MLM token selection logic)\n",
    "mask_values_chooser = tf_text.MaskValuesChooser(_VOCAB_SIZE, _MASK_TOKEN, 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864daa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_mlm(item):\n",
    "    # Tokenizer (always return Ragged tensor I think)\n",
    "    #segments = tokenizer.tokenize(item).merge_dims(1, -1)\n",
    "    # Trim based on maximum Sequence Length (list is important)\n",
    "    \n",
    "    segments = item['input_ids']\n",
    "    trimmed_segments = trimmer.trim([segments])\n",
    "    \n",
    "    # We replace trimmer with slice [:_MAX_SEQ_LEN-2] operation # 2 to add CLS and SEP\n",
    "    # input_ids = item['input_ids'][:_MAX_SEQ_LEN-2]\n",
    "    \n",
    "    # Combine segments, get segment ids and add special tokens.\n",
    "    segments_combined, segment_ids = tf_text.combine_segments(\n",
    "          trimmed_segments,\n",
    "          start_of_sequence_id=_START_TOKEN,\n",
    "          end_of_segment_id=_END_TOKEN)\n",
    "    \n",
    "    # We replace segment with concat\n",
    "    # input_ids = tf.concat([[_START_TOKEN], input_ids, [_END_TOKEN]], axis=0)\n",
    "\n",
    "    # Apply dynamic masking\n",
    "    masked_token_ids, masked_pos, masked_lm_ids = tf_text.mask_language_model(\n",
    "      segments_combined,\n",
    "      item_selector=random_selector,\n",
    "      mask_values_chooser=mask_values_chooser)\n",
    "\n",
    "    # Prepare and pad combined segment inputs\n",
    "    input_word_ids, input_mask = tf_text.pad_model_inputs(\n",
    "        masked_token_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "    input_type_ids, _ = tf_text.pad_model_inputs(\n",
    "        segment_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "\n",
    "    # Prepare and pad masking task inputs\n",
    "    # Masked lm weights will mask the weights\n",
    "    masked_lm_positions, masked_lm_weights = tf_text.pad_model_inputs(\n",
    "      masked_pos, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "    masked_lm_ids, _ = tf_text.pad_model_inputs(\n",
    "      masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "    \n",
    "\n",
    "#     model_inputs = {\n",
    "#       \"input_ids\": input_word_ids,\n",
    "#       \"input_mask\": input_mask,\n",
    "#       \"input_type_ids\": input_type_ids,\n",
    "#       \"masked_lm_ids\": masked_lm_ids,\n",
    "#       \"masked_lm_positions\": masked_lm_positions,\n",
    "#       \"masked_lm_weights\": masked_lm_weights,\n",
    "#     }\n",
    "    \n",
    "    inputs = {}\n",
    "    inputs['input_ids'] = input_word_ids\n",
    "    inputs['input_type_ids'] = input_type_ids\n",
    "    inputs['input_mask'] = input_mask\n",
    "    inputs['masked_lm_positions'] = masked_lm_positions\n",
    "    \n",
    "    labels = {}\n",
    "    labels['masked_lm_labels'] = masked_lm_ids\n",
    "    labels['masked_lm_weights']   = masked_lm_weights # Mask\n",
    "    \n",
    "    return (inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c899ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset and check timings\n",
    "\n",
    "import json\n",
    "import glob\n",
    "\n",
    "tfrecord_train_dir = '/home/sidhu/Datasets/PRETRAIN_DATA/TFRECORD'\n",
    "schema = json.load(open(\"{}/schema.json\".format(tfrecord_train_dir)))\n",
    "all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_train_dir))\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "x_keys = ['input_ids']\n",
    "\n",
    "MAX_LEN = 128\n",
    "batch_size = 1024\n",
    "# padded_shapes = {'input_ids': [MAX_LEN], \n",
    "#                  'labels': [MAX_LEN], \n",
    "#                  'labels_mask': [MAX_LEN]}\n",
    "train_dataset = tf_reader.read_record(auto_batch=False, \n",
    "                                   keys=x_keys,\n",
    "                                   batch_size=batch_size, \n",
    "                                   x_keys = x_keys, \n",
    "                                   shuffle=True,\n",
    "                                   drop_remainder=True\n",
    "                                  )\n",
    "\n",
    "\n",
    "\n",
    "def filter_by_length(x):\n",
    "    return tf.squeeze(tf.greater_equal(tf.shape(x['input_ids']) ,tf.constant(_MIN_SEN_LEN)), axis=0)\n",
    "train_dataset = train_dataset.filter(filter_by_length)\n",
    "train_dataset = train_dataset.apply(\n",
    "    tf.data.experimental.dense_to_ragged_batch(batch_size=batch_size))\n",
    "train_dataset = train_dataset.map(map_mlm)\n",
    "\n",
    "for (batch_inputs, batch_labels) in tqdm.tqdm(train_dataset):\n",
    "    print(batch_inputs, batch_labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc0531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad93e83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c461fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Check with HF tokenizer\n",
    "\n",
    "from transformers import AlbertTokenizer\n",
    "tokenizer_hf = AlbertTokenizer(vocab_file='/home/sidhu/Datasets/PRETRAIN_DATA/vocab/bert-joint.model')\n",
    "\n",
    "file_paths = ['/home/sidhu/Datasets/wikipedia.txt']\n",
    "dataset = tf.data.TextLineDataset(file_paths)\n",
    "\n",
    "\n",
    "def filter_empty_string(line):\n",
    "    return tf.not_equal(tf.strings.length(line),0)\n",
    "\n",
    "def normalize_string(line):\n",
    "    return tf.strings.strip(line)\n",
    "\n",
    "def hf_tokenize(item):\n",
    "    item = item.numpy().decode()\n",
    "    input_ids = tokenizer_hf(item, add_special_tokens=False)['input_ids']\n",
    "    return [input_ids]\n",
    "\n",
    "def map_mlm_hf(item):\n",
    "    input_ids = tf.py_function(hf_tokenize, [item],\n",
    "                   tf.int32)\n",
    "    # MAX_SEQ_LEN - 2 # 2 for CLS and SEP\n",
    "    input_ids = input_ids[:_MAX_SEQ_LEN-2]\n",
    "    input_ids = tf.concat([[_START_TOKEN], input_ids, [_END_TOKEN]], axis=0)\n",
    "    segments_combined = tf.RaggedTensor.from_tensor([input_ids])\n",
    "    segment_ids = tf.zeros_like(segments_combined)\n",
    "    # Apply dynamic masking\n",
    "    masked_token_ids, masked_pos, masked_lm_ids = tf_text.mask_language_model(\n",
    "      segments_combined,\n",
    "      item_selector=random_selector,\n",
    "      mask_values_chooser=mask_values_chooser)\n",
    "    \n",
    "    masked_token_ids = tf.squeeze(masked_token_ids.to_tensor(), axis=0)\n",
    "    masked_pos = tf.squeeze(masked_pos.to_tensor(), axis=0)\n",
    "    masked_lm_ids = tf.squeeze(masked_lm_ids.to_tensor(), axis=0)\n",
    "    # Prepare and pad combined segment inputs\n",
    "    # input_word_ids, input_mask = tf_text.pad_model_inputs(\n",
    "        # masked_token_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "    # input_type_ids, _ = tf_text.pad_model_inputs(\n",
    "        # segment_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "\n",
    "    # Prepare and pad masking task inputs\n",
    "    # Masked lm weights will mask the weights\n",
    "#     masked_lm_positions, masked_lm_weights = tf_text.pad_model_inputs(\n",
    "#       masked_pos, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "#     masked_lm_ids, _ = tf_text.pad_model_inputs(\n",
    "#       masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "    \n",
    "\n",
    "    model_inputs = {\n",
    "      \"input_ids\": masked_token_ids,\n",
    "      \"input_mask\": tf.ones_like(masked_token_ids),\n",
    "      \"input_type_ids\": tf.zeros_like(masked_token_ids),\n",
    "      \"masked_lm_labels\": masked_lm_ids,\n",
    "      \"masked_lm_positions\": masked_pos,\n",
    "      \"masked_lm_weights\": tf.ones_like(masked_pos),\n",
    "    }\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "dataset = dataset.map(normalize_string)\n",
    "dataset = dataset.filter(filter_empty_string)\n",
    "\n",
    "train_dataset = dataset.map(map_mlm_hf)    \n",
    "batch_size = 1024\n",
    "padded_shapes = {\n",
    "      \"input_ids\": [_MAX_SEQ_LEN],\n",
    "      \"input_mask\": [_MAX_SEQ_LEN],\n",
    "      \"input_type_ids\": [_MAX_SEQ_LEN],\n",
    "      \"masked_lm_labels\": [_MAX_PREDICTIONS_PER_BATCH],\n",
    "      \"masked_lm_positions\": [_MAX_PREDICTIONS_PER_BATCH],\n",
    "      \"masked_lm_weights\": [_MAX_PREDICTIONS_PER_BATCH],\n",
    "    }\n",
    "train_dataset = auto_batch(\n",
    "    train_dataset,\n",
    "    batch_size,\n",
    "    padded_values=None,\n",
    "    padded_shapes=padded_shapes,\n",
    "    x_keys=['input_ids', 'input_type_ids', 'input_mask', 'masked_lm_positions'],\n",
    "    y_keys=['masked_lm_labels', 'masked_lm_weights'],\n",
    "    shuffle=False,\n",
    "    drop_remainder=False,\n",
    "    shuffle_buffer_size=100,\n",
    "    prefetch_buffer_size=100,\n",
    ")\n",
    "\n",
    "for (batch_inputs, batch_labels) in train_dataset:\n",
    "    print(batch_inputs, batch_labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8af9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740da71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Build a dummy dataset and see whether it works\n",
    "\n",
    "input_ids = tf.random.uniform(minval=0, maxval=32000, shape=(4000, 128), dtype=tf.int32)\n",
    "masked_lm_positions = tf.random.uniform(minval=0, maxval=128, shape=(4000, 20), dtype=tf.int32)\n",
    "masked_lm_labels = tf.random.uniform(minval=0, maxval=32000, shape=(4000, 20), dtype=tf.int32)\n",
    "masked_lm_weights = tf.random.uniform(minval=0, maxval=2, shape=(4000, 20), dtype=tf.int32)\n",
    "dummy_batch_inputs = {\"input_ids\": input_ids, \n",
    "                      \"input_mask\": tf.ones_like(input_ids), \n",
    "                      \"input_type_ids\": tf.zeros_like(input_ids), \n",
    "                      \"masked_lm_positions\": masked_lm_positions}\n",
    "\n",
    "dummy_batch_labels = {\"masked_lm_labels\": masked_lm_labels, \n",
    "                     \"masked_lm_weights\": masked_lm_weights}\n",
    "\n",
    "\n",
    "batch_size = 1024\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dummy_batch_inputs, dummy_batch_labels)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacba48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d757912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Modeling\n",
    "\n",
    "from tf_transformers.losses import cross_entropy_loss\n",
    "from tf_transformers.optimization import create_optimizer\n",
    "from tf_transformers.core import Trainer, TPUTrainer\n",
    "\n",
    "def get_model():\n",
    "    config = {\n",
    "        \"attention_probs_dropout_prob\": 0.1,\n",
    "        \"hidden_act\": \"gelu\",\n",
    "        \"intermediate_act\": \"gelu\",\n",
    "        \"hidden_dropout_prob\": 0.1,\n",
    "        \"embedding_size\": 768,\n",
    "        \"initializer_range\": 0.02,\n",
    "        \"intermediate_size\": 3072,\n",
    "        \"max_position_embeddings\": 512,\n",
    "        \"num_attention_heads\": 12,\n",
    "        \"attention_head_size\": 64,\n",
    "        \"num_hidden_layers\": 12,\n",
    "        \"type_vocab_size\": 2,\n",
    "        \"vocab_size\": 32000,\n",
    "        \"layer_norm_epsilon\": 1e-12,\n",
    "        \"mask_mode\": \"user_defined\",\n",
    "    }\n",
    "    \n",
    "    from tf_transformers.models import BertModel\n",
    "    model = BertModel.from_config(config,\n",
    "                                  batch_size=batch_size,\n",
    "                                 use_masked_lm_positions=True, # Add batch_size to avoid dynamic shapes\n",
    "                                return_all_layer_outputs=True) \n",
    "    return model\n",
    "def get_optimizer():\n",
    "    init_lr = 2e-05\n",
    "    optimizer, learning_rate_fn = create_optimizer(init_lr=init_lr, \n",
    "                                                 num_train_steps=100000,\n",
    "                                                 num_warmup_steps=10000)\n",
    "    return optimizer\n",
    "\n",
    "def lm_loss(y_true_dict, y_pred_dict):\n",
    "    \n",
    "    loss_dict = {}\n",
    "    loss_holder = []\n",
    "    for layer_count, per_layer_output in enumerate(y_pred_dict['all_layer_token_logits']):\n",
    "        \n",
    "        loss = cross_entropy_loss(labels=y_true_dict['masked_lm_labels'], \n",
    "                                logits=per_layer_output, \n",
    "                                label_weights=y_true_dict['masked_lm_weights'])\n",
    "        loss_dict['loss_{}'.format(layer_count+1)] = loss\n",
    "        loss_holder.append(loss)\n",
    "    loss_dict['loss'] = tf.reduce_mean(loss_holder, axis=0)\n",
    "    return loss_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tpu_address = 'local'\n",
    "trainer =  TPUTrainer(\n",
    "    tpu_address=tpu_address,\n",
    "    dtype='bf16'\n",
    ")\n",
    "\n",
    "GLOBAL_BATCH_SIZE = batch_size\n",
    "\n",
    "training_loss_names = ['loss_1',\n",
    " 'loss_2',\n",
    " 'loss_3',\n",
    " 'loss_4',\n",
    " 'loss_5',\n",
    " 'loss_6',\n",
    " 'loss_7',\n",
    " 'loss_8',\n",
    " 'loss_9',\n",
    " 'loss_10',\n",
    " 'loss_11',\n",
    " 'loss_12']\n",
    "\n",
    "trainer.run(\n",
    "    model_fn=get_model,\n",
    "    optimizer_fn=get_optimizer,\n",
    "    train_dataset=train_dataset,\n",
    "    train_loss_fn=lm_loss,\n",
    "    epochs=2,\n",
    "    steps_per_epoch=1000,\n",
    "    model_checkpoint_dir=\"temp_dir\", # gs://tft_free/\n",
    "    batch_size=GLOBAL_BATCH_SIZE,\n",
    "    training_loss_names=training_loss_names,\n",
    "    validation_loss_names=None,\n",
    "    validation_dataset=None,\n",
    "    validation_loss_fn=None,\n",
    "    validation_interval_steps=None,\n",
    "    steps_per_call=100,\n",
    "    enable_xla=False,\n",
    "    callbacks=None,\n",
    "    callbacks_interval_steps=None,\n",
    "    overwrite_checkpoint_dir=True,\n",
    "    max_number_of_models=10,\n",
    "    model_save_interval_steps=None,\n",
    "    repeat_dataset=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
