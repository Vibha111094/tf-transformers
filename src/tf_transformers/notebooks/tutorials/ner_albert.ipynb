{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "ner_albert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTy_I5RsT3to"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE1LHLVeU3qR"
      },
      "source": [
        "# Install tf-transformers\n",
        "\n",
        "pip install tf-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc2fkTiwT3tw"
      },
      "source": [
        "### SNIPS NLU\n",
        "\n",
        "#### credit -> https://colab.research.google.com/drive/1wgWdxUpKf3FWJgqA6ogBGDEzxAosjJMI\n",
        "\n",
        "Snips NLU consists of 2 tasks (Slot Filling and Classification)\n",
        "\n",
        "Slot filling can be formulated as NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfRs3DlUUGHC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaL8m4yyUDYB"
      },
      "source": [
        "# NER + Albert base using Fast Sentence Piece Alignment + Joint loss + TFlite Conversion\n",
        "\n",
        "This tutorial contains code to fine-tune an Albert Model for NER (Token Classification).\n",
        "\n",
        "In this notebook:\n",
        "- Load the data + create ```tf.data.Dataset``` using TFProcesor\n",
        "- Load Albert Model V2 and use it to create a Token Classification Model\n",
        "- Train using ```tf.keras.Model.fit``` and ```Custom Trainer``` \n",
        "- Minimze loss per layer to find optimal layer\n",
        "- Evaluate EXACT MATCH per layer\n",
        "- Convert TFlite\n",
        "- In production using faster ```tf.SavedModel``` + no architecture code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-dOtkaT3tx"
      },
      "source": [
        "### Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lX5zHRPT3tx"
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "SNIPS_DATA_BASE_URL = (\n",
        "    \"https://github.com/ogrisel/slot_filling_and_intent_detection_of_SLU/blob/\"\n",
        "    \"master/data/snips/\"\n",
        ")\n",
        "for filename in [\"train\", \"valid\", \"test\", \"vocab.intent\", \"vocab.slot\"]:\n",
        "    path = Path(filename)\n",
        "    if not path.exists():\n",
        "        print(f\"Downloading {filename}...\")\n",
        "        urlretrieve(SNIPS_DATA_BASE_URL + filename + \"?raw=true\", path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX9J6JTNT3ty"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MIQngfWT3ty"
      },
      "source": [
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import glob\n",
        "\n",
        "from tf_transformers.utils.tokenization import BasicTokenizer, SPIECE_UNDERLINE # Special Piece for Albert\n",
        "from tf_transformers.data.ner_utils_sp import fast_tokenize_and_align_sentence_for_ner\n",
        "\n",
        "from tf_transformers.data import TFWriter, TFReader, TFProcessor\n",
        "from tf_transformers.models import AlbertModel\n",
        "from tf_transformers.tasks import Token_Classification_Model\n",
        "from tf_transformers.core import optimization, SimpleTrainer\n",
        "from tf_transformers.losses import cross_entropy_loss_fast\n",
        "\n",
        "from transformers import AlbertTokenizer\n",
        "from absl import logging\n",
        "logging.set_verbosity(\"INFO\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkkocYTzWIW5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnULAzdZT3ty"
      },
      "source": [
        "# Read SNIPS data to dataframe\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def parse_line(line):\n",
        "    utterance_data, intent_label = line.split(\" <=> \")\n",
        "    items = utterance_data.split()\n",
        "    words = [item.rsplit(\":\", 1)[0] for item in items]\n",
        "    word_labels = [item.rsplit(\":\", 1)[1] for item in items]\n",
        "    return {\n",
        "        \"intent_label\": intent_label,\n",
        "        \"words\": \" \".join(words),\n",
        "        \"word_labels\": \" \".join(word_labels),\n",
        "        \"length\": len(words),\n",
        "    }\n",
        "\n",
        "lines_train = Path(\"train\").read_text().strip().splitlines()\n",
        "lines_valid = Path(\"valid\").read_text().strip().splitlines()\n",
        "lines_test  = Path(\"test\").read_text().strip().splitlines()\n",
        "\n",
        "df_train = pd.DataFrame([parse_line(line) for line in lines_train])\n",
        "df_valid = pd.DataFrame([parse_line(line) for line in lines_valid])\n",
        "df_test  = pd.DataFrame([parse_line(line) for line in lines_test])\n",
        "\n",
        "# Slot labels\n",
        "slot_names = [\"[PAD]\"]\n",
        "slot_names += Path(\"vocab.slot\").read_text().strip().splitlines()\n",
        "slot_map = {}\n",
        "for label in slot_names:\n",
        "    slot_map[label] = len(slot_map)\n",
        "    \n",
        "# id ->  labels\n",
        "slot_map_reverse = {v:k for k,v in slot_map.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaLF0TpwT3tz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-mcNcefWtxi"
      },
      "source": [
        "### Load Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huEnTGgbT3tz"
      },
      "source": [
        "# Load HuggingFace Tokenizer\n",
        "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSTmXIx3Wz77"
      },
      "source": [
        "## Parse train data \n",
        "\n",
        "We will use a simple ```generator``` to iterate over the data and use ```TFProcessor``` to convert it to ```tf.data.Dataset``` in 2 lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEBK5zC9T3t1"
      },
      "source": [
        "is_training = True\n",
        "\n",
        "# Convert train examples to properly aligned examples\n",
        "ignored_examples_index = []\n",
        "\n",
        "all_flat_tokens = []\n",
        "all_flat_labels = []\n",
        "for index, row in df_train.iterrows():\n",
        "    sentence = row[\"words\"]\n",
        "    labels = row[\"word_labels\"]\n",
        "    word_tokens = sentence.split()\n",
        "    label_tokens = labels.split()\n",
        "    if len(word_tokens) != len(label_tokens):\n",
        "        ignored_examples_index.append(index)\n",
        "        continue\n",
        "    aligned_words, sub_words_mapped, flat_tokens, flat_labels = fast_tokenize_and_align_sentence_for_ner(\n",
        "        sentence, word_tokens, SPIECE_UNDERLINE, is_training, label_tokens, label_pad_token=\"[PAD]\")\n",
        "    all_flat_tokens.append(flat_tokens)\n",
        "    all_flat_labels.append(flat_labels)\n",
        "    \n",
        "\n",
        "# Convert tokens to id and add type_ids\n",
        "# input_mask etc\n",
        "# This is user specific/ tokenizer specific\n",
        "# Eg: Roberta has input_type_ids = 0, BERT has input_type_ids = [0, 1]\n",
        "\n",
        "label_pad_token=\"[PAD]\"\n",
        "def parse_train():\n",
        "    \"\"\"\n",
        "    convert text to inputs (ids)\n",
        "    \"\"\"\n",
        "    \n",
        "    for i in range(len(all_flat_tokens)):\n",
        "        \n",
        "        flat_tokens = all_flat_tokens[i]\n",
        "        flat_labels = all_flat_labels[i]\n",
        "        # Tokenizer will automatically set [BOS] <text> [EOS]\n",
        "        result = {}\n",
        "        result[\"input_ids\"] = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] +  flat_tokens + [tokenizer.bos_token])\n",
        "        result[\"input_mask\"] = [1] * len(result[\"input_ids\"])\n",
        "        result[\"input_type_ids\"] = [0] * len(result[\"input_ids\"])\n",
        "        labels = [slot_map[token] for token in flat_labels]\n",
        "        labels = [slot_map[label_pad_token]] + labels + [slot_map[label_pad_token]]  # for [CLS] and [SEP]\n",
        "        \n",
        "        # We dont want label_pad_token (which is mostly because of multiple subowrds)\n",
        "        label_mask = []\n",
        "        for token in flat_labels:\n",
        "            if token == [label_pad_token]:\n",
        "                label_mask.append(0)\n",
        "                continue\n",
        "            label_mask.append(1)\n",
        "        label_mask = [0] + label_mask + [0]  # for [CLS] and [SEP]\n",
        "        \n",
        "        result[\"labels\"] = labels\n",
        "        result[\"label_mask\"] = label_mask\n",
        "        yield result\n",
        "\n",
        "\n",
        "# use TFProcessor only if your data is in range of 10k - 20k maximum\n",
        "# otherwise use TFWriter\n",
        "# This will create a (batch_inputs, batch_labels) tuple dataset\n",
        "batch_size = 32\n",
        "tf_processor = TFProcessor()\n",
        "train_dataset = tf_processor.process(parse_fn=parse_train())\n",
        "x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
        "y_keys = ['labels', 'label_mask']\n",
        "train_dataset = tf_processor.auto_batch(train_dataset, batch_size=batch_size,\n",
        "                                        x_keys = x_keys,\n",
        "                                        y_keys = y_keys,\n",
        "                                        shuffle=True, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH_LJ-pGT3t2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJTxyR85XI5o"
      },
      "source": [
        "### Load Albert V2 Model \n",
        "\n",
        "We will use Albert Model and load the checkpoints.\n",
        "To convert Huggingface models to checkpoints, refer ```conversion``` notebooks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6VhTeDgT3t2",
        "outputId": "a336eaff-374d-4726-a884-8650b0ba6f2f"
      },
      "source": [
        "# Lets load Albert Model\n",
        "tf.keras.backend.clear_session()\n",
        "model_layer, model, config = AlbertModel(model_name='albert_base_v2', \n",
        "                   is_training=True, \n",
        "                   use_dropout=False\n",
        "                   )\n",
        "model.load_checkpoint(\"/mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/albert-base-v2/\")\n",
        "\n",
        "# model_layer -> Legacylayer inherited from tf.keras.Layer\n",
        "# model -> legacyModel inherited from tf.keras.Model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Initialized Variables\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_4I9uuwYRcF"
      },
      "source": [
        "### Load Token Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDugadEAT3t4"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model_ner = Token_Classification_Model(model=model,\n",
        "                                      token_vocab_size=len(slot_map), # Vocab Size\n",
        "                                      use_all_layers=True, \n",
        "                                      is_training=True)\n",
        "model_ner = model_ner.get_model()\n",
        "\n",
        "# Delete models to save memory\n",
        "\n",
        "del model\n",
        "del model_layer\n",
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqPIxjMwT3t4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhazY1sBYxdB"
      },
      "source": [
        "### Define Loss\n",
        "\n",
        "Loss function is simple, softmax over NER token vocab ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQzwBqj7T3t5"
      },
      "source": [
        "def token_loss(y_true_dict, token_logits):\n",
        "    loss = cross_entropy_loss_fast(\n",
        "        labels=y_true_dict[\"labels\"],\n",
        "        logits=token_logits,\n",
        "        label_weights=y_true_dict[\"label_mask\"],\n",
        "    )\n",
        "    return loss\n",
        "\n",
        "def token_loss_all_layers(y_true_dict, y_pred_dict):\n",
        "    layer_loss = []\n",
        "    for token_logits in y_pred_dict['token_logits']:\n",
        "        loss = token_loss(y_true_dict, token_logits)\n",
        "        layer_loss.append(loss)\n",
        "    return tf.reduce_mean(layer_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duxA4U8IY665"
      },
      "source": [
        "### Define Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6hpWdQPT3t5"
      },
      "source": [
        "train_data_size = 13000\n",
        "learning_rate   = 2e-5\n",
        "steps_per_epoch = int(train_data_size / batch_size)\n",
        "EPOCHS = 3\n",
        "num_train_steps = steps_per_epoch * EPOCHS\n",
        "warmup_steps = int(0.1 * num_train_steps)\n",
        "# creates an optimizer with learning rate schedule\n",
        "optimizer_type = 'adamw'\n",
        "optimizer, learning_rate_fn = optimization.create_optimizer(learning_rate,\n",
        "                                                steps_per_epoch * EPOCHS,\n",
        "                                                warmup_steps,\n",
        "                                                optimizer_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnTnKmQfc-j0"
      },
      "source": [
        "### Train Using Keras :-)\n",
        "\n",
        "- ```compile2``` allows you to have directly use model outputs as well batch dataset outputs into the loss function, without any further complexity.\n",
        "\n",
        "Note: For ```compile2```, loss_fn must be None, and custom_loss_fn must be active. Metrics are not supprted for time being."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_pF-RJuT3t6"
      },
      "source": [
        "# # Compile\n",
        "loss_fn = {'token_logits': token_loss_all_layers}\n",
        "model_ner.compile2(optimizer=optimizer, \n",
        "                             loss=None, \n",
        "                             custom_loss=loss_fn)\n",
        "\n",
        "history = model_ner.fit(train_dataset, epochs=2, steps_per_epoch=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnW1Cc6SdDTL"
      },
      "source": [
        "### Train using SimpleTrainer (part of tf-transformers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVkGlR2wT3t6"
      },
      "source": [
        "history = SimpleTrainer(model = model_ner,\n",
        "             optimizer = optimizer,\n",
        "             loss_fn = token_loss_all_layers,\n",
        "             dataset = train_dataset.repeat(EPOCHS+1), # This is important\n",
        "             epochs = EPOCHS, \n",
        "             num_train_examples = train_data_size, \n",
        "             batch_size = batch_size, \n",
        "             steps_per_call=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C1BSIc4T3t6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrIxd04dV1a"
      },
      "source": [
        "### Save Models \n",
        "\n",
        "You can save models as checkpoints using ```.save_checkpoint``` attribute, which is a part of all ```LegacyModels```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYuclU5AT3t6"
      },
      "source": [
        "model_save_dir = \"../OFFICIAL_MODELS/snips/albert_base\"\n",
        "model_ner.save_checkpoint(model_save_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdmwBfDgeGF6"
      },
      "source": [
        "### Parse validation data\n",
        "\n",
        "Like before we use ```TFProcessor``` to create datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UruCrzwRT3t7"
      },
      "source": [
        "\n",
        "\n",
        "# Convert train examples to properly aligned examples\n",
        "ignored_examples_index = []\n",
        "\n",
        "dev_flat_tokens = []\n",
        "dev_flat_labels = []\n",
        "for index, row in df_valid.iterrows():\n",
        "    sentence = row[\"words\"]\n",
        "    labels = row[\"word_labels\"]\n",
        "    word_tokens = sentence.split()\n",
        "    label_tokens = labels.split()\n",
        "    if len(word_tokens) != len(label_tokens):\n",
        "        ignored_examples_index.append(index)\n",
        "        continue\n",
        "    aligned_words, sub_words_mapped, flat_tokens, flat_labels = fast_tokenize_and_align_sentence_for_ner(\n",
        "        sentence, word_tokens, SPIECE_UNDERLINE, is_training, label_tokens, label_pad_token=\"[PAD]\")\n",
        "    dev_flat_tokens.append(flat_tokens)\n",
        "    dev_flat_labels.append(flat_labels)\n",
        "    \n",
        "\n",
        "# Convert tokens to id and add type_ids\n",
        "# input_mask etc\n",
        "# This is user specific/ tokenizer specific\n",
        "# Eg: Roberta has input_type_ids = 0, BERT has input_type_ids = [0, 1]\n",
        "\n",
        "label_pad_token=\"[PAD]\"\n",
        "def parse_dev():\n",
        "    \"\"\"\n",
        "    convert text to inputs (ids)\n",
        "    \"\"\"\n",
        "    \n",
        "    for i in range(len(dev_flat_tokens)):\n",
        "        \n",
        "        flat_tokens = dev_flat_tokens[i]\n",
        "        flat_labels = dev_flat_labels[i]\n",
        "        # Tokenizer will automatically set [BOS] <text> [EOS]\n",
        "        result = {}\n",
        "        result[\"input_ids\"] = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] +  flat_tokens + [tokenizer.bos_token])\n",
        "        result[\"input_mask\"] = [1] * len(result[\"input_ids\"])\n",
        "        result[\"input_type_ids\"] = [0] * len(result[\"input_ids\"])\n",
        "        labels = [slot_map[token] for token in flat_labels]\n",
        "        labels = [slot_map[label_pad_token]] + labels + [slot_map[label_pad_token]]  # for [CLS] and [SEP]\n",
        "        \n",
        "        # We dont want label_pad_token (which is mostly because of multiple subowrds)\n",
        "        label_mask = []\n",
        "        for token in flat_labels:\n",
        "            if token == [label_pad_token]:\n",
        "                label_mask.append(0)\n",
        "                continue\n",
        "            label_mask.append(1)\n",
        "        label_mask = [0] + label_mask + [0]  # for [CLS] and [SEP]\n",
        "        result[\"labels\"] = labels\n",
        "        result[\"label_mask\"] = label_mask\n",
        "        yield result\n",
        "\n",
        "batch_size = 32\n",
        "tf_processor = TFProcessor()\n",
        "dev_dataset = tf_processor.process(parse_fn=parse_dev())\n",
        "x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
        "y_keys = ['labels', 'label_mask']\n",
        "dev_dataset = tf_processor.auto_batch(dev_dataset, batch_size=batch_size,\n",
        "                                        x_keys = x_keys,\n",
        "                                        y_keys = y_keys,\n",
        "                                        shuffle=False, drop_remainder=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP80kTL6ennW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bhQRVI1etcZ"
      },
      "source": [
        "### EXACT MATCH based evaluation\n",
        "Lets see our idea of jointly minimze loss will bring\n",
        "some benefits or not. If so, that will reduce the overall latency :-)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAHMmBg7T3t7"
      },
      "source": [
        "# Exact Match Evaluation .\n",
        "\n",
        "num_layers = 12\n",
        "prediction_per_layer = {i:[] for i in range(num_layers)}\n",
        "original_labels = []\n",
        "for (batch_inputs, batch_labels) in dev_dataset:\n",
        "    results = model_ner(batch_inputs)\n",
        "    model_logits = results['token_logits'][-1]\n",
        "    \n",
        "    for i, model_logits in enumerate(results['token_logits']):\n",
        "    \n",
        "        # Iterate over each example\n",
        "        for index, per_example_logits in enumerate(model_logits):\n",
        "            per_example_length = tf.reduce_sum(batch_inputs['input_mask'][index])\n",
        "            per_example_label  = batch_labels['labels'][index][:per_example_length][1:-1] # we dont want pad positions and 1:-1 is to remove CLS and SEP\n",
        "            per_example_logits = per_example_logits[:per_example_length][1:-1] # 1:-1 CLS and SEP\n",
        "            per_example_preds  = tf.argmax(per_example_logits, axis=-1)\n",
        "            prediction_per_layer[i].append(per_example_preds)\n",
        "            \n",
        "            # We want the original label only once\n",
        "            if i == 0:\n",
        "                original_labels.append(per_example_label)\n",
        "    \n",
        "    \n",
        "# We have 700 examples\n",
        "for layer_iter in range(num_layers):\n",
        "    result = prediction_per_layer[layer_iter]\n",
        "    \n",
        "    pred_list = []\n",
        "    for i in range(700):\n",
        "        pred = list(result[i].numpy())\n",
        "        orig = list(original_labels[i].numpy())\n",
        "        if pred == orig:\n",
        "            pred_list.append(1)\n",
        "        else:\n",
        "            pred_list.append(0)\n",
        "    print(\"Layer {} exact match {} / 700\".format(layer_iter, sum(pred_list)))\n",
        "    \n",
        "    \n",
        "# Layer 0 exact match 223 / 700\n",
        "# Layer 1 exact match 428 / 700\n",
        "# Layer 2 exact match 511 / 700\n",
        "# Layer 3 exact match 558 / 700\n",
        "# Layer 4 exact match 571 / 700\n",
        "# Layer 5 exact match 576 / 700\n",
        "# Layer 6 exact match 587 / 700\n",
        "# Layer 7 exact match 594 / 700\n",
        "# Layer 8 exact match 592 / 700\n",
        "# Layer 9 exact match 591 / 700\n",
        "# Layer 10 exact match 596 / 700\n",
        "# Layer 11 exact match 586 / 700"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocejDeOjioJF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KswX5gokiqF7"
      },
      "source": [
        "### Save as Serialized version \n",
        "\n",
        "- Now we can use ```save_as_serialize_module``` to save a model directly to saved_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0lNb4TRioRL"
      },
      "source": [
        "model_ner.save_as_serialize_module(\"{}/saved_model\".format(model_save_dir))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnlgeVSYe3Ux"
      },
      "source": [
        "### TFLite Conversion\n",
        "\n",
        "TFlite conversion requires:\n",
        "- static batch size\n",
        "- static sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx1Vxp9fT3t7"
      },
      "source": [
        "# So, layer 5 itself is giving 576/700 \n",
        "# Thats great. Lets finalize the model with 5 hidden layers (instead of 12)\n",
        "\n",
        "model_layer, model, config = AlbertModel(model_name='albert-base-v2', \n",
        "                                     batch_size=1, \n",
        "                                     sequence_length=45,  # 45 is enough for SNIP \n",
        "                                     num_hidden_layers=5, \n",
        "                                     is_training=False\n",
        "                                     )\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "model_ner = Token_Classification(model=model,\n",
        "                                      token_vocab_size=len(slot_map),\n",
        "                                      use_all_layers=False, \n",
        "                                      is_training=False)\n",
        "model_ner = model_ner.get_model()\n",
        "model_ner.load_checkpoint(model_save_dir)\n",
        "\n",
        "# Save to .pb format , we need it for tflite\n",
        "\n",
        "model_ner.save_as_serialize_module(\"{}/saved_model_for_tflite\".format(model_save_dir))\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"{}/saved_model_for_tflite\".format(model_save_dir)) # path to the SavedModel directory\n",
        "converter.experimental_new_converter = True\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "open(\"{}/converted_model.tflite\".format(model_save_dir), \"wb\").write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTXqsVR4iSSn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nhP-DYdh0XR"
      },
      "source": [
        "### **In production**\n",
        "\n",
        "- We can use either ```tf.keras.Model``` or ```saved_model```. I recommend saved_model, which is much much faster and no hassle of having architecture code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF53k89jT3t8"
      },
      "source": [
        "def tokenizer_fn(feature):\n",
        "    result = {}\n",
        "    result[\"input_ids\"] = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] +  feature['input_ids'] + [tokenizer.bos_token])\n",
        "    result[\"input_mask\"] = [1] * len(result[\"input_ids\"])\n",
        "    result[\"input_type_ids\"] = [0] * len(result[\"input_ids\"])\n",
        "    return result\n",
        "\n",
        "# load serialized model\n",
        "model_ner = tf.saved_model.load(\"{}/saved_model\".format(model_save_dir))\n",
        "slot_map_reverse = {v:k for k,v in slot_map.items()}\n",
        "pipeline = Token_Classification_Pipeline( model = model_ner, \n",
        "                tokenizer = tokenizer, \n",
        "                tokenizer_fn = tokenizer_fn, \n",
        "                SPECIAL_PIECE = SPIECE_UNDERLINE,\n",
        "                label_map = slot_map_reverse,\n",
        "                max_seq_length = 128,\n",
        "                batch_size=32)\n",
        "\n",
        "sentences = ['I would love to listen to Carnatic music by Yesudas', \n",
        "            'Play Carnatic Fusion by Various Artists', \n",
        "            'Please book 2 tickets from Bangalore to Kerala']\n",
        "result = pipeline(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvHetDqRhlVT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzdvcZyBhnHb"
      },
      "source": [
        "### Sanity Check for TFlite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORoD0UXET3t8"
      },
      "source": [
        "# Check same model with tflite\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=\"{}/converted_model.tflite\".format(model_save_dir))\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "tflite_seq_length = 45\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "sample_inputs = {}\n",
        "sample_inputs['input_ids'] = tf.random.uniform(minval=0, maxval=100, \n",
        "                                                                    shape=(1, tflite_seq_length), dtype=tf.int32)\n",
        "sample_inputs['input_type_ids'] = tf.zeros_like(sample_inputs['input_ids'])\n",
        "sample_inputs['input_mask'] = tf.ones_like(sample_inputs['input_ids'])\n",
        "\n",
        "interpreter.set_tensor(input_details[0]['index'], sample_inputs['input_ids'])\n",
        "\n",
        "interpreter.set_tensor(input_details[1]['index'],  sample_inputs['input_mask'])\n",
        "\n",
        "interpreter.set_tensor(input_details[2]['index'], sample_inputs['input_type_ids'])\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# The function `get_tensor()` returns a copy of the tensor data.\n",
        "# Use `tensor()` in order to get a pointer to the tensor.\n",
        "tflite_output = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "model_output = model_ner(sample_inputs)\n",
        "\n",
        "# Both matches :-)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
