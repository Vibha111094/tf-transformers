{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNIPS NLU\n",
    "\n",
    "#### credit -> https://colab.research.google.com/drive/1wgWdxUpKf3FWJgqA6ogBGDEzxAosjJMI\n",
    "\n",
    "Snips NLU consists of 2 tasks (Slot Filling and Classification)\n",
    "\n",
    "Slot filling can be formulated as NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "SNIPS_DATA_BASE_URL = (\n",
    "    \"https://github.com/ogrisel/slot_filling_and_intent_detection_of_SLU/blob/\"\n",
    "    \"master/data/snips/\"\n",
    ")\n",
    "for filename in [\"train\", \"valid\", \"test\", \"vocab.intent\", \"vocab.slot\"]:\n",
    "    path = Path(filename)\n",
    "    if not path.exists():\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        urlretrieve(SNIPS_DATA_BASE_URL + filename + \"?raw=true\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SNIPS data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def parse_line(line):\n",
    "    utterance_data, intent_label = line.split(\" <=> \")\n",
    "    items = utterance_data.split()\n",
    "    words = [item.rsplit(\":\", 1)[0] for item in items]\n",
    "    word_labels = [item.rsplit(\":\", 1)[1] for item in items]\n",
    "    return {\n",
    "        \"intent_label\": intent_label,\n",
    "        \"words\": \" \".join(words),\n",
    "        \"word_labels\": \" \".join(word_labels),\n",
    "        \"length\": len(words),\n",
    "    }\n",
    "\n",
    "\n",
    "lines_train = Path(\"train\").read_text().strip().splitlines()\n",
    "lines_valid = Path(\"valid\").read_text().strip().splitlines()\n",
    "lines_test = Path(\"test\").read_text().strip().splitlines()\n",
    "\n",
    "df_train = pd.DataFrame([parse_line(line) for line in lines_train])\n",
    "df_valid = pd.DataFrame([parse_line(line) for line in lines_valid])\n",
    "df_test = pd.DataFrame([parse_line(line) for line in lines_test])\n",
    "\n",
    "# Slot labels\n",
    "slot_names = [\"[PAD]\"]\n",
    "slot_names += Path(\"vocab.slot\").read_text().strip().splitlines()\n",
    "slot_map = {}\n",
    "for label in slot_names:\n",
    "    slot_map[label] = len(slot_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent_label</th>\n",
       "      <th>words</th>\n",
       "      <th>word_labels</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AddToPlaylist</td>\n",
       "      <td>Add Don and Sherri to my Meditate to Sounds of...</td>\n",
       "      <td>O B-entity_name I-entity_name I-entity_name O ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AddToPlaylist</td>\n",
       "      <td>put United Abominations onto my rare groove pl...</td>\n",
       "      <td>O B-entity_name I-entity_name O B-playlist_own...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AddToPlaylist</td>\n",
       "      <td>add the tune by misato watanabe to the Trapeo ...</td>\n",
       "      <td>O O B-music_item O B-artist I-artist O O B-pla...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AddToPlaylist</td>\n",
       "      <td>add this artist to my this is miguel bosé play...</td>\n",
       "      <td>O O B-music_item O B-playlist_owner B-playlist...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AddToPlaylist</td>\n",
       "      <td>add heresy and the hotel choir to the evening ...</td>\n",
       "      <td>O B-entity_name I-entity_name I-entity_name I-...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13079</th>\n",
       "      <td>SearchScreeningEvent</td>\n",
       "      <td>find a Consolidated Theatres showing The Good ...</td>\n",
       "      <td>O O B-location_name I-location_name O B-movie_...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13080</th>\n",
       "      <td>SearchScreeningEvent</td>\n",
       "      <td>where can i see animated movies in the neighbo...</td>\n",
       "      <td>O O O O B-movie_type I-movie_type B-spatial_re...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13081</th>\n",
       "      <td>SearchScreeningEvent</td>\n",
       "      <td>Showtimes for animated movies in the area .</td>\n",
       "      <td>O O B-movie_type I-movie_type B-spatial_relati...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13082</th>\n",
       "      <td>SearchScreeningEvent</td>\n",
       "      <td>Which animated movies are playing at Megaplex ...</td>\n",
       "      <td>O B-movie_type I-movie_type O O O B-location_n...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13083</th>\n",
       "      <td>SearchScreeningEvent</td>\n",
       "      <td>What movie schedules start at sunset ?</td>\n",
       "      <td>O B-object_type I-object_type O O B-timeRange O</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13084 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               intent_label  \\\n",
       "0             AddToPlaylist   \n",
       "1             AddToPlaylist   \n",
       "2             AddToPlaylist   \n",
       "3             AddToPlaylist   \n",
       "4             AddToPlaylist   \n",
       "...                     ...   \n",
       "13079  SearchScreeningEvent   \n",
       "13080  SearchScreeningEvent   \n",
       "13081  SearchScreeningEvent   \n",
       "13082  SearchScreeningEvent   \n",
       "13083  SearchScreeningEvent   \n",
       "\n",
       "                                                   words  \\\n",
       "0      Add Don and Sherri to my Meditate to Sounds of...   \n",
       "1      put United Abominations onto my rare groove pl...   \n",
       "2      add the tune by misato watanabe to the Trapeo ...   \n",
       "3      add this artist to my this is miguel bosé play...   \n",
       "4      add heresy and the hotel choir to the evening ...   \n",
       "...                                                  ...   \n",
       "13079  find a Consolidated Theatres showing The Good ...   \n",
       "13080  where can i see animated movies in the neighbo...   \n",
       "13081        Showtimes for animated movies in the area .   \n",
       "13082  Which animated movies are playing at Megaplex ...   \n",
       "13083             What movie schedules start at sunset ?   \n",
       "\n",
       "                                             word_labels  length  \n",
       "0      O B-entity_name I-entity_name I-entity_name O ...      12  \n",
       "1      O B-entity_name I-entity_name O B-playlist_own...       8  \n",
       "2      O O B-music_item O B-artist I-artist O O B-pla...      10  \n",
       "3      O O B-music_item O B-playlist_owner B-playlist...      10  \n",
       "4      O B-entity_name I-entity_name I-entity_name I-...      11  \n",
       "...                                                  ...     ...  \n",
       "13079  O O B-location_name I-location_name O B-movie_...      10  \n",
       "13080  O O O O B-movie_type I-movie_type B-spatial_re...       9  \n",
       "13081  O O B-movie_type I-movie_type B-spatial_relati...       8  \n",
       "13082  O B-movie_type I-movie_type O O O B-location_n...      11  \n",
       "13083    O B-object_type I-object_type O O B-timeRange O       7  \n",
       "\n",
       "[13084 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer\n",
    "from tf_transformers.losses import cross_entropy_loss_fast\n",
    "from tf_transformers.core import optimization\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "SPECIAL_PIECE = \"▁\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_tokens_labels(\n",
    "    aligned_words, orig_to_new_index, label_tokens, sub_words_mapped, label_pad_token=\"[PAD]\"\n",
    "):\n",
    "    \"\"\"\n",
    "    convert each sub word into labels\n",
    "    If a word is split into multiple sub words, \n",
    "    then first sub word is assigned with label and other sub words will be padded \n",
    "    \"\"\"\n",
    "    aligned_labels = [label_pad_token] * len(aligned_words)\n",
    "    for original_pos, new_pos in enumerate(orig_to_new_index):\n",
    "        aligned_labels[new_pos] = label_tokens[original_pos]\n",
    "    flat_tokens = []\n",
    "    flat_labels = []\n",
    "\n",
    "    # The first word of the subword token is assigned entity\n",
    "    # other tokens will be add PAD labels (we will mask it while training)\n",
    "    assert (len(aligned_words) == len(sub_words_mapped) == len(aligned_labels))\n",
    "    for (_align_word, _align_word, _align_label) in zip(\n",
    "        aligned_words, sub_words_mapped, aligned_labels\n",
    "    ):\n",
    "        temp_w = []\n",
    "        for _align_word in _align_word:\n",
    "            temp_w.append(_align_word)\n",
    "        temp_l = [label_pad_token] * len(temp_w)\n",
    "        temp_l[0] = _align_label\n",
    "        flat_tokens.extend(temp_w)\n",
    "        flat_labels.extend(temp_l)\n",
    "        \n",
    "    return flat_tokens, flat_labels\n",
    "\n",
    "\n",
    "# from tf_transformers.utils import fast_sp_alignment\n",
    "\n",
    "def fast_sp_split(sentence, tokenizer, SPECIAL_PIECE):\n",
    "    original_words = sentence.split()\n",
    "    subwords = tokenizer.tokenize(sentence)\n",
    "\n",
    "    # Convert text into main_words (list of list of subwords per word)\n",
    "    sub_words_mapped = []\n",
    "    temp_tokens = []\n",
    "    for tok in subwords:\n",
    "        if tok == SPECIAL_PIECE:\n",
    "            if temp_tokens:\n",
    "                sub_words_mapped.append(temp_tokens)\n",
    "                temp_tokens = []\n",
    "            sub_words_mapped.append([tok])\n",
    "\n",
    "        else:\n",
    "            if tok.startswith(SPECIAL_PIECE):\n",
    "                if temp_tokens:\n",
    "                    sub_words_mapped.append(temp_tokens)\n",
    "                    temp_tokens = []\n",
    "                temp_tokens.append(tok)\n",
    "            else:\n",
    "                temp_tokens.append(tok)\n",
    "\n",
    "    if temp_tokens:\n",
    "        sub_words_mapped.append(temp_tokens)\n",
    "    return original_words, sub_words_mapped\n",
    "\n",
    "def account_for_special_piece_local(\n",
    "    word_tokens, sub_words_mapped\n",
    "):\n",
    "    # this loop is used to accout for extra SPECIAL_PIECE\n",
    "    # if any\n",
    "    special_counter = 0\n",
    "    aligned_words = []\n",
    "    orig_to_new_index = []\n",
    "    for index, _sub_word in enumerate(sub_words_mapped):\n",
    "        # this is some extra SPECIAL PIECE character\n",
    "        # add it to original word\n",
    "        if len(_sub_word) == 1 and _sub_word[0] == SPECIAL_PIECE:\n",
    "            special_counter += 1\n",
    "            aligned_words.append(_sub_word[0])\n",
    "        else:\n",
    "            pos = index - special_counter\n",
    "            aligned_words.append(word_tokens[pos])\n",
    "            orig_to_new_index.append(index) # whenever original words comes, we need old-new mapping\n",
    "    return orig_to_new_index, aligned_words\n",
    "\n",
    "def fast_sp_alignment(sentence, tokenizer, SPECIAL_PIECE):\n",
    "    \"\"\"Fast Sentence Piece Alignment\n",
    "\n",
    "    A sentence will be split into tokens based on whitespace, then tokenize using\n",
    "    sentence piece tokenizer (GPT2, Albert, etc).\n",
    "\n",
    "    Args:\n",
    "        sentence ([type]): [description]\n",
    "        tokenizer ([type]): [description]\n",
    "        SPECIAL_PIECE ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [orig_to_new_index]: [list: Old to new index mapping alignment]\n",
    "        [aligned_words]: [list of string: Aligns words (by adding SPECIAL_PIECE) if required]\n",
    "        [sub_words_mapped]: [list of list of subwords]\n",
    "    \"\"\"\n",
    "\n",
    "    original_words, sub_words_mapped = fast_sp_split(sentence, tokenizer, SPECIAL_PIECE)\n",
    "    # If they are of different length mostly due to\n",
    "    # extra SPECIAL_PIECE or unicode characters\n",
    "    if len(original_words) != len(sub_words_mapped):\n",
    "        # Try to re-align if possible\n",
    "        try:\n",
    "            orig_to_new_index, aligned_words = account_for_special_piece_local(original_words, sub_words_mapped)\n",
    "        except:\n",
    "            # if re-align fails, then tokenize like word-piece tokenizer\n",
    "            # but, using sentence piece\n",
    "            aligned_words = original_words\n",
    "            sub_words_mapped = [tokenizer.tokenize(word) for word in original_words]\n",
    "            orig_to_new_index = range(len(original_words))\n",
    "\n",
    "        assert(len(aligned_words) == len(sub_words_mapped))\n",
    "        return orig_to_new_index,  aligned_words, sub_words_mapped\n",
    "    else:\n",
    "        # If this mapping fails, logic fails\n",
    "        orig_to_new_index = range(len(original_words))\n",
    "        return orig_to_new_index, original_words, sub_words_mapped\n",
    "\n",
    "\n",
    "# new_to_orig_dict = dict(zip(orig_to_new_index, range(len(orig_to_new_index))))\n",
    "# # reverse mapping\n",
    "\n",
    "# reverse_start_index_align = tok_to_orig_index[tok_start_position] # aligned index\n",
    "# reverse_end_index_align   = tok_to_orig_index[tok_end_position]\n",
    "\n",
    "\n",
    "# reverse_start_index_original = new_to_orig_dict[reverse_start_index_align] # original (doc_tokens) index\n",
    "# reverse_start_index_stop = new_to_orig_dict[reverse_end_index_align]\n",
    "\n",
    "def tokenize_and_align_sentence_label(\n",
    "    sentence, word_tokens, label_tokens, label_pad_token\n",
    "):\n",
    "        \n",
    "    \"\"\"\n",
    "    align sentence sub words and labels using fast_sp\n",
    "    \"\"\"\n",
    "    subwords = tokenizer.tokenize(sentence)\n",
    "    orig_to_new_index, aligned_words, sub_words_mapped = fast_sp_alignment(\n",
    "            sentence, tokenizer, SPECIAL_PIECE\n",
    "        )\n",
    "    \n",
    "    flat_tokens, flat_labels = get_tokens_labels(aligned_words, orig_to_new_index, label_tokens, sub_words_mapped, label_pad_token\n",
    "    )\n",
    "    return flat_tokens, flat_labels\n",
    "\n",
    "def tokenize_and_align_sentence_label_valid(\n",
    "    sentence, word_tokens, label_tokens, label_pad_token\n",
    "):\n",
    "        \n",
    "    \"\"\"\n",
    "    align sentence sub words and labels using fast_sp\n",
    "    \"\"\"\n",
    "    subwords = tokenizer.tokenize(sentence)\n",
    "    orig_to_new_index, aligned_words, sub_words_mapped = fast_sp_alignment(\n",
    "            sentence, tokenizer, SPECIAL_PIECE\n",
    "        )\n",
    "    \n",
    "    flat_tokens, flat_labels = get_tokens_labels(aligned_words, orig_to_new_index, label_tokens, sub_words_mapped, label_pad_token\n",
    "    )\n",
    "    return aligned_words, sub_words_mapped, flat_tokens, flat_labels\n",
    "\n",
    "\n",
    "encoder_max_length = 50  # 50 is enough for SNIPS\n",
    "def process_data_to_model_inputs(sentence, flat_labels, label_pad_token):\n",
    "    \"\"\"\n",
    "    convert text to inputs (ids)\n",
    "    \"\"\"\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    result = {}\n",
    "    result[\"input_ids\"] = tokenizer.encode(\n",
    "        sentence, truncation=True, max_length=encoder_max_length\n",
    "    )\n",
    "    result[\"input_mask\"] = [1] * len(result[\"input_ids\"])\n",
    "    result[\"input_type_ids\"] = [0] * len(result[\"input_ids\"])\n",
    "    labels = [slot_map[token] for token in flat_labels]\n",
    "    labels = [slot_map[label_pad_token]] + labels + [slot_map[label_pad_token]]  # for [CLS] and [SEP]\n",
    "    label_mask = []\n",
    "    for token in flat_labels:\n",
    "        if token == [label_pad_token]:\n",
    "            label_mask.append(0)\n",
    "            continue\n",
    "        label_mask.append(1)\n",
    "    label_mask = [0] + label_mask + [0]  # for [CLS] and [SEP]\n",
    "    result[\"labels\"] = labels\n",
    "    result[\"label_mask\"] = label_mask\n",
    "    return result\n",
    "\n",
    "\n",
    "ignored_index = []\n",
    "mysample = []\n",
    "def make_parse_fn(df):\n",
    "    \"\"\"\n",
    "    Iterte over data frame\n",
    "    \n",
    "    convert (text, labels) -> sub words and labels for each sub_words\n",
    "    \n",
    "    \"\"\"\n",
    "    for index, row in df.iterrows():\n",
    "        sentence = row[\"words\"]\n",
    "        labels = row[\"word_labels\"]\n",
    "        word_tokens = sentence.split()\n",
    "        label_tokens = labels.split()\n",
    "        if len(word_tokens) != len(label_tokens):\n",
    "            ignored_index.append(index)\n",
    "            continue\n",
    "        aligned_words_new, main_words_new, flat_tokens, flat_labels = tokenize_and_align_sentence_label_valid(\n",
    "            sentence, word_tokens, label_tokens, label_pad_token=\"[PAD]\")\n",
    "        mysample.append((flat_tokens, flat_labels))\n",
    "        yield process_data_to_model_inputs(sentence, flat_labels, label_pad_token=\"[PAD]\")\n",
    "    print(\"Ignored {} indexes\".format(len(ignored_index)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4307a7d42413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabel_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m flat_tokens, flat_labels = tokenize_and_align_sentence_label(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_pad_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"[PAD]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-4-f0534e9d40ed>\u001b[0m in \u001b[0;36mtokenize_and_align_sentence_label\u001b[0;34m(sentence, word_tokens, label_tokens, label_pad_token)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0malign\u001b[0m \u001b[0msentence\u001b[0m \u001b[0msub\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0musing\u001b[0m \u001b[0mfast_sp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0msubwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     orig_to_new_index, aligned_words, sub_words_mapped = fast_sp_alignment(\n\u001b[1;32m    139\u001b[0m             \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSPECIAL_PIECE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Sample sentence\n",
    "sentence = 'I love to listen to Carnatic songs by K.J.Yesudas'\n",
    "labels   = 'O O O O O I-genre O O I-artist'\n",
    "word_tokens = sentence.split()\n",
    "label_tokens = labels.split()\n",
    "assert(len(word_tokens) == len(label_tokens))\n",
    "flat_tokens, flat_labels = tokenize_and_align_sentence_label(\n",
    "    sentence, word_tokens, label_tokens, label_pad_token=\"[PAD]\"\n",
    ")\n",
    "sample_inputs = process_data_to_model_inputs(sentence, flat_labels, label_pad_token=\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use TFProcessor only if your data is in range of 10k - 20k maximum\n",
    "# otherwise use TFWriter\n",
    "from tf_transformers.data import TFProcessor\n",
    "\n",
    "tf_processor = TFProcessor()\n",
    "parse_fn = make_parse_fn(df_train)\n",
    "train_dataset = tf_processor.process(parse_fn)\n",
    "\n",
    "parse_fn_valid = make_parse_fn(df_valid)\n",
    "valid_dataset = tf_processor.process(parse_fn_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def ragged_dict_to_tensor(x):\n",
    "    x_temp = {}\n",
    "    for k,v in x.items():\n",
    "        x_temp[k] = v.to_tensor()\n",
    "    return x_temp\n",
    "\n",
    "def separate_x_y(input_dict):\n",
    "    x = {}\n",
    "    y = {}\n",
    "    for k,v in input_dict.items():\n",
    "        if k in x_keys:\n",
    "            x[k] = v\n",
    "        else:\n",
    "            y[k] = v\n",
    "    return (x, y)\n",
    "\n",
    "x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
    "y_keys = ['labels', 'label_mask']\n",
    "batch_size = 32\n",
    "dataset = train_dataset.batch(batch_size)\n",
    "dataset = dataset.map(ragged_dict_to_tensor)\n",
    "dataset = dataset.map(separate_x_y)\n",
    "\n",
    "dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Initialized Variables\n"
     ]
    }
   ],
   "source": [
    "# Lets load Albert Model\n",
    "from tf_transformers.models import AlbertModel\n",
    "from tf_transformers.core import optimization\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(\"INFO\")\n",
    "\n",
    "model_layer, model, config = AlbertModel(model_name='albert_base_v2', \n",
    "                   is_training=True, \n",
    "                   use_dropout=False\n",
    "                   )\n",
    "model.load_checkpoint(\"/mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/albert-base-v2/\")\n",
    "del model_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_checkpoint(\"your checkpoint dir\")\n",
    "\n",
    "# Please have a look at tf_transformers/extra/*.py for reference values\n",
    "\n",
    "input_ids  = tf.constant([[1, 9, 10, 11, 23], \n",
    "                         [1, 22, 234, 432, 2349]])\n",
    "input_mask = tf.ones_like(input_ids)\n",
    "input_type_ids = tf.ones_like(input_ids)\n",
    "\n",
    "inputs = {'input_ids': input_ids, \n",
    "          'input_mask': input_mask, \n",
    "          'input_type_ids': input_type_ids}\n",
    "\n",
    "results_tf_transformers   = model(inputs)\n",
    "for k, r in results_tf_transformers.items():\n",
    "    if isinstance(r, list):\n",
    "        continue\n",
    "    print(k, '-->', tf.reduce_sum(r), '-->', r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.core import LegacyModel, LegacyLayer\n",
    "\n",
    "\n",
    "class Token_Classification(LegacyLayer):\n",
    "    def __init__(self, model, token_vocab_size, use_all_layers=False, activation=None, **kwargs):\n",
    "        super(Token_Classification, self).__init__(**kwargs)\n",
    "        self.model = model\n",
    "        if isinstance(model, LegacyModel):\n",
    "            self.model_config = model.model_config\n",
    "        elif isinstance(model, tf.keras.layers.Layer):\n",
    "            self.model_config = model._config_dict\n",
    "        self.use_all_layers = use_all_layers\n",
    "        self.logits_layer = tf.keras.layers.Dense(\n",
    "            token_vocab_size,\n",
    "            activation=activation\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        result = self.model(inputs)\n",
    "        token_logits = []\n",
    "        if self.use_all_layers:\n",
    "            # each layer token embeddings\n",
    "            for token_embeddings in result[\"all_layer_token_embeddings\"]:\n",
    "                outputs = self.logits_layer(token_embeddings)\n",
    "                token_logits.append(outputs)\n",
    "            return {'token_logits': token_logits}            \n",
    "\n",
    "        else:\n",
    "            # last layer token embeddings\n",
    "            token_embeddings = result[\"token_embeddings\"]\n",
    "            outputs = self.logits_layer(token_embeddings)\n",
    "            return {\n",
    "                    \"token_logits\": outputs\n",
    "            }\n",
    "        \n",
    "    def get_model(self):\n",
    "        layer_output = self(self.model.input)\n",
    "        model = LegacyModel(inputs=self.model.input, outputs=layer_output, name='token_classification')\n",
    "        model.model_config = self.model_config\n",
    "        return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model_ner = Token_Classification(model=model,\n",
    "                                      token_vocab_size=len(slot_map),\n",
    "                                      use_all_layers=True, \n",
    "                                      is_training=True)\n",
    "model_ner = model_ner.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_loss(y_true_dict, token_logits):\n",
    "    loss = cross_entropy_loss_fast(\n",
    "        labels=y_true_dict[\"labels\"],\n",
    "        logits=token_logits,\n",
    "        label_weights=y_true_dict[\"label_mask\"],\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def token_loss_all_layers(y_true_dict, y_pred_dict):\n",
    "    layer_loss = []\n",
    "    for token_logits in y_pred_dict['token_logits']:\n",
    "        loss = token_loss(y_true_dict, token_logits)\n",
    "        layer_loss.append(loss)\n",
    "    return tf.reduce_mean(layer_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_size = 13000\n",
    "learning_rate   = 2e-5\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "EPOCHS = 3\n",
    "num_train_steps = steps_per_epoch * EPOCHS\n",
    "warmup_steps = int(EPOCHS * train_data_size * 0.1 / batch_size)\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer_type = 'adamw'\n",
    "optimizer, learning_rate_fn = optimization.create_optimizer(learning_rate,\n",
    "                                                steps_per_epoch * EPOCHS,\n",
    "                                                warmup_steps,\n",
    "                                                optimizer_type)\n",
    "\n",
    "# # Compile\n",
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "# loss_fn = {'token_logits': token_loss_all_layers}\n",
    "# model_ner.compile2(optimizer=optimizer, \n",
    "#                             loss=None, \n",
    "#                             custom_loss=loss_fn)\n",
    "\n",
    "\n",
    "\n",
    "# class LossHistory(tf.keras.callbacks.Callback):\n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         self.losses = []\n",
    "#         self.val_losses = []\n",
    "\n",
    "#     def on_batch_end(self, batch, logs={}):\n",
    "#         self.losses.append(logs.get('loss'))\n",
    "#         self.val_losses.append(logs.get('val_loss'))\n",
    "# # Fit\n",
    "# history_callbacks = LossHistory()\n",
    "# history = model_ner.fit(dataset, epochs=3, callbacks=[history_callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_map_reverse = {v:k for k,v in slot_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ner.save_checkpoint(\"temp_model\", overwrite=True)\n",
    "model_ner = Token_Classification(model=model,\n",
    "                                      token_vocab_size=len(slot_map),\n",
    "                                      use_all_layers=True, \n",
    "                                      is_training=False)\n",
    "model_ner = model_ner.get_model()\n",
    "model_ner.load_checkpoint(\"temp_model\")\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "dataset_valid = valid_dataset.batch(batch_size)\n",
    "dataset_valid = dataset_valid.map(ragged_dict_to_tensor)\n",
    "dataset_valid = dataset_valid.map(separate_x_y)\n",
    "\n",
    "num_layers = 12\n",
    "prediction_per_layer = {i:[] for i in range(num_layers)}\n",
    "original_labels = []\n",
    "for (batch_inputs, batch_labels) in dataset_valid:\n",
    "    results = model_ner(batch_inputs)\n",
    "    model_logits = results['token_logits'][-1]\n",
    "    \n",
    "    for i, model_logits in enumerate(results['token_logits']):\n",
    "    \n",
    "        # Iterate over each example\n",
    "        for index, per_example_logits in enumerate(model_logits):\n",
    "            per_example_length = tf.reduce_sum(batch_inputs['input_mask'][index])\n",
    "            per_example_label  = batch_labels['labels'][index][:per_example_length][1:-1] # we dont want pad positions and 1:-1 is to remove CLS and SEP\n",
    "            per_example_logits = per_example_logits[:per_example_length][1:-1] # 1:-1 CLS and SEP\n",
    "            per_example_preds  = tf.argmax(per_example_logits, axis=-1)\n",
    "            prediction_per_layer[i].append(per_example_preds)\n",
    "            \n",
    "            # We want the original label only once\n",
    "            if i == 0:\n",
    "                original_labels.append(per_example_label)\n",
    "    \n",
    "    \n",
    "# We have 700 examples\n",
    "for layer_iter in range(num_layers):\n",
    "    result = prediction_per_layer[layer_iter]\n",
    "    \n",
    "    pred_list = []\n",
    "    for i in range(700):\n",
    "        pred = list(result[i].numpy())\n",
    "        orig = list(original_labels[i].numpy())\n",
    "        if pred == orig:\n",
    "            pred_list.append(1)\n",
    "        else:\n",
    "            pred_list.append(0)\n",
    "    print(\"Layer {} exact match {} / 700\".format(layer_iter, sum(pred_list)))\n",
    "    \n",
    "# Layer 0 exact match 110 / 700\n",
    "# Layer 1 exact match 321 / 700\n",
    "# Layer 2 exact match 436 / 700\n",
    "# Layer 3 exact match 500 / 700\n",
    "# Layer 4 exact match 531 / 700\n",
    "# Layer 5 exact match 553 / 700\n",
    "# Layer 6 exact match 567 / 700\n",
    "# Layer 7 exact match 575 / 700\n",
    "# Layer 8 exact match 570 / 700\n",
    "# Layer 9 exact match 573 / 700\n",
    "# Layer 10 exact match 572 / 700\n",
    "# Layer 11 exact match 566 / 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, 5 the layer itself is giving 531/700 \n",
    "# Thats great. Lets finalize the model with 5 hidden layers (instead of 12)\n",
    "\n",
    "from tf_transformers.models import AlbertEncoder\n",
    "\n",
    "config_copy = config.copy()\n",
    "config_copy['num_hidden_layers'] = 5\n",
    "model_final = AlbertEncoder(config, \n",
    "                            mask_mode=config['mask_mode'], \n",
    "                            is_training=False, \n",
    "                            name='albert')\n",
    "model_final  = model_final.get_model()\n",
    "model_final.load_checkpoint(\"/mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/albert-base-v2/\")\n",
    "\n",
    "# As we finalized the best layer\n",
    "# we use_all_layers=False\n",
    "tf.keras.backend.clear_session()\n",
    "model_ner_final = Token_Classification(model=model_final,\n",
    "                                      token_vocab_size=len(slot_map),\n",
    "                                      use_all_layers=False, \n",
    "                                      is_training=False)\n",
    "model_ner_final = model_ner_final.get_model()\n",
    "model_ner_final.load_checkpoint(\"temp_model\")\n",
    "\n",
    "\n",
    "predictions_final = []\n",
    "original_labels = []\n",
    "for (batch_inputs, batch_labels) in dataset_valid:\n",
    "    results = model_ner_final(batch_inputs)\n",
    "    model_logits = results['token_logits']\n",
    "    \n",
    "    \n",
    "    # Iterate over each example\n",
    "    for index, per_example_logits in enumerate(model_logits):\n",
    "        per_example_length = tf.reduce_sum(batch_inputs['input_mask'][index])\n",
    "        per_example_label  = batch_labels['labels'][index][:per_example_length][1:-1] # we dont want pad positions and 1:-1 is to remove CLS and SEP\n",
    "        per_example_logits = per_example_logits[:per_example_length][1:-1] # 1:-1 CLS and SEP\n",
    "        per_example_preds  = tf.argmax(per_example_logits, axis=-1)\n",
    "        \n",
    "        predictions_final.append(per_example_preds)\n",
    "        original_labels.append(per_example_label)\n",
    "        \n",
    "# Evaluate  \n",
    "pred_list = []\n",
    "for i in range(700):\n",
    "    pred = list(predictions_final[i].numpy())\n",
    "    orig = list(original_labels[i].numpy())\n",
    "    if pred == orig:\n",
    "        pred_list.append(1)\n",
    "    else:\n",
    "        pred_list.append(0)\n",
    "print(\"exact match {} / 700\".format(sum(pred_list)))\n",
    "\n",
    "# It matches with the result of layer 5 (actualy layer 4 if we start from index 0)\n",
    "# exact match 566 / 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the model\n",
    "model_ner_final.save_as_serialize_module(\"temp_model_pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tflite\n",
    "\n",
    "# Make sure none of the input dimension is None\n",
    "model_final = AlbertEncoder(config, \n",
    "                            mask_mode=config['mask_mode'], \n",
    "                            is_training=False,\n",
    "                            batch_size=1,\n",
    "                            sequence_length=50, # 50 is enough for SNIPS\n",
    "                            name='albert')\n",
    "model_final  = model_final.get_model()\n",
    "model_final.load_checkpoint(\"/mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/albert-base-v2/\")\n",
    "\n",
    "# As we finalized the best layer\n",
    "# we use_all_layers=False\n",
    "tf.keras.backend.clear_session()\n",
    "model_ner_final = Token_Classification(model=model_final,\n",
    "                                      token_vocab_size=len(slot_map),\n",
    "                                      use_all_layers=False, \n",
    "                                      is_training=False)\n",
    "model_ner_final = model_ner_final.get_model()\n",
    "model_ner_final.load_checkpoint(\"temp_model\")\n",
    "model_ner_final.save_as_serialize_module(\"temp_model_pb_tflite\")\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"temp_model_pb_tflite\") # path to the SavedModel directory\n",
    "converter.experimental_new_converter = True\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(\"converted_model.tflite\", \"wb\").write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Production\n",
    "\n",
    "# Load saved model\n",
    "\n",
    "loaded = tf.saved_model.load(\"temp_model_pb\")\n",
    "model = loaded.signatures['serving_default']\n",
    "\n",
    "\n",
    "sentence = 'I love to listen to Carnatic songs by K.J.Yesudas'\n",
    "labels   = 'O O O O O I-genre O O I-artist'\n",
    "word_tokens = sentence.split()\n",
    "label_tokens = labels.split()\n",
    "assert(len(word_tokens) == len(label_tokens))\n",
    "aligned_words, main_words, flat_tokens, flat_labels = tokenize_and_align_sentence_label_valid(\n",
    "        sentence, word_tokens, label_tokens, label_pad_token=\"[PAD]\"\n",
    "    )\n",
    "sample_inputs = process_data_to_model_inputs(sentence, flat_labels, label_pad_token=\"[PAD]\")\n",
    "sample_inputs = {k: tf.constant([v]) for  k, v in sample_inputs.items() if k in ['input_ids', 'input_mask', 'input_type_ids']}\n",
    "\n",
    "results = model(**sample_inputs)\n",
    "\n",
    "slot_logits = results['token_logits']\n",
    "slot_ids = slot_logits.numpy().argmax(axis=-1)[0, 1:-1] # to avoid CLS and SEP\n",
    "slot_probs = slot_logits.numpy().max(axis=-1)[0, 1:-1]\n",
    "\n",
    "# Extract only words that we want\n",
    "subword_counter = -1\n",
    "predicted_ids = []\n",
    "for sub_word_list in main_words:\n",
    "    if len(sub_word_list) == 1 and sub_word_list[0] == SPECIAL_PIECE:\n",
    "        subword_counter += 1\n",
    "        continue\n",
    "    else:\n",
    "        predicted_ids.append(slot_ids[subword_counter+1])\n",
    "        subword_counter += len(sub_word_list)\n",
    "\n",
    "predicted_labels = [slot_map_reverse[idx] for idx in predicted_ids]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check same model with tflite\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "sample_inputs = process_data_to_model_inputs(sentence, flat_labels, label_pad_token=\"[PAD]\")\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], tf.concat([sample_inputs['input_ids'],\n",
    "                                                             tf.zeros((1, 31), dtype=tf.int32)], axis=1))\n",
    "\n",
    "interpreter.set_tensor(input_details[1]['index'], tf.concat([sample_inputs['input_mask'],\n",
    "                                                             tf.zeros((1, 31), dtype=tf.int32)], axis=1))\n",
    "\n",
    "interpreter.set_tensor(input_details[2]['index'], tf.concat([sample_inputs['input_type_ids'],\n",
    "                                                             tf.zeros((1, 31), dtype=tf.int32)], axis=1))\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_data = np.squeeze(output_data, 0)[:18, :]\n",
    "\n",
    "slot_ids = output_data.argmax(axis=-1)\n",
    "slot_probs = output_data.max(axis=-1)\n",
    "\n",
    "# Extract only words that we want\n",
    "subword_counter = -1\n",
    "predicted_ids = []\n",
    "for sub_word_list in main_words:\n",
    "    if len(sub_word_list) == 1 and sub_word_list[0] == SPECIAL_PIECE:\n",
    "        subword_counter += 1\n",
    "        continue\n",
    "    else:\n",
    "        predicted_ids.append(slot_ids[subword_counter+1])\n",
    "        subword_counter += len(sub_word_list)\n",
    "\n",
    "predicted_labels = [slot_map_reverse[idx] for idx in predicted_ids]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
